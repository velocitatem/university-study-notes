:PROPERTIES:
:ID:       a66d2149-cb01-4124-bcc8-c5e9f1669d3d
:END:
#+title: Simulating and Modeling to Understand Change - Class Notes
#+HTML_HEAD: <link rel="stylesheet" href="https://alves.world/org.css" type="text/css">
#+HTML_HEAD: <style type="text/css" media="print"> body { visibility: hidden; display: none } </style>
#+OPTIONS: toc:2
#+HTML_HEAD: <script src="https://alves.world/tracking.js" ></script>
#+HTML_HEAD: <script src="anti-cheat.js"></script>
#+HTML: <script data-name="BMC-Widget" data-cfasync="false" src="https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js" data-id="velocitatem24" data-description="Support me on Buy me a coffee!" data-message="" data-color="#5F7FFF" data-position="Right" data-x_margin="18" data-y_margin="18"></script>
#+HTML: <script>setTimeout(() => {alert("Finding this useful? Consider buying me a coffee! Bottom right cornner :) Takes just a few seconds")}, 60*1000);_paq.push(['trackEvent', 'Exposure', 'Exposed to beg']);</script>


*New* search feature! Make use of the amazing [[https://en.wikipedia.org/wiki/Approximate_string_matching][fuzzy search]] algorithm. Just type in the search box and it will find the closest match in the page. Hit =Enter= to jump to the next match. Lmk if it doesn't work for you.
#+HTML: <input id="search" type="text" placeholder="Search" /> <span id="resultCount"></span>
#+HTML: <script src="https://alves.world/fuzzy.js"></script>


#+HTML: <a href="https://colab.research.google.com/github/velocitatem/university-study-notes/blob/master/content/20221231174804-simulating_and_modeling_to_understand_change_class_notes.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

* Programming (Python)
+ Optional Arguments :: This is a great way to make code more flexible. It is used in function definitions like so:
  #+BEGIN_src ipython :results output
  def my_function(a, b, c=1, d=2):
      print(a, b, c, d)
  #+END_SRC

  #+RESULTS:

+ Global Variables :: This is a variable that can be accessed from anywhere in the code. It is defined outside of any function. Here is an example:
  #+BEGIN_src ipython :results output
    x = 1
    def my_function():
        global x
        x = 2
    my_function()
    print(x)
  #+END_SRC

  #+RESULTS:
  : 2


** Lists
+ Adding normal lists :: If we add normal, lists we get a new list with the elements of both lists
  #+BEGIN_src ipython :results output
  a = [1, 2, 3]
  b = [4, 5, 6]
  c = a + b
  print(c)
  #+END_SRC

  #+RESULTS:
  : [1, 2, 3, 4, 5, 6]

+ Numpy *atomic* vector :: This is a vector, that can only store *one* type of data.
+ Dataframe Statistics :: We can get statistics from a dataframe using the describe() function
  #+BEGIN_src ipython :results output
  import pandas as pd
  df = pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6]})
  print(df.describe())
  #+END_SRC

  #+RESULTS:
  :          a    b
  : count  3.0  3.0
  : mean   2.0  5.0
  : std    1.0  1.0
  : min    1.0  4.0
  : 25%    1.5  4.5
  : 50%    2.0  5.0
  : 75%    2.5  5.5
  : max    3.0  6.0

* Introduction
** Systems
A system is a group of objects that work together to achieve a goal. It consists of two environments: the inside and the outside. Each entity in the system has attributes or can do activities. A state can be defined for the system or components of the system. Examples of systems include computer systems, forests, restaurants, and banks. The key components of a system are the entities, attributes, activities, states, and events. Systems can be either discrete or continuous, such as the people in a bank or the water levels in a river.
** Models
A model is an abstraction of a real-life situation, where the complexity of the model increases with the number of variables that are taken into account. The simplicity or realism of a model is determined by how closely it reflects the actual situation it is representing.
** Simulation

Simulation is a great tool to use when attempting to gain insights into the behavior of complex systems. Stochastic simulations rely on randomness to predict events, while deterministic simulations are based on predetermined inputs. For example, a restaurant/shop system can be simulated by making assumptions about the number of customers and employees in the system. Simulations are advantageous because they are cheaper, faster, replicable, safer, ethical, and legal when compared to real-life experiments. The decision between using a stochastic or deterministic simulation depends on the circumstances and the desired outputs.

+ The point in which the state of a system changes is called an event.


*** Deterministic

In physics, determinism is the idea that all events are predetermined. There is no room for randomness or probability, as all changes are predetermined by the laws of nature. An example of this is an account gaining and losing followers, which is predetermined by the actions of the account holder. To simulate this process in Python, one could create a loop that tracks the number of followers gained and lost over time and stores it in a variable. This variable could then be used to print out the number of followers at any given time.
*** Stochastic
# stochastic - has randomness, uses random variables
Stochastic simulation is a modelling technique which incorporates randomness, making use of random variables to generate a variety of possible outcomes. It is used for analysing complex systems in which the effects of randomness cannot be predicted deterministically, and thus provides a useful tool for predicting and understanding the behaviour of such systems.
*** Statics vs Dynamic
# Static: there is no time variable
# Dymanic: there is a time variable
Simulations are typically classified as either static or dynamic. In a static simulation, there is no time variable; the system is unchanging and the same set of conditions is used throughout the simulation. In a dynamic simulation, time is a variable, meaning that the system is constantly changing and the conditions of the simulation can evolve over time.

*** Decision Tree
# #+BEGIN_SRC plantuml
# @startuml
# start
# if (Is there randomness?) then (yes)
#   :Stochastic;
#   if (Is there time?) then (yes)
#     :Dynamic;
#     if (Is there a continuous variable?) then (yes)
#       :Continuous;
#       stop
#     else (no)
#       :Discrete;
#       stop
#     endif
#   else (no)
#     :Static;
#     if (Is there a continuous variable?) then (yes)
#       :Continuous;
#       stop
#     else (no)
#       :Discrete;
#       stop
#     endif
#   endif
# else (no)
#   :Deterministic;
#   if (Is there time?) then (yes)
#     :Dynamic;
#     if (Is there a continuous variable?) then (yes)
#       :Continuous;
#       stop
#     else (no)
#       :Discrete;
#       stop
#     endif
#   else (no)
#     :Static;
#     if (Is there a continuous variable?) then (yes)
#       :Continuous;
#       stop
#     else (no)
#       :Discrete;
#       stop
#     endif
#   endif
# stop
# @enduml
# #+END_SRC


#+DOWNLOADED: https://www.plantuml.com/plantuml/png/xP0n3i8m34Ntdi9ZUmKwCD25cHEOr5KjDGwANQJU7jEY2Weh4WDJag_thFYhORQ2EoYF2jPJ9iWeROf2gllg7WcK4sbL0EnEbaAFQXngVxvOnf6sl1lD91WNV2CWX2J6CgPHuOgPyJnycE0p3YjNFoArT9clD2X1AMbb6zQGoUYTb9QxOcluHcY_zBtbVFBSSJv75lxt_ktUv_W6 @ 2023-02-25 16:21:50
[[file:./Introduction/2023-02-25_16-21-50_xP0n3i8m34Ntdi9ZUmKwCD25cHEOr5KjDGwANQJU7jEY2Weh4WDJag_thFYhORQ2EoYF2jPJ9iWeROf2gllg7WcK4sbL0EnEbaAFQXngVxvOnf6sl1lD91WNV2CWX2J6CgPHuOgPyJnycE0p3YjNFoArT9clD2X1AMbb6zQGoUYTb9QxOcluHcY_zBtbVFBSSJv75lxt_ktUv_W6.png]]

I hope this is right.

** A Seed
# it can be any number
# allows us to replicate semi-random experiments
A seed is any number that can be used to replicate semi-random experiments and simulations. It allows for the same experiment to be repeated in the same way, with the same conditions and results, by using the same seed each time. This makes it easy to compare results from different experiments and simulations, as the same starting point can be replicated.

* Random Numbers Generation
+ getting randomness is almost impossible
+ People confuse randomness with strangeness
+ Continuous distribution to discrete on range 0 to 1 - can be done by rounding
+ Properties of pseudo-random numbers: uniform, independent, unpredictable
+ Testing the randomness of a sequence of numbers:
  + Look at the distribution of the numbers (visual) - should be uniform
  + See if there is any pattern
+ Algorithms:
  + must be fast
  + must be long
  + should be repeatable with a seed
+ Setting seed in python =random.seed(2023)=
+ We can generate $n$ random numbers with python by using =np.random.uniform(min,max,nax)=

** Linear Congruential Method
+ We get a random like pattern.
+ It is a linear transformation of a previous number
+ Equation: $x_{n} = (a x_{n-1} + c) \mod m$
#+begin_src ipython :results output

  import numpy as np
  def LCM(n, seed, a, c, m):
      x = [seed]
      for i in range(1,n+1):
          x.append((a*x[i-1] + c) % m)
      u = np.array(x)/m
      return u

  seq =LCM(n=8, seed=4, a=13, c=0, m=64)
  print(seq)
#+end_src

#+RESULTS:
: [0.0625 0.8125 0.5625 0.3125 0.0625 0.8125 0.5625 0.3125 0.0625]

+ Those results are pretty bad
* Testing Randomness
To check if we have an actually random generator, we need to test *uniformity* and *independence*.
** Testing Uniformity
+ We test using hypothesis testing
  + Null hypothesis = sequence is uniform
  + Alternative hypothesis = sequence is not uniform
+ We use an alpha level of 0.05. If our $p$ is less than 0.05 we reject the null hypothesis, otherwise we fail to reject the null hypothesis
+ We want to fail to reject the null hypothesis to have uniformity.
+ The test we use is *Kolmogrov-Smirnov* test
+ We use the function =stats.kstest= from the =scipy.stats= library

** Testing Independence
+ We again make use of hypothesis testing
  + Null hypothesis = sequence is independent
  + Alternative hypothesis = sequence is not independent
+ To test for the dependence of each number, we use correlation
+ The specific type of correlation we use is *autocorrelation*
  + This means that we correlate the number and the sequence
+ When we auto-correlate, we need to have a lag
  + This is the number of steps we take ahead in the sequence
+ We can use a pandas data frame:
#+begin_src ipython :tangle yes :results file :exports both :noweb yes
  import numpy as np
  import pandas as pd
  import matplotlib.pyplot as plt
  seq = np.random.uniform(0,1,100)
  df = pd.DataFrame(seq, columns=['values'])
  pd.plotting.autocorrelation_plot(df['values'])
  plt.savefig("autocorrelation.png")
#+end_src

#+RESULTS:
[[file:./obipy-resources/6oGzv6.png]]

In the above plot:
+ If the line is within the dashed lines, we fail to reject the null hypothesis

We need a more robust way of assessing if the sequence is independent:
#+begin_src ipython :tangle yes :results output :exports both :noweb yes :eval no
  acf, confint = statstools.acf(seq, alpha=0.05, nlags=10)
  lbvalue, pvalue = statstools.q_stat(acf[1:], len(seq))
  print("p-value: ", pvalue)
#+end_src

Now we can use the p-value to test for independence.

A key point here, is the difference between correlation and autocorrelation.
| Correlation              | Autocorrelation                   |
|--------------------------+-----------------------------------|
| Correlates two variables | Correlates a variable with itself |
| No lag                   | Lag                               |



* Discrete Random Variable Simulation
A random variable is a variable, with some potential outcomes, that is determined by their respective probabilities.
+ Probability Mass Function :: This is a function that gives the probability of a discrete random variable taking on a specific value.
+ Cumulative Distribution Function :: This is a function that gives the probability of a discrete random variable taking on a value less than or equal to a specific value.

** Bernoulli Distribution
+ This is a discrete random variable with two possible outcomes
+ The probability of the first outcome is $p$
+ The probability of the second outcome is $1-p$
+ The general pmf is given by $f(x) = p^x(1-p)^{1-x}$
+ Expected value and variance are given by $E(X) = p$ and $Var(X) = p(1-p)$
+ We can simulate this in python using =np.random.binomial(1,p,n)=

** Binomial Distribution
+ Very similar to the Bernoulli distribution
  + Key difference is that we have $n$ trials
+ The general pmf is given by $f(x) = \binom{n}{x}p^x(1-p)^{n-x}$
+ Expected value and variance are given by $E(X) = np$ and $Var(X) = np(1-p)$
+ We can simulate this in python using =np.random.binomial(n,p,n_1)=
  + This will give us $n_1$ samples of $n$ trials with probability $p$

** Geometric Distribution
+ This distributions gives us the probability of the first success in $n$ trials
+ The general pmf is given by $f(X = x) = (1-p)^x p$
+ Expected value and variance are given by $E(X) = \frac{1 - p}{p}$ and $Var(X) = \frac{1-p}{p^2}$
+ We can simulate this in python using =np.random.geometric(p,n)=
+ There is also the stats library which gives
  + =stats.geom.pmf(x,p)= and =stats.geom.cdf(x,p)=

** Poisson Distribution
+ This distribution gives us the probability of $k$ events in a given time period
+ The general pmf is given by $f(x) = \frac{\lambda^x e^{-\lambda}}{x!}$
  + Turns into an exponential distribution when $\lambda \rightarrow \infty$
  + $\lambda$ is the mean number of events in the time period
  + It can take negative values
  + Values can be non-integer
+ Expected value and variance are given by $E(X) = \lambda$ and $Var(X) = \lambda$
+ We can simulate this in python using =np.random.poisson(lam,n)=
  + To compute the pdf we can use =stats.poisson.pmf(x,lam)=
+ Approximation
* Continuous Random Variable Simulation
** Cumulative Distribution Function
+ This is a function that gives the probability of a continuous random variable taking on a value less than or equal to a specific value.
+ The general cdf is given by $F(x) = \int_{-\infty}^{x} f(x) dx$

** Uniform Distribution
+ In this distribution, all values are equally likely
+ The pdf is given by $f(x) = \frac{1}{b-a}$
+ Expected value and variance are given by $E(X) = \frac{a+b}{2}$ and $Var(X) = \frac{(b-a)^2}{12}$
+ The cumulative distribution function is given by $F(x) = \frac{x-a}{b-a}$
+ We can simulate this in python using =np.random.uniform(a,b,n)=
  + We can get the pdf using =stats.uniform.pdf(x,a,b)=
** Exponential Distribution
+ This distribution gives us the probability of the time between events in Poisson processes.
  + It answers a question such as: "What is the probability that something will happen in the next n minutes?"
+ The pdf is given by $f(x) = \lambda e^{-\lambda x}$ where $\lambda = \frac{1}{E(X)}$
+ Expected value and variance are given by $E(X) = \frac{1}{\lambda}$ and $Var(X) = \frac{1}{\lambda^2}$
+ The cumulative distribution function is given by $F(x) = 1 - e^{-\lambda x}$
+ We can simulate this in python using =np.random.exponential(scale,n)=
  + The scale is the inverse of the rate parameter $\lambda$
  + We can get the pdf using =stats.expon.pdf(x,scale)= or

** Normal Distribution
+ This distribution is the most common distribution
+ The pdf is given by $f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}$ - This is horrible.
+ Expected value and variance are given by $E(X) = \mu$ and $Var(X) = \sigma^2$
+ We can simulate this in python using =np.random.normal(mu,sigma,n)=
  + We can get the pdf using =stats.norm.pdf(x,mu,sigma)=
  + We can get the cdf using =stats.norm.cdf(x,mu,sigma)=
* Choosing the right distribution
How do we know which distribution to use? We can use the following table to help us out.

| Distribution | Use Case                                 |
|--------------+------------------------------------------|
| Bernoulli    | Binary outcome                           |
| Binomial     | Number of successes in $n$ trials        |
| Geometric    | Number of trials until first success     |
| Poisson      | Number of events in a given time period  |
| Uniform      | All values are equally likely            |
| Exponential  | Time between events in Poisson processes |
| Normal       | Most common distribution                 |

# Key variable characteristics:
# + Discrete or continuous
# + Symmetric or asymmetric
# + Binary or non-binary

# | Distribution | Discrete | Continuous | Symmetric | Asymmetric | Binary | Non-Binary |
# |--------------+----------+------------+-----------+------------+--------+------------|
# | Bernoulli    | Yes      | No         | No        | Yes        | Yes    | No         |
# | Binomial     | Yes      | No         | No        | Yes        | No     | Yes        |
# | Geometric    | Yes      | No         | No        | Yes        | No     | Yes        |
# | Poisson      | Yes      | No         | No        | Yes        | No     | Yes        |
# | Uniform      | No       | Yes        | Yes       | No         | No     | Yes        |
# | Exponential  | No       | Yes        | Yes       | No         | No     | Yes        |
# | Normal       | No       | Yes        | Yes       | No         | No     | Yes        |


# Flow chart to help us choose the right distribution:
# #+BEGIN_SRC plantuml
# @startuml
# start
# :Discrete or continuous?;
# if (Discrete) then (Yes)
#   :Symmetric or asymmetric?;
#   if (Symmetric) then (Yes)
#     :Binary or non-binary?;
#     if (Binary) then (Yes)
#       :Bernoulli;
#     else (No)
#       :Binomial;
#     endif
#   else (No)
#     :Binary or non-binary?;
#     if (Binary) then (Yes)
#       :Geometric;
#     else (No)
#       :Poisson;
#     endif
#   endif
# else (No)
#   :Symmetric or asymmetric?;
#   if (Symmetric) then (Yes)
#     :Uniform;
#   else (No)
#     :Exponential;
#   endif
# endif
# stop
# @enduml
# #+END_SRC




file:./Choosing_the_right_distribution/plot.png

* Monte Carlo Method
First, what is the Monte Carlo method? It is the aggregation of multiple simulations, to infer something.
This should not be confused with the Monte Carlo simulation, which is a simulation of a random variable. Here is a table to help you remember the difference, it highlights the key differences between the two:

| Monte Carlo Method                                                                               | Monte Carlo Simulation                                                                                     |
|--------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------|
| A mathematical technique used for solving problems through repeated random sampling              | A type of Monte Carlo method that involves generating random input values to simulate a system             |
| Used to estimate the value of an unknown quantity based on a sample of random data               | Used to simulate the behavior of a system under different scenarios                                        |
| Can be used to solve problems in various fields like finance, engineering, and physics           | Can be used to analyze the behavior of systems in various fields like finance, engineering, and physics    |
| Can be used to generate random numbers, simulate random walks, and perform numerical integration | Can be used to answer the "what if" questions and incorporate a wider variety of scenarios than historical data |
| Samples are independent and identically distributed                                              | Samples can be positively correlated and thereby increases the variance of your estimates                  |
| Can be used in combination with Markov chains                                                    | Can be used to estimate the probability of a particular outcome or the range of possible outcomes          |

[[https://phind.com/search?q=Create+a+table+of+the+key+differences+between+the+monte+carlo+method+and+monte+carlo+simulation][ref]]
* Monte Carlo Simulation
Now that we have a basic understanding of the Monte Carlo method, let's talk about the Monte Carlo simulation. This is a simulation of a random variable. We can use this to estimate the expected value of a random variable.


Characteristics:
+ Randomness - has to have a large period between repeats (stochastic)
+ Large sample
+ Valid algorithm
+ Accurately simulates
** Process
1. Define the domain of inputs (What kind of inputs are we going to use?)
2. Generate the inputs from a distribution (How are we going to generate the inputs?)
3. Run a simulation
4. Replicate
5. Aggregate

** Using Python
We will often have to select some categorical value when it comes to MCS. In python, we can make use of =numpy.random.choice()= to do this. This function takes in a list of values and a probability distribution and returns a random value from the list. For example, if we wanted to simulate a coin flip, we could do the following:

#+BEGIN_src ipython :results output
  import numpy as np
  print(np.random.choice(['heads', 'tails'], p=[0.5, 0.5]))
#+END_SRC

#+RESULTS:
: tails

We do not have to give it the probability distribution, if we do not, it will assume that all values are equally likely. For example, if we wanted to simulate a die roll, we could do the following:

#+BEGIN_src ipython :results output
  import numpy as np
  print(np.random.choice([1, 2, 3, 4, 5, 6]))
#+END_SRC

#+RESULTS:
: 4

** Inferential Statistics
+ We use inferential statistics to make inferences about a population from a sample
+ We simulate a sample, calculate the statistics and then use the statistics to make inferences about the population
* Discrete Events Simulation
In this type of simulation, we model real-world systems as a sequence of discrete events. We can use this to model things like a manufacturing process, a supply chain, or a financial market. We can use this to answer questions like "What is the probability that a product will be delivered on time?" or "What is the probability that a customer will buy a product?".

We can also answer questions about how efficient a system is or howmany resources are needed to run a system. For example, we can answer questions like "How many employees are needed to run a manufacturing process?" or "How many machines are needed to run a manufacturing process?".

+ Warmup Period :: This is a period of time where the simulation is preparing, data is being loaded.

** Components
+ Entities :: These are the objects that are being modeled. For example, in a manufacturing process, the entities could be products.
+ Events :: These are the actions that are performed on the entities. For example, in a manufacturing process, the events could be the actions that are performed on the products.
+ Resources :: These are the things that perform the events. For example, in a manufacturing process, the resources could be the machines that perform the actions on the products.


** Types
+ Activity Oriented
  + We model the system as a series of activities
+ Event Oriented
  + We create various events
  + Benefit: we can keep track of time
+ Process Oriented
  + Models entire life cycles
  + Benefit: we can keep track of time and resources

** =simpy= Library
We can create these simulations using =simpy=, a python library for discrete event simulation. We can install it using =pip install simpy=. We can then import it using =import simpy=.
+ The type of simulation is process oriented

The structure of a simulation in =simpy= designed with functional programming is as follows:

1. Define the environment
2. Define the resources
3. Define the processes
4. Run the simulation

What is a process? A process is a function that defines the behavior of an entity in the simulation. For example, if we were simulating a manufacturing process, we could have a process that defines the behavior of a machine.

We make use of generators to simulate new entities entering the system. We can then use =yield= to wait for a certain amount of time or for a resource to become available. We can then use =env.run()= to run the simulation.

** Designing Process
Let's take a look at a very simple example of a DES to learn how to use generators within discrete eventsimulations. The following graph describes a very simple experiment in which we simulate the queue ofpatients arriving at a weight loss clinic. We will have inter-arrival times of consultations, the entities aspatients, and the activity times will be represented by the consultation time of the patients with the nurse.

This is the process of how to design this simulation:

1. create a patient generator that generates patients at a certain rate
2. create an activity generator for each of the patients
   1. Request a resource (nurse)
   2. Create a queue time for the patient
   3. Create a consultation time for the patient
   4. Release the resource (nurse)
3. Run the simulation


# #+BEGIN_SRC plantuml
# @startuml
# PatientGenerator -> ActivityGenerator: Generates a patient
# ActivityGenerator --> Resource: Requests a nurse
# Resource --> ActivityGenerator: Gives the nurse to the patient
# ActivityGenerator --> QueueTime: Calculates queue time
# QueueTime --> ConsultationTime: Calculates consultation time
# ConsultationTime --> Resource: Releases the nurse
# @enduml

# #+END_SRC



#+DOWNLOADED: https://www.plantuml.com/plantuml/png/XP1H2i8m38RVUufSO1VOWqpsuAkA5mWrOA5hNPCClBtjDeTQnBVip_VnP-rOC8aEtXnH70KvKA244XCqEzXRSPEJnnQsi8x4W71V55DXq6JvJ1mrMMhpD2gn52LeOZAl5JG_IjoK2-H62myIoyUVlfEIqiKDkR17RzNFLuub1Sanc8sO-Ju6LYyeBePAis_BrVxcgv_qXFnnjZ4TXMjvt2S0 @ 2023-02-25 16:47:11
[[file:./Discrete_Events_Simulation/2023-02-25_16-47-11_XP1H2i8m38RVUufSO1VOWqpsuAkA5mWrOA5hNPCClBtjDeTQnBVip_VnP-rOC8aEtXnH70KvKA244XCqEzXRSPEJnnQsi8x4W71V55DXq6JvJ1mrMMhpD2gn52LeOZAl5JG_IjoK2-H62myIoyUVlfEIqiKDkR17RzNFLuub1Sanc8sO-Ju6LYyeBePAis_BrVxcgv_qXFnnjZ4TXMjvt2S0.png]]


** Example
Let's say we have a manufacturing process that has 3 machines. We want to know how many products we can make in a day. We can model this using =simpy= as follows:

#+BEGIN_src ipython
  import simpy
  import numpy as np

  env = simpy.Environment()

  # Define the resources
  machine = simpy.Resource(env, capacity=3)

  # Define the processes
  def manufacturing_process(env, machine):
      # Wait for a machine to become available
      with machine.request() as request:
          # Wait for the machine to become available
          yield request
          # Wait for the manufacturing process to complete
          yield env.timeout(np.random.uniform(0, 1))

  # Run the simulation
  env.process(manufacturing_process(env, machine))
  env.run(until=1)
#+END_SRC


# Midterm cutoff
#+HTML: <hr><h2>Midterm cutoff</h2><hr>


* Data Exploration
When building a model, it is necessary to use data exploration to understand the data. This is done by using descriptive statistics and visualizations. We also apply various procedures to adjust the data to make it more suitable for modeling.

** Missing Values
Missing values are a common problem in data. We can deal with missing values in the following ways:
+ Drop the missing values :: In python we can use =df.dropna()= to drop the missing values. This is not a good idea because we are losing a lot of data. We can also use =df.dropna(axis=1)= to drop columns with missing values.
+ Imputation :: We can use imputation to fill in the missing values. We can use the mean, median, or mode to fill in the missing values. We can also use a linear regression to fill in the missing values. We can use =df.fillna()= to fill in the missing values.
** Outliers
Our data might also have outliers. We can check for outliers using =df.boxplot()= but can also make use of the $IQR$.

#+DOWNLOADED: https://alejandrommingo.github.io/SMUCbook/images/EDA/Fig1.png @ 2023-04-13 11:05:06
[[file:./Data_Exploration/2023-04-13_11-05-06_Fig1.png]]

* Model Design
When we create a model, we have to consider three key things:
+ The data :: When it comes to data, it is important to highlight how the data was collected. We also need to consider the quality of the data. We need to make sure that the data is accurate and that it is representative of the population.
+ Response variable :: This is the variable that we are trying to predict. It is also known as the dependent variable.
+ Explantory variables :: These are the variables that we use to predict the response variable. They are also known as the independent variables. Sometimes, there are explanatory variables that hurt the model, these are known as confounding variables.

** Cross Validation
In order to have a good model, it is important not to mix the training and testing data. We can do this using cross validation. [[https://open.substack.com/pub/aisnakeoil/p/gpt-4-and-professional-benchmarks?r=2035qn&utm_campaign=post&utm_medium=web][Unlike OpenAI]]...

This is done by splitting the data into 2 parts, the training data and the testing data. We then use the training data to build the model and the testing data to evaluate the model. We can then repeat this process multiple times to get a better idea of how the model performs.

Here are some common ratios for splitting the data:

| Training Data | Testing Data |
|---------------+--------------|
| 80%           | 20%          |
| 70%           | 30%          |

Why do we do this?
+ Preventing overfitting :: If we use all the data to build the model, we will get a model that is very accurate on the training data. However, this model will not be able to generalize to new data. This is known as overfitting.
+ Preventing underfitting :: If we use too little data to build the model, we will get a model that is not accurate on the training data. This is known as underfitting.

In python we can use the =train_test_split= function from the =sklearn.model_selection= library to split the data.

#+begin_src ipython :eval no
  from sklearn.model_selection import train_test_split

  # Split the data into training and testing data
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
#+end_src

** Cross Validation (LOOCV)
With this method, we go through the data one by one. We use all the data except for the current data point to build the model. We then use the current data point to evaluate the model. We then repeat this process for all the data points.

What this does is it assures a lower probability for bias and variance. In python, we can use the =LeaveOneOut= function from the =sklearn.model_selection= library to do this.

#+begin_src ipython :eval no


  # Split the data into training and testing data
  loo = LeaveOneOut()
  for train_index, test_index in loo.split(X):
      X_train, X_test = X[train_index], X[test_index]
      y_train, y_test = y[train_index], y[test_index]
#+end_src

** Cross Validation (K-Fold)
It is a bit similar to LOOVC but we split the data into =k= parts. We then use =k-1= parts to build the model and the remaining part to evaluate the model. We then repeat this process for all the parts.

In python, we can use the =KFold= function from the =sklearn.model_selection= library to do this.

#+begin_src ipython :eval no
  from sklearn.model_selection import KFold

  # Split the data into training and testing data
  kf = KFold(n_splits=5)
  for train_index, test_index in kf.split(X):
      X_train, X_test = X[train_index], X[test_index]
      y_train, y_test = y[train_index], y[test_index]
#+end_src

* Regression Models
We start of with SLR (Simple linear regression). There are 3 key steps in this process:
1. Build the model
2. Evaluate the model
3. Use the model

We have two types of basic models, deterministic and probabilistic:
+ Deterministic model :: Describes an exact relationship between the independent and dependent variables: $y = \beta_0 + \beta_1 x$
+ Probabilistic model :: It builds ontop of the deterministic model by adding a random component to the model: $y = \beta_0 + \beta_1 x + \epsilon$

** Probabilistic Model
The random component is called the error term, it adds an element of randomness to the model. For an ideal model, the error term should be normally distributed with a mean of 0.

We mix this with a bit of statistics, we have the population parameters $\sigma^2_\epsilon$, $\beta_0$, and $\beta_1$. The best we can do is use estimators: $\hat{\sigma}^2_\epsilon$, $\hat{\beta}_0$, and $\hat{\beta}_1$.

** Least Squares
Say we collect some data from a sample. We now want to build a model that best fits the data. We can do this by minimizing the sum of the squared errors. This is called the least squares method.

The first step in processing this data, is to create a scatter plot of the data. We can then draw a line of best fit through the data. To obtain the equation of that line, we can use the following formulas. We will be using a deterministic example.

The equation:

\[
\bar{y} = \hat{\beta}_0 + \hat{\beta}_1 \bar{x}
\]

Sample data:
| x | y |
|---+---|
| 1 | 1 |
| 2 | 1 |
| 3 | 2 |
| 4 | 2 |
| 5 | 4 |

The formulas to compute $\hat{\beta}_0$ and $\hat{\beta}_1$ are as follows:

\[
\hat{\beta}_1 = \frac{SS_{xy}}{SS_{xx}}
\]

\[
\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}
\]

Where:

\[
SS_{xy} = \sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})
\]

\[
SS_{xx} = \sum_{i=1}^n (x_i - \bar{x})^2
\]

SS means sum of squares. We can compute these values in python using the =numpy= library.


Lets apply this to our example:

#+begin_src ipython :session session01 :exports both :results file
  import numpy as np
  import pandas as pd
  import matplotlib.pyplot as plt
  import seaborn as sns

  # Create the data
  x = np.array([1, 2, 3, 4, 5])
  y = np.array([1, 1, 2, 2, 4])

  # Create the dataframe
  df = pd.DataFrame({'x': x, 'y': y})

  # Create the scatter plot
  sns.scatterplot(x='x', y='y', data=df)
  plt.savefig('scatter_plot.png')
  print("scatter_plot.png")
#+end_src

#+RESULTS:
[[file:./obipy-resources/K8y6k9.png]]

Lets compute the sample mean of x and y:

#+begin_src ipython :session session01 :file ./ipython-TfpviU.png :exports both :results output
  x_bar = np.mean(x)
  y_bar = np.mean(y)
  print(x_bar, y_bar)
#+end_src

#+RESULTS:
: 3.0 2.0

We now have everything to compute the coefficients $\hat{\beta}_0$ and $\hat{\beta}_1$:

#+begin_src ipython :session session01 :file ./ipython-6EN8Mx.png :exports both :results output
  beta_hat_1 = np.sum((x - x_bar) * (y - y_bar)) / np.sum((x - x_bar)**2)
  beta_hat_0 = y_bar - beta_hat_1 * x_bar

  print(f"y = {beta_hat_0} + {beta_hat_1} * x")

#+end_src

#+RESULTS:
: y = -0.09999999999999964 + 0.7 * x

And we can plot this line of best fit:

#+begin_src ipython :session session01 :exports both :results file
  sns.scatterplot(x='x', y='y', data=df)
  plt.plot(x, beta_hat_0 + beta_hat_1 * x, color='red')
  plt.savefig('scatter_plot_with_line.png')
  print("scatter_plot_with_line.png")
#+end_src

#+RESULTS:
[[file:./obipy-resources/ZousAl.png]]

With this, we have created a LSRL (Least Squares Regression Line). We can use this to make predictions. This kind of model, should primarily be used within the range of the data. If we want to make predictions outside of the range of the data, we should use a different model.

Confidence in the model can be measured using the coefficient of determination ($R^2$). This is a measure of how well the model fits the data. The closer the value is to 1, the better the model fits the data. The value is always between 0 and 1.

The formula used to compute $\beta_1$ is:

\[
\beta_1 = \frac{SS_{xy}}{SS_{xx}} = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2} = \frac{Cov(x, y)}{Var(x)}
\]



** Random Error
Might have guessed already, the random error $\epsilon$ is a distribution. Lets start with some assumptions:
+ $\epsilon$ is normally distributed $N(\mu, \sigma^2)$
+ $\epsilon$ is independent of $x$

What is left for us to figure out is the variance of epsilon.
+ We know that it will be constant for all values of $x$ (homoscedasticity) [fn:homoscedasticity: This is a fancy way of saying that the variance is the same for all values of x]

We can use the following formula to compute the variance of $\epsilon$:

\[
\sigma^2_\epsilon = \frac{1}{n-2} \sum_{i=1}^n (y_i - \hat{y}_i)^2
\]


If we return to our example, we can compute the variance of $\epsilon$:

#+begin_src ipython :session session01 :file ./ipython-HRz6dP.png :exports both :results output
  y_hat = beta_hat_0 + beta_hat_1 * x
  epsilon = y - y_hat
  epsilon_var = np.sum((y - y_hat)**2) / (len(x) - 2)
  print(epsilon_var)
#+end_src

#+RESULTS:
: 0.36666666666666664

How do we interpret this result? We use the empirical rule, which tells us that *95% of the observed $y$ values will be within 2 standard deviations of the LSRL*.

With this information, we can build a simple confidence interval: $(\hat{y} - 2\sigma_\epsilon, \hat{y} + 2\sigma_\epsilon)$ which tells us that 95% of the observed $y$ values will be within this interval.

Even better, we can calculate the *mean error*:

\[
me = \frac{s}{\bar{y}} * 100
\]

#+begin_src ipython :session session01  :exports both :results output
  me = np.sqrt(epsilon_var) / y_bar * 100
  print(me)
#+end_src

#+RESULTS:
: 30.276503540974915

From this number, we can infer that *~30% of our estimates are off*.

Summary of assumptions for the error:

| Assumption | Description                                                              |
|------------+--------------------------------------------------------------------------|
|          1 | $\epsilon$ is normally distributed $N(0, \sigma^2)$                                   |
|          2 | $\epsilon$ is independent of $x$                                                |
|          3 | $\epsilon$ is homoscedastic (constant variance)                                 |
|          3 | The influence of some $y$ on $\epsilon$ does not influence any other value $y_1$ |
** Adequacy
We now need to check if the model is adequate. We can do this by checking if the error is different from 0. We can do this by using a t-test. The null hypothesis is that the error is 0, and the alternative hypothesis is that the error is not 0.

\[
H_0: \beta = 0 \\
H_1: \beta \neq 0
\]

We can use the following formula to compute the t-statistic:

\[
t = \frac{\hat{\beta}}{s_{\hat{\beta}}}
\]

Where $\hat{\beta}$ is the estimated coefficient, and $s_{\hat{\beta}}$ is the standard error of the estimated coefficient. With the computed t-statistic, we can compute the p-value using statistical software or python or a table. In python:

#+begin_src ipython :session session01 :exports both :results output :eval no
  t = beta_hat_1 / np.sqrt(epsilon_var / np.sum((x - x_bar)**2))
  print(t)
#+end_src

Based on the p-val we either reject or fail to reject the null hypothesis. If we reject the null hypothesis, we can conclude that the model is adequate.

Another much easier way to find the p-value is the *f-statistic* for the entire model:

\[
H_0: \beta_1 = \beta_2 = \dots = \beta_p = 0 \\
H_1: \beta_1 \neq \beta_2 \neq \dots \neq \beta_p
\]
** Usefulness
We use the $R^2$ to measure how well the model fits the data. The closer the value is to 1, the better the model fits the data. The value is always between 0 and 1.

Definition of r^2
#+begin_quote
The percentage of the variance in the dependent variable that is predictable from the independent variable(s).
#+end_quote

In simpler terms: The percentage of variance is like a way to measure how much one thing affects another. It tells us how likely it is that when one thing changes, the other thing will change too. For example, if you change the amount of sugar you put in your tea, it will change how sweet it is. The percentage of variance tells us how much the sugar affects how sweet the tea is.

** AIC and BIC

** Prediction
We can predict a specific case, or use the error component to get a confidence interval for the prediction.
* Multiple Linear Regression
When we have multiple independent variables, we can use multiple linear regression to predict the value of a dependent variable. The formula for this is:

\[
\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x_1 + \hat{\beta}_2 x_2 + \dots + \hat{\beta}_p x_p
\]

Where $\hat{\beta}_0$ is the intercept, $\hat{\beta}_1$ is the coefficient for $x_1$, $\hat{\beta}_2$ is the coefficient for $x_2$, and so on. $p$ is the number of independent variables.

We can see that this is a deterministic model, because the coefficients are fixed. The coefficients are estimated using the same method as in simple linear regression. To make this model deterministic, we would have to add a random error term to the model.

* Advanced Linear Regression
We start with mummification of data, also called hot-encoding. There are two types of hot-encoding:
+ One-hot encoding
+ Dummy encoding

To better understand these we will use this example data:
| Country | City | Population |
|---------+------+------------|
|   US    |  NY  |    1000    |
|   US    |  LA  |    2000    |
|   US    |  SF  |    3000    |
|   CA    |  TO  |    4000    |
|   CA    |  MO  |    5000    |
|   CA    |  OT  |    6000    |

** Dummy encoding
We parse categorical data into a model. We use some sort of encoding to represent the data. For example, if we have a variable called country, and it has two values: US and CA, we can encode it as:

\begin{align}
Country &= \begin{cases}
1 & \text{if US} \\
0 & \text{if CA}
\end{cases}
\end{align}

We can now use a simple linear regression to predict the population of a city. The formula for this is: $y = \beta_0 + \beta_1 x_1 + \epsilon$. Where $y$ is the population, $\beta_0$ is the intercept, $\beta_1$ is the coefficient for country, and $\epsilon$ is the error term.

** One-hot encoding
One-hot encoding is a way to encode categorical variables. It is called one-hot because only one of the values is hot (1), and the rest are cold (0). For example, if we have a variable called country, and it has two values: US and CA, we can encode it as:

| City | Country_US | Country_CA |
|------+------------+------------|
|  NY  |      1     |      0     |
|  LA  |      1     |      0     |
|  SF  |      1     |      0     |
|  TO  |      0     |      1     |
|  MO  |      0     |      1     |
|  OT  |      0     |      1     |

This method is better for data with categories of more than two. For example, if we have a variable called color, and it has three values: red, blue, and green. This process is very commonly used in machine learning models to encode some data that might generally be hard to encode or pass to a model. We can also use it to *combine categorical and numerical data* into a model.
* Classification Models
