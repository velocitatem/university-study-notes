{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell<sub>type</sub>\": \"markdown\",\n",
    "   \"id\": \"916e7bd1-eaa7-4eec-b2f9-7cf12826e876\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"<script data-name=\\\\\"BMC-Widget\\\\\" data-cfasync=\\\\\"false\\\\\" src=\\\\\"[https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js](https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js)\\\\\" data-id=\\\\\"velocitatem24\\\\\" data-description=\\\\\"Support me on Buy me a coffee!\\\\\" data-message=\\\\\"\\\\\" data-color=\\\\\"#5F7FFF\\\\\" data-position=\\\\\"Right\\\\\" data-x<sub>margin</sub>=\\\\\"18\\\\\" data-y<sub>margin</sub>=\\\\\"18\\\\\"></script>\\n\",\n",
    "    \"<script>setTimeout(() ``> {alert(\\\"Finding this useful? Consider buying me a coffee! Bottom right cornner :) Takes just a few seconds\\\")}, 60*1000);_paq.push(['trackEvent', 'Exposure', 'Exposed to beg']);</script>\\n\",\n",
    "    \"\\n\",\n",
    "    \"**New** search feature! Make use of the amazing [fuzzy\\n\",\n",
    "    \"search](https://en.wikipedia.org/wiki/Approximate_string_matching)\\n\",\n",
    "    \"algorithm. Just type in the search box and it will find the closest\\n\",\n",
    "    \"match in the page. Hit `Enter` to jump to the next match. Lmk if it\\n\",\n",
    "    \"doesn't work for you.\\n\",\n",
    "    \"\\n\",\n",
    "    \"<input id``\\\\\"search\\\\\" type=\\\\\"text\\\\\" placeholder=\\\\\"Search\\\\\" *> <span id=\\\\\"resultCount\\\\\"></span>\\n\",\n",
    "    \"<script src=\\\\\"[https://alves.world/fuzzy.js](https://alves.world/fuzzy.js)\\\\\"></script>\\n\",\n",
    "    \"\\n\",\n",
    "    \"Useful Resources:\\n\",\n",
    "    \"\\n\",\n",
    "    \"-   [Seeing Theory - Frequentist\\n\",\n",
    "    \"    Inference]([https://seeing-theory.brown.edu/frequentist-inference/index.html#section1](https://seeing-theory.brown.edu/frequentist-inference/index.html#section1))\\n\",\n",
    "    \"-   [Penn State Notes]([https://online.stat.psu.edu/stat415](https://online.stat.psu.edu/stat415)*)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Data Manipulation & Python\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Pandas DataFrames\\n\",\n",
    "    \"\\n\",\n",
    "    \"It is best to imagine a DataFrame as spreadsheet.\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`\\`\\` python\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create a DataFrame from a dictionary\\n\",\n",
    "    \"df = pd.DataFrame({'name': ['John', 'Jane', 'Joe'],\\n\",\n",
    "    \"                  'age': [34, 25, 67],\\n\",\n",
    "    \"                  'height': [1.78, 1.65, 1.89]})\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create a DataFrame from a list of lists\\n\",\n",
    "    \"df = pd.DataFrame([['John', 34, 1.78],\\n\",\n",
    "    \"                  ['Jane', 25, 1.65],\\n\",\n",
    "    \"                  ['Joe', 67, 1.89]],\\n\",\n",
    "    \"                  columns=['name', 'age', 'height'])\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`\\`\\`\\n\",\n",
    "    \"\\n\",\n",
    "    \"Here are some of the most useful methods for working with DataFrames:\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`.head()\\`  \\n\",\n",
    "    \"returns the first 5 rows of the DataFrame\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`.tail()\\`  \\n\",\n",
    "    \"returns the last 5 rows of the DataFrame\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`.transpose()\\`  \\n\",\n",
    "    \"returns the transpose of the DataFrame\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`.plot.scatter(*cols*)\\`  \\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`.shape\\`  \\n\",\n",
    "    \"returns the number of rows and columns in the DataFrame\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`.dtypes\\`  \\n\",\n",
    "    \"returns the data types of each column in the DataFrame\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Filtering DataFrames\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`\\`\\` python\\n\",\n",
    "    \"# Create a DataFrame from a dictionary\\n\",\n",
    "    \"df = pd.DataFrame({'name': ['John', 'Jane', 'Joe'],\\n\",\n",
    "    \"                  'age': [34, 25, 67],\\n\",\n",
    "    \"                  'height': [1.78, 1.65, 1.89]})\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Filter the DataFrame to only include people over 30\\n\",\n",
    "    \"df[df['age'] > 30]\\n\",\n",
    "    \"\\`\\`\\`\\n\",\n",
    "    \"\\n\",\n",
    "    \"The general syntax for filtering a DataFrame is: \\`df[condition]\\`. The\\n\",\n",
    "    \"\\`condition\\` is a boolean expression that evaluates to either \\`True\\` or\\n\",\n",
    "    \"\\`False\\` for each row in the DataFrame. The result of the filter is a new\\n\",\n",
    "    \"DataFrame containing only the rows where the condition is \\`True\\`.\\n\",\n",
    "    \"\\n\",\n",
    "    \"We can also use the \\`.loc\\` method to filter a DataFrame. The \\`.loc\\`\\n\",\n",
    "    \"method takes a list of row labels and a list of column labels as\\n\",\n",
    "    \"arguments. The result is a new DataFrame containing only the rows and\\n\",\n",
    "    \"columns specified. Here is an example:\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`\\`\\` python\\n\",\n",
    "    \"# Create a DataFrame from a dictionary\\n\",\n",
    "    \"df = pd.DataFrame({'name': ['John', 'Jane', 'Joe'],\\n\",\n",
    "    \"                  'age': [34, 25, 67],\\n\",\n",
    "    \"                  'height': [1.78, 1.65, 1.89]})\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Filter the DataFrame to only include people over 30\\n\",\n",
    "    \"df.loc[df['age'] > 30, ['name', 'age']]\\n\",\n",
    "    \"\\`\\`\\`\\n\",\n",
    "    \"\\n\",\n",
    "    \"The main difference between the \\`.loc\\` method and the \\`[]\\` operator is\\n\",\n",
    "    \"that the \\`.loc\\` method can be used to filter rows and columns at the\\n\",\n",
    "    \"same time. We might want to do that if we want to filter a DataFrame by\\n\",\n",
    "    \"rows and then select a subset of the columns.\\n\",\n",
    "    \"\\n\",\n",
    "    \"## \\`np.where()\\`\\n\",\n",
    "    \"\\n\",\n",
    "    \"The \\`np.where()\\` function is a vectorized version of the \\`if\\` statement.\\n\",\n",
    "    \"It takes a condition, a value to return if the condition is \\`True\\`, and\\n\",\n",
    "    \"a value to return if the condition is \\`False\\`. Here is an example:\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`\\`\\` python\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create a NumPy array\\n\",\n",
    "    \"arr = np.array([1, 2, 3, 4, 5])\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Use np.where() to replace all values less than 3 with 0\\n\",\n",
    "    \"res = np.where(arr < 3, 0, arr)\\n\",\n",
    "    \"print(res)\\n\",\n",
    "    \"\\`\\`\\`\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`\\`\\` example\\n\",\n",
    "    \"[0 0 3 4 5]\\n\",\n",
    "    \"\\`\\`\\`\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Sorting\\n\",\n",
    "    \"\\n\",\n",
    "    \"We can use the \\`.sort<sub>values</sub>()\\` method to sort a DataFrame by one or\\n\",\n",
    "    \"more columns. Here is an example:\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`\\`\\` python\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"# Create a DataFrame from a dictionary\\n\",\n",
    "    \"df = pd.DataFrame({'name': ['John', 'Jane', 'Joe'],\\n\",\n",
    "    \"                  'age': [34, 25, 67],\\n\",\n",
    "    \"                  'height': [1.78, 1.65, 1.89]})\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Sort the DataFrame by age\\n\",\n",
    "    \"print(df.sort<sub>values</sub>('age'))\\n\",\n",
    "    \"\\`\\`\\`\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`\\`\\` example\\n\",\n",
    "    \"   name  age  height\\n\",\n",
    "    \"1  Jane   25    1.65\\n\",\n",
    "    \"0  John   34    1.78\\n\",\n",
    "    \"2   Joe   67    1.89\\n\",\n",
    "    \"\\`\\`\\`\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Grouping\\n\",\n",
    "    \"\\n\",\n",
    "    \"To avoid redundant filtering and aggregation, we can use the\\n\",\n",
    "    \"\\`.groupby()\\` method to group a DataFrame by one or more columns. Here is\\n\",\n",
    "    \"an example:\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`\\`\\` python\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"# Create a DataFrame from a dictionary\\n\",\n",
    "    \"df = pd.DataFrame({'name': ['John', 'Jane', 'Joe'],\\n\",\n",
    "    \"                  'age': [34, 25, 67],\\n\",\n",
    "    \"                  'gender': [\\\\\"M\\\\\", \\\\\"F\\\\\", \\\\\"M\\\\\"],\\n\",\n",
    "    \"                  'height': [1.78, 1.65, 1.89]})\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Group the DataFrame by gender\\n\",\n",
    "    \"print(df.groupby('gender').describe())\\n\",\n",
    "    \"# Group the DataFrame by gender and calculate the mean of each group\\n\",\n",
    "    \"print(df.groupby('gender').mean())\\n\",\n",
    "    \"# calculate the mean age for each gender\\n\",\n",
    "    \"print(df.groupby('gender')['age'].mean())\\n\",\n",
    "    \"\\`\\`\\`\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`\\`\\` example\\n\",\n",
    "    \"         age                                &#x2026; height\\n\",\n",
    "    \"       count  mean        std   min    25%  &#x2026;    min     25%    50%     75%   max\\n\",\n",
    "    \"gender                                      &#x2026;\\n\",\n",
    "    \"F        1.0  25.0        NaN  25.0  25.00  &#x2026;   1.65  1.6500  1.650  1.6500  1.65\\n\",\n",
    "    \"M        2.0  50.5  23.334524  34.0  42.25  &#x2026;   1.78  1.8075  1.835  1.8625  1.89\\n\",\n",
    "    \"\\n\",\n",
    "    \"[2 rows x 16 columns]\\n\",\n",
    "    \"         age  height\\n\",\n",
    "    \"gender\\n\",\n",
    "    \"F       25.0   1.650\\n\",\n",
    "    \"M       50.5   1.835\\n\",\n",
    "    \"gender\\n\",\n",
    "    \"F    25.0\\n\",\n",
    "    \"M    50.5\\n\",\n",
    "    \"Name: age, dtype: float64\\n\",\n",
    "    \"\\`\\`\\`\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Python: Descriptive Statistics\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`\\`\\` python\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"plt.style.use(\\\\\"seaborn\\\\\")\\n\",\n",
    "    \"\\`\\`\\`\\n\",\n",
    "    \"\\n\",\n",
    "    \"We will be\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Histograms\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`\\`\\` python\\n\",\n",
    "    \"df['some<sub>values</sub>'].hist(bins=15, edgecolor='white')\\n\",\n",
    "    \"\\`\\`\\`\\n\",\n",
    "    \"\\n\",\n",
    "    \"We can also set some other parameters such as the title and labels:\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`\\`\\` python\\n\",\n",
    "    \"plt.title('Some Title')\\n\",\n",
    "    \"plt.xlabel('Some X Label')\\n\",\n",
    "    \"plt.ylabel('Some Y Label')\\n\",\n",
    "    \"\\`\\`\\`\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Histograms: Side by Side\\n\",\n",
    "    \"\\n\",\n",
    "    \"If we have two different groups of data, we can plot them side by side:\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`\\`\\` python\\n\",\n",
    "    \"group1 = DataFrame\\n\",\n",
    "    \"group2 = DataFrame\\n\",\n",
    "    \"plt.hist([group1, group2], bins=15, edgecolor='white', label=['Group 1', 'Group 2'])\\n\",\n",
    "    \"plt.legend()\\n\",\n",
    "    \"\\`\\`\\`\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Bar Plots\\n\",\n",
    "    \"\\n\",\n",
    "    \"We can also plot bar plots (they are very similar to histograms, but\\n\",\n",
    "    \"plot the frequency of categorical data):\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`\\`\\` python\\n\",\n",
    "    \"categories = ['A', 'B', 'C', 'D']\\n\",\n",
    "    \"frequencies = [10, 20, 30, 40]\\n\",\n",
    "    \"plt.bar(categories, frequencies, edgecolor='white')\\n\",\n",
    "    \"\\`\\`\\`\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Box Plots\\n\",\n",
    "    \"\\n\",\n",
    "    \"Box plots are a great way to visualize the distribution of data. They\\n\",\n",
    "    \"are very useful for comparing different groups of data.\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`\\`\\` python\\n\",\n",
    "    \"plt.boxplot([group1, group2])\\n\",\n",
    "    \"plt.xticks([1, 2], ['Group 1', 'Group 2'])\\n\",\n",
    "    \"\\`\\`\\`\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Annotations\\n\",\n",
    "    \"\\n\",\n",
    "    \"We can also add annotations to our plots:\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`\\`\\` python\\n\",\n",
    "    \"plt.annotate('Some Text', xy=(x, y), xytext=(x, y), arrowprops={'arrowstyle': '->'})\\n\",\n",
    "    \"\\`\\`\\`\\n\",\n",
    "    \"\\n\",\n",
    "    \"The \\`xy\\` and \\`xytext\\` parameters are the coordinates of the text and the\\n\",\n",
    "    \"arrow, respectively.\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Centrality and Spread\\n\",\n",
    "    \"\\n\",\n",
    "    \"We can use the \\`mean\\` and \\`median\\` functions to calculate the mean and\\n\",\n",
    "    \"median of a dataset:\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`\\`\\` python\\n\",\n",
    "    \"mean = df['some<sub>values</sub>'].mean()\\n\",\n",
    "    \"median = df['some<sub>values</sub>'].median()\\n\",\n",
    "    \"\\`\\`\\`\\n\",\n",
    "    \"\\n\",\n",
    "    \"We can also use the \\`std\\` function to calculate the standard deviation:\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`\\`\\` python\\n\",\n",
    "    \"std = df['some<sub>values</sub>'].std()\\n\",\n",
    "    \"\\`\\`\\`\\n\",\n",
    "    \"\\n\",\n",
    "    \"To get a summary of the descriptive statistics of a dataset, we can use\\n\",\n",
    "    \"the \\`describe\\` function:\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`\\`\\` python\\n\",\n",
    "    \"df['some<sub>values</sub>'].describe()\\n\",\n",
    "    \"\\`\\`\\`\\n\",\n",
    "    \"\\n\",\n",
    "    \"All of these functions are methods on the DataFrame object.\\n\",\n",
    "    \"\\n\",\n",
    "    \"Minimum  \\n\",\n",
    "    \"\\`df['some<sub>values</sub>'].min()\\`\\n\",\n",
    "    \"\\n\",\n",
    "    \"Quartile  \\n\",\n",
    "    \"\\`df['some<sub>values</sub>'].quantile(0.25)\\`\\n\",\n",
    "    \"\\n\",\n",
    "    \"IQR  \\n\",\n",
    "    \"\\`df['some<sub>values</sub>'].quantile(0.75) - df['some<sub>values</sub>'].quantile(0.25)\\`\\n\",\n",
    "    \"\\n\",\n",
    "    \"Mode  \\n\",\n",
    "    \"\\`df['some<sub>values</sub>'].mode()\\`\\n\",\n",
    "    \"\\n\",\n",
    "    \"Skew  \\n\",\n",
    "    \"\\`df['some<sub>values</sub>'].skew()\\`\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Using \\`numpy\\`\\n\",\n",
    "    \"\\n\",\n",
    "    \"For each of the following methods, we need to pass the dataframe column\\n\",\n",
    "    \"as a numpy array:\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`np.mean\\`  \\n\",\n",
    "    \"The mean of the array\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`np.median\\`  \\n\",\n",
    "    \"The median of the array\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`np.std\\`  \\n\",\n",
    "    \"The standard deviation of the array\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`np.var\\`  \\n\",\n",
    "    \"The variance of the array\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`np.percentile\\`  \\n\",\n",
    "    \"The percentile of the array\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`np.quantile\\`  \\n\",\n",
    "    \"The quantile of the array\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`np.corrcoef\\`  \\n\",\n",
    "    \"The correlation coefficient of the array\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Using \\`scipy.stats\\`\\n\",\n",
    "    \"\\n\",\n",
    "    \"Here we assume it is imported as \\`ss\\`. We can use the following methods:\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`ss.mode\\`  \\n\",\n",
    "    \"The mode of the array\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`ss.skew\\`  \\n\",\n",
    "    \"The skew of the array\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`ss.iqr\\`  \\n\",\n",
    "    \"The interquartile range of the array\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`ss.pearsonr\\`  \\n\",\n",
    "    \"The Pearson correlation coefficient of two arrays\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Statistical Distributions\\n\",\n",
    "    \"\\n\",\n",
    "    \"A statistic is a metric, which can be calculated for any sample. Before\\n\",\n",
    "    \"that sample is collected, we do not know what the values are going to\\n\",\n",
    "    \"be. That is why we can represent a statistic as a ****random variable****.\\n\",\n",
    "    \"\\n\",\n",
    "    \"For example, the sample mean of a distribution, before we actually take\\n\",\n",
    "    \"the samples, is going to be $\\\\bar{X}$. Once we take the samples, and\\n\",\n",
    "    \"calculate the statistics, we get $\\\\bar{x}$.\\n\",\n",
    "    \"\\n\",\n",
    "    \"Since any statistic can also be a random variable, we can make\\n\",\n",
    "    \"distributions for these random variables. This distribution, is called\\n\",\n",
    "    \"the ****sampling distribution****.\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Random Samples\\n\",\n",
    "    \"\\n\",\n",
    "    \"<center>\\n\",\n",
    "    \"<iframe class=\\\\\"centeredIframe\\\\\" scrolling=\\\\\"NO\\\\\" src=\\\\\"[https://www.statcrunch.com/app/index.html?launch=scapps&amp;app=samplingdist&amp;dist=uniform&amp;firststat=0&amp;secondstat=1](https://www.statcrunch.com/app/index.html?launch=scapps&amp;app=samplingdist&amp;dist=uniform&amp;firststat=0&amp;secondstat=1)\\\\\" title=\\\\\"Sampling distributions applet\\\\\" width=\\\\\"650\\\\\" height=\\\\\"700\\\\\" frameborder=\\\\\"0\\\\\"></iframe>\\n\",\n",
    "    \"</center>\\n\",\n",
    "    \"\\n\",\n",
    "    \"So what determines the distribution of a statistic? It is determined by\\n\",\n",
    "    \"the ****random samples**** that we take from the population. If we take a\\n\",\n",
    "    \"random sample from a population, and calculate the statistic, we get a\\n\",\n",
    "    \"value. If we take another random sample, and calculate the statistic, we\\n\",\n",
    "    \"get another value. And so on.\\n\",\n",
    "    \"\\n\",\n",
    "    \"The key factors which determine the distribution of a statistic are:\\n\",\n",
    "    \"\\n\",\n",
    "    \"-   The size of the sample\\n\",\n",
    "    \"-   The distribution of the population\\n\",\n",
    "    \"-   Sampling method\\n\",\n",
    "    \"\\n\",\n",
    "    \"For our sample to be representative or valid, they must be\\n\",\n",
    "    \"****independent**** and ****identically distributed****. This means that the\\n\",\n",
    "    \"samples must be independent of each other, and the distribution of the\\n\",\n",
    "    \"population must be the same for each sample.\\n\",\n",
    "    \"\\n\",\n",
    "    \"These conditions will be satisfied if:\\n\",\n",
    "    \"\\n\",\n",
    "    \"-   We have no replacement\\n\",\n",
    "    \"-   We have a large enough sample size\\n\",\n",
    "    \"\\n\",\n",
    "    \"Generally, if at most, we sample 5% of the populations, we can assume\\n\",\n",
    "    \"that the X<sub>i</sub> distribution is a random sample.\\n\",\n",
    "    \"\\n\",\n",
    "    \"Here is an implementation of the example 5.12 from the book:\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`\\`\\` python\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"sns.set<sub>style</sub>('whitegrid')\\n\",\n",
    "    \"mu = 106\\n\",\n",
    "    \"variance = 244\\n\",\n",
    "    \"sigma = np.sqrt(variance)\\n\",\n",
    "    \"og<sub>population</sub> = {\\n\",\n",
    "    \"    80: 0.2,\\n\",\n",
    "    \"    100: 0.3,\\n\",\n",
    "    \"    120: 0.5\\n\",\n",
    "    \"}\\n\",\n",
    "    \"samples = np.arange(10, 110, 30)\\n\",\n",
    "    \"fig, axes = plt.subplots(1, len(samples), figsize=(15, 5))\\n\",\n",
    "    \"for sampleSize in samples:\\n\",\n",
    "    \"    sample<sub>means</sub> = []\\n\",\n",
    "    \"    for i in range(1000):\\n\",\n",
    "    \"        sample = np.random.choice(list(og<sub>population.keys</sub>()), size=sampleSize, p=list(og<sub>population.values</sub>()))\\n\",\n",
    "    \"        sample<sub>mean</sub> = np.mean(sample)\\n\",\n",
    "    \"        sample<sub>means.append</sub>(sample<sub>mean</sub>)\\n\",\n",
    "    \"    sns.distplot(sample<sub>means</sub>, ax=axes[samples.tolist().index(sampleSize)])\\n\",\n",
    "    \"    axes[samples.tolist().index(sampleSize)].set<sub>title</sub>('Sample Size: {}'.format(sampleSize))\\n\",\n",
    "    \"    axes[samples.tolist().index(sampleSize)].set<sub>xlabel</sub>('Sample Mean')\\n\",\n",
    "    \"    axes[samples.tolist().index(sampleSize)].set<sub>ylabel</sub>('Probability')\\n\",\n",
    "    \"plt.show()\\n\",\n",
    "    \"\\`\\`\\`\\n\",\n",
    "    \"\\n\",\n",
    "    \"And here is the output:\\n\",\n",
    "    \"\\n\",\n",
    "    \"<span class=\\\\\"image placeholder\\\\\"\\n\",\n",
    "    \"original-image-src=\\\\\"./sampling-distributions-5.21-extra.png\\\\\"\\n\",\n",
    "    \"original-image-title=\\\\\"\\\\\"></span>\\n\",\n",
    "    \"\\n\",\n",
    "    \"You can see that as the sample size increases, the distribution of the\\n\",\n",
    "    \"sample means becomes more normal (I think).\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Derivation\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let's say we have a population with a mean of $\\\\mu$, a standard\\n\",\n",
    "    \"deviation of $\\\\sigma$ and any probability distribution. We take a random\\n\",\n",
    "    \"sample of size $n$ from this population. We calculate the sample mean,\\n\",\n",
    "    \"and we get $\\\\bar{x}$. We can represent this as a random variable,\\n\",\n",
    "    \"$\\\\bar{X}$. We have to consider all the possible values of $\\\\bar{x}$, and\\n\",\n",
    "    \"their probabilities. From this, we can then calculate the distribution\\n\",\n",
    "    \"of $\\\\bar{X}$.\\n\",\n",
    "    \"\\n\",\n",
    "    \"To now calculate the statistics for the distribution of $\\\\bar{X}$, we\\n\",\n",
    "    \"can use the following formulas\\n\",\n",
    "    \"\\n\",\n",
    "    \"Mean  \\n\",\n",
    "    \"$\\\\&mu;<sub>\\\\\\bar{X}</sub> = \\\\&mu;$\\n\",\n",
    "    \"\\n\",\n",
    "    \"Variance  \\n\",\n",
    "    \"$\\\\sigma_{\\\\bar{X}}^2 = \\\\frac{\\\\sigma^2}{n}$ (this is also called the\\n\",\n",
    "    \"****standard error \\\\$$se\\$$****)\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Sample Mean\\n\",\n",
    "    \"\\n\",\n",
    "    \"The sample mean is the most common statistic. It is the average of the\\n\",\n",
    "    \"sample. It is also the most common statistic to use in hypothesis\\n\",\n",
    "    \"testing.\\n\",\n",
    "    \"\\n\",\n",
    "    \"We previously defined the mean and variance for sampling distributions.\\n\",\n",
    "    \"Now we change that up a bit. We first sum up all the random statistics\\n\",\n",
    "    \"$T_O = X_0 + X_1 + \\\\dots + X_n$. From there on, we can get the expected\\n\",\n",
    "    \"value and variance of this ****sample total****:\\n\",\n",
    "    \"\\n\",\n",
    "    \"Expected Value  \\n\",\n",
    "    \"$E(T<sub>O</sub>) `n \\\\mu$\\n\",\n",
    "    \"\\n\",\n",
    "    \"Variance  \\n\",\n",
    "    \"$V(T_O) = n \\\\sigma^2$\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Central Limit Theorem\\n\",\n",
    "    \"\\n\",\n",
    "    \"The central limit theorem states that the sampling distribution of the\\n\",\n",
    "    \"sample mean will be approximately normal, as long as the sample size is\\n\",\n",
    "    \"large enough.\\n\",\n",
    "    \"\\n\",\n",
    "    \"<span class`\\\\\"image placeholder\\\\\"\\n\",\n",
    "    \"original-image-src=\\\\\"./Statistical<sub>Distributions</sub>/2023-02-07<sub>13</sub>-00-52<sub>.png</sub>\\\\\"\\n\",\n",
    "    \"original-image-title=\\\\\"\\\\\"></span>\\n\",\n",
    "    \"\\n\",\n",
    "    \"| Population | Sample Size | Sample |\\n\",\n",
    "    \"|-------&#x2013;&#x2014;|--------&#x2013;&#x2014;|---&#x2013;&#x2014;|\\n\",\n",
    "    \"| Normal     | Any         | Normal |\\n\",\n",
    "    \"| Unknown    | Huge        | Normal |\\n\",\n",
    "    \"\\n\",\n",
    "    \"We can infer from the CLT, that with a higher $n$, we will have a lower\\n\",\n",
    "    \"standard error.\\n\",\n",
    "    \"\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\\\sigma_{\\\\bar{X}} = \\\\frac{\\\\sigma}{\\\\sqrt{n}}\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\n\",\n",
    "    \"The conditions to satisfy the CLT if the population is not normal are:\\n\",\n",
    "    \"\\n\",\n",
    "    \"-   The population must be ****unimodal****\\n\",\n",
    "    \"-   The sample size must be large enough (usually $n \\\\geq 30$)\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Linear Combinations\\n\",\n",
    "    \"\\n\",\n",
    "    \"If we have a random variable $X$, and come constants $c$, we can define\\n\",\n",
    "    \"a new random variable $Y$ as a linear combination of $X$ and $c$:\\n\",\n",
    "    \"\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"Y = c_1 X_1 + c_2 X_2 + \\\\dots + c_n X_n\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\n\",\n",
    "    \"Where the expected value and variance of $Y$ are:\\n\",\n",
    "    \"\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"E(Y) = c_1 E(X_1) + c_2 E(X_2) + \\\\dots + c_n E(X_n)\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"V(Y) = c_1^2 V(X_1) + c_2^2 V(X_2) + \\\\dots + c_n^2 V(X_n)\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\n\",\n",
    "    \"For the above, we assume that the $X_i$ are independent of each other.\\n\",\n",
    "    \"If they are not, we have to add the covariance terms.\\n\",\n",
    "    \"\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"V(Y) = c_1^2 V(X_1) + c_2^2 V(X_2) + \\\\dots + c_n^2 V(X_n) + 2c_1c_2Cov(X_1, X_2) + \\\\dots + 2c_1c_nCov(X_1, X_n) + \\\\dots + 2c_2c_nCov(X_2, X_n)\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Point Estimation\\n\",\n",
    "    \"\\n\",\n",
    "    \"With point estimation, we are trying to estimate a single value, which\\n\",\n",
    "    \"is the best estimate of the population parameter. We can use the sample\\n\",\n",
    "    \"statistics to do this.\\n\",\n",
    "    \"\\n\",\n",
    "    \"The core idea is that if we take a random sample from a population, and\\n\",\n",
    "    \"calculate the sample statistics, ****also a random variable****, we can use\\n\",\n",
    "    \"that to estimate the population parameter.\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Properties\\n\",\n",
    "    \"\\n\",\n",
    "    \"Generally, any estimator $\\\\hat{\\\\theta}$ is just a function of the\\n\",\n",
    "    \"population parameter $\\\\theta$.\\n\",\n",
    "    \"\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\\\hat{\\\\theta} = \\\\theta + \\\\epsilon\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\n\",\n",
    "    \"Where $\\\\epsilon$ is the error term. This error term is the difference\\n\",\n",
    "    \"between the estimator and the actual population parameter.\\n\",\n",
    "    \"\\n\",\n",
    "    \"A way to measure the ****accuracy**** of an estimator is to use the ****mean\\n\",\n",
    "    \"squared error****:\\n\",\n",
    "    \"\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"MSE = \\\\frac{1}{n} \\\\sum_{i=1}^{n} (\\\\hat{\\\\theta} - \\\\theta)^2\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\n\",\n",
    "    \"The smaller the MSE, the better the estimator.\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Estimator Bias\\n\",\n",
    "    \"\\n\",\n",
    "    \"An estimator is unbiased only if the expected value of the estimator is\\n\",\n",
    "    \"equal to the population parameter. This is represented by the following\\n\",\n",
    "    \"formula:\\n\",\n",
    "    \"\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"E(\\\\hat{\\\\theta}) = \\\\theta\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\n\",\n",
    "    \"If there is any difference, that difference is the bias of the\\n\",\n",
    "    \"estimator.\\n\",\n",
    "    \"\\n\",\n",
    "    \"If $X$ is a random variable given by a ****binomial**** distribution, then\\n\",\n",
    "    \"$\\\\hat{p} = \\\\frac{X}{n}$ is an unbiased estimator of $p$.\\n\",\n",
    "    \"\\n\",\n",
    "    \"We previously defined the estimate for the mean, now lets take a look at\\n\",\n",
    "    \"the estimate for the variance:\\n\",\n",
    "    \"\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\\\hat{\\\\sigma}^2 = \\\\frac{1}{n-1} \\\\sum_{i=1}^{n} (x_i - \\\\bar{x})^2\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\n\",\n",
    "    \"This is an unbiased estimator of $\\\\sigma^2$. This is because the\\n\",\n",
    "    \"expected value of the estimator is equal to the population parameter:\\n\",\n",
    "    \"$E(\\\\hat{\\\\sigma}^2) = \\\\sigma^2$. Really? How? Well, we can use the\\n\",\n",
    "    \"****linearity of expectation**** to show that:\\n\",\n",
    "    \"\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"E(S^2)=E\\\\left[\\\\dfrac{\\\\sigma^2}{n-1}\\\\cdot \\\\dfrac{(n-1)S^2}{\\\\sigma^2}\\\\right]=\\\\dfrac{\\\\sigma^2}{n-1} E\\\\left[\\\\dfrac{(n-1)S^2}{\\\\sigma^2}\\\\right]=\\\\dfrac{\\\\sigma^2}{n-1}\\\\cdot (n-1)=\\\\sigma^2\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\n\",\n",
    "    \"Here it is demonstrated in python:\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`\\`\\` python\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Generate 1000 samples from a normal distribution\\n\",\n",
    "    \"samples = np.random.normal(0, 1, 1000)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Calculate the sample mean\\n\",\n",
    "    \"sample<sub>mean</sub> = np.mean(samples)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Calculate the sample variance\\n\",\n",
    "    \"sample<sub>variance</sub> = np.var(samples, ddof=1)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# linearity of expectation\\n\",\n",
    "    \"E<sub>hat</sub><sub>sigma</sub><sub>squared</sub> = (1/(len(samples)-1)) \\* np.sum(np.var(samples, ddof=1))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Print the results\\n\",\n",
    "    \"print(\\\\\"Sample Mean: {}\\\\\".format(sample<sub>mean</sub>))\\n\",\n",
    "    \"print(\\\\\"Sample Variance: {}\\\\\".format(sample<sub>variance</sub>))\\n\",\n",
    "    \"print(\\\\\"E(hat(sigma)<sup>2</sup>): {}\\\\\".format(E<sub>hat</sub><sub>sigma</sub><sub>squared</sub>))\\n\",\n",
    "    \"\\`\\`\\`\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Minimum Variance Estimators\\n\",\n",
    "    \"\\n\",\n",
    "    \"We look at all the unbiased estimators of $\\\\theta$, and we choose the\\n\",\n",
    "    \"one with the smallest variance. This is called the ****minimum variance\\n\",\n",
    "    \"estimator****.\\n\",\n",
    "    \"\\n\",\n",
    "    \"-   The less variance, the more accurate the estimator\\n\",\n",
    "    \"\\n\",\n",
    "    \"The primary influence over the estimator, is still the original\\n\",\n",
    "    \"distribution.\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Estimator Reporting\\n\",\n",
    "    \"\\n\",\n",
    "    \"When we report an estimator, we have to report the ****standard error**** of\\n\",\n",
    "    \"the estimator. This is the standard deviation of the estimator.\\n\",\n",
    "    \"\\n\",\n",
    "    \"$\\\\hat{\\\\theta}$ has a normal distribution  \\n\",\n",
    "    \"The value of $\\\\theta$ lies within $\\\\pm 2 se$ of $\\\\\\hat{\\\\theta}$\\n\",\n",
    "    \"\\n\",\n",
    "    \"$\\\\hat{\\\\theta}$ has a non-normal distribution  \\n\",\n",
    "    \"The value of $\\\\theta$ lies within $\\\\pm 4 se$ of $\\\\\\hat{\\\\theta}$\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Point Estimation (Methods)\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Method of Moments\\n\",\n",
    "    \"\\n\",\n",
    "    \"The method of moments is a method to estimate the parameters of a\\n\",\n",
    "    \"distribution. We use the sample moments to estimate the population\\n\",\n",
    "    \"moments. In simpler terms, we use the sample statistics to estimate the\\n\",\n",
    "    \"population parameters.\\n\",\n",
    "    \"\\n\",\n",
    "    \"-   What is a moment? A moment is a function of the random variable $X$:\\n\",\n",
    "    \"    $E(X^k)$ (where $k$ is the order of the moment)\\n\",\n",
    "    \"\\n\",\n",
    "    \"The way we go about this is by using the following formula:\\n\",\n",
    "    \"\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\\\hat{\\\\theta} = \\\\frac{1}{n} \\\\sum_{i=1}^{n} x_i^k\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\n\",\n",
    "    \"Where $k$ is the order of the moment, and $x_i$ is the $i<sup>th</sup>$\\n\",\n",
    "    \"observation.\\n\",\n",
    "    \"\\n\",\n",
    "    \"Example\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let $X$ be a random variable with a normal distribution with mean $\\\\&mu;$\\n\",\n",
    "    \"and variance $\\\\sigma^2$. We take a random sample of size $n$ from the\\n\",\n",
    "    \"population, and calculate the sample mean $\\\\bar{x}$. We want to estimate\\n\",\n",
    "    \"$\\\\mu$ using the method of moments.\\n\",\n",
    "    \"\\n\",\n",
    "    \"Solution:\\n\",\n",
    "    \"\\n\",\n",
    "    \"The first step to solving this problem is to find the sample mean\\n\",\n",
    "    \"$\\\\bar{x}$:\\n\",\n",
    "    \"\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\\\bar{x} = \\\\frac{1}{n} \\\\sum_{i=1}^{n} x_i\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\n\",\n",
    "    \"In the above equation we can clearly demonstrate how the method of\\n\",\n",
    "    \"moments applies. In fact, the definition of the method of moments, if we\\n\",\n",
    "    \"set $k=1$ is the mean of the sample: $\\\\frac{1}{n} \\\\sum_{i=1}^{n} x_i$.\\n\",\n",
    "    \"\\n\",\n",
    "    \"The next step is to find the sample variance $s^2$:\\n\",\n",
    "    \"\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"s^2 = \\\\frac{1}{n-1} \\\\sum_{i=1}^{n} (x_i - \\\\bar{x})^2\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\n\",\n",
    "    \"For the above, we can see that the method of moments applies again. If\\n\",\n",
    "    \"we set $k=2$ and consider $x_i$ to be $x_i - \\\\bar{x}$, we get the sample\\n\",\n",
    "    \"variance. There is of course the matter of the $-1$ in the denominator,\\n\",\n",
    "    \"this can be explained by the fact that the sample variance is an\\n\",\n",
    "    \"****unbiased**** estimator of the population variance.\\n\",\n",
    "    \"\\n\",\n",
    "    \"Now we can use the method of moments to estimate $\\\\mu$:\\n\",\n",
    "    \"\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\\\hat{\\\\mu} = \\\\bar{x}\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Maximum Likelihood Estimation\\n\",\n",
    "    \"\\n\",\n",
    "    \"<span class=\\\\\"image placeholder\\\\\" original-image-src=\\\\\"./mle.png\\\\\"\\n\",\n",
    "    \"original-image-title=\\\\\"\\\\\"></span>\\n\",\n",
    "    \"\\n\",\n",
    "    \"Maximum likelihood estimation is a method of estimating the parameters\\n\",\n",
    "    \"of a statistical model, given observations. It uses calculus to find the\\n\",\n",
    "    \"maximum likelihood of the parameters.\\n\",\n",
    "    \"\\n\",\n",
    "    \"First, we need a likelihood function. This is a function of the\\n\",\n",
    "    \"parameters, which gives the probability of the observations. The\\n\",\n",
    "    \"likelihood function is defined as:\\n\",\n",
    "    \"\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"L(\\\\theta) = P(X_1, X_2, \\\\dots, X_n | \\\\theta)\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\n\",\n",
    "    \"Where $\\\\theta$ is the parameter of the distribution. The likelihood\\n\",\n",
    "    \"function is the probability of the observations, given the parameter.\\n\",\n",
    "    \"\\n\",\n",
    "    \"How can we get this probability? We can use the following formula:\\n\",\n",
    "    \"\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"L(\\\\theta) = \\\\prod_{i=1}^{n} f(x_i | \\\\theta)\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\n\",\n",
    "    \"Where $f(x_i | \\\\theta)$ is the probability density function of the\\n\",\n",
    "    \"distribution, given the parameter $\\\\theta$. $x_i$ is the $i<sup>th</sup>$\\n\",\n",
    "    \"statistic of the sample.\\n\",\n",
    "    \"\\n\",\n",
    "    \"The maximum likelihood estimator is the value of the parameter that\\n\",\n",
    "    \"maximizes the likelihood function. This is represented by the following\\n\",\n",
    "    \"formula:\\n\",\n",
    "    \"\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\\\hat{\\\\theta} = \\\\underset{\\\\theta}{\\\\text{argmax}} L(\\\\theta)\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\n\",\n",
    "    \"We will not be using this formula, but it is a good step to\\n\",\n",
    "    \"understanding. We will take our likelihood function and wrap a natural\\n\",\n",
    "    \"log around it. This is called the ****log-likelihood function****. The\\n\",\n",
    "    \"log-likelihood function is defined as:\\n\",\n",
    "    \"\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"l(\\\\theta) = \\\\ln L(\\\\theta)\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\n\",\n",
    "    \"We will then take the derivative of the log-likelihood function, and set\\n\",\n",
    "    \"it equal to zero. This will give us the maximum likelihood estimator.\\n\",\n",
    "    \"\\n\",\n",
    "    \"> This might seem a bit pointless, but as AI students, this somewhat\\n\",\n",
    "    \"> resembles the process of backpropagation. We take the derivative of\\n\",\n",
    "    \"> the loss function, and set it equal to zero. This gives us the\\n\",\n",
    "    \"> gradient of the loss function, which we can use to update the weights\\n\",\n",
    "    \"> of the neural network. (This is a very basic explanation, but it is a\\n\",\n",
    "    \"> good way to understand the concept)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Point Estimation (Python)\\n\",\n",
    "    \"\\n\",\n",
    "    \"For ease, we will use built-in datasets from pandas, such as the iris\\n\",\n",
    "    \"dataset. We will use the sepal length of the iris dataset.\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`\\`\\` python\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"from scipy import stats\\n\",\n",
    "    \"iris = pd.read<sub>csv</sub>('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv')\\n\",\n",
    "    \"# craete a random sample of size 60\\n\",\n",
    "    \"sample = iris.sample(60)\\n\",\n",
    "    \"\\`\\`\\`\\n\",\n",
    "    \"\\n\",\n",
    "    \"We will try to estimate the mean of the sepal length of the iris\\n\",\n",
    "    \"dataset. We will use the method of moments to estimate the mean.\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`\\`\\` python\\n\",\n",
    "    \"sepal<sub>length</sub> = iris['sepal<sub>length</sub>']\\n\",\n",
    "    \"mean = sepal<sub>length.mean</sub>()\\n\",\n",
    "    \"print(mean)\\n\",\n",
    "    \"\\`\\`\\`\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`\\`\\` example\\n\",\n",
    "    \"5.843333333333334\\n\",\n",
    "    \"\\`\\`\\`\\n\",\n",
    "    \"\\n\",\n",
    "    \"We now know the actual mean of the sepal length of the iris dataset. We\\n\",\n",
    "    \"will now try to estimate the mean using the method of moments.\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`\\`\\` python\\n\",\n",
    "    \"# \\\\\\hat{\\\\theta} = \\\\\\frac{1}{n} \\\\&sum;<sub>i=1</sub><sup>n</sup> x<sub>i</sub><sup>k\\n</sup>\",\n",
    "    \"# we use the sample\\n\",\n",
    "    \"mean = sample['sepal<sub>length</sub>'].sum() / sample['sepal<sub>length</sub>'].size\\n\",\n",
    "    \"print(mean)\\n\",\n",
    "    \"\\`\\`\\`\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`\\`\\` example\\n\",\n",
    "    \"5.691666666666666\\n\",\n",
    "    \"\\`\\`\\`\\n\",\n",
    "    \"\\n\",\n",
    "    \"Now, we will try to estimate the variance using the maximum likelihood\\n\",\n",
    "    \"estimator.\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`\\`\\` python\\n\",\n",
    "    \"iris = pd.read<sub>csv</sub>('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv')\\n\",\n",
    "    \"# craete a random sample of size 60\\n\",\n",
    "    \"n = 10\\n\",\n",
    "    \"samples = [iris.sample(n\\*2) for i in range(n)]\\n\",\n",
    "    \"variances = [sample['sepal<sub>length</sub>'].var() for sample in samples]\\n\",\n",
    "    \"\\n\",\n",
    "    \"# we get the probability density function for each variance\\n\",\n",
    "    \"f = [stats.norm.pdf(variances[i], iris['sepal<sub>length</sub>'].var(), iris['sepal<sub>length</sub>'].std()) for i in range(n)]\\n\",\n",
    "    \"\\n\",\n",
    "    \"# we get the log of the probability density function\\n\",\n",
    "    \"f<sub>log</sub> = np.log(f)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# we get the maximum likelihood estimator\\n\",\n",
    "    \"mle = variances[f<sub>log.argmax</sub>()]\\n\",\n",
    "    \"# we can express the estimated variance as a sum of the actual variance and the error\\n\",\n",
    "    \"print(f\\\\\"Estimated variance: {mle}, Actual variance: {iris['sepal<sub>length</sub>'].var()}\\\\\")\\n\",\n",
    "    \"print(\\\\\"Error: \\\\\", mle - iris['sepal<sub>length</sub>'].var())\\n\",\n",
    "    \"\\`\\`\\`\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`\\`\\` example\\n\",\n",
    "    \"Estimated variance: 0.6918684210526318, Actual variance: 0.6856935123042507\\n\",\n",
    "    \"Error:  0.006174908748381114\\n\",\n",
    "    \"\\`\\`\\`\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Single Sample Intervals\\n\",\n",
    "    \"\\n\",\n",
    "    \"[Animation](./Animations/cls.mp4)\\n\",\n",
    "    \"\\n\",\n",
    "    \"In this section, we will look at confidence intervals for a single\\n\",\n",
    "    \"sample. This will combine the idea id random variables, and the idea of\\n\",\n",
    "    \"sampling distributions.\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Confidence Intervals\\n\",\n",
    "    \"\\n\",\n",
    "    \"This is a range between two values, which we are P% confident that the\\n\",\n",
    "    \"population parameter lies in. To better understand this, here is a very\\n\",\n",
    "    \"'boilerplate' example:\\n\",\n",
    "    \"\\n\",\n",
    "    \"1.  We choose a confidence level, $P$.\\n\",\n",
    "    \"2.  We get its z-score\\n\",\n",
    "    \"\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"(\\\\bar{X} - z_P \\\\frac{\\\\sigma}{\\\\sqrt{n}}, \\\\bar{X} + z_P \\\\frac{\\\\sigma}{\\\\sqrt{n}})\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\n\",\n",
    "    \"Where $\\\\bar{X}$ is the sample mean, and $\\\\sigma$ is the population\\n\",\n",
    "    \"standard deviation, therefore $\\\\frac{\\\\sigma}{\\\\sqrt{n}}$ is the standard\\n\",\n",
    "    \"error.\\n\",\n",
    "    \"\\n\",\n",
    "    \"We can also write this as:\\n\",\n",
    "    \"\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\\\bar{X} \\\\pm z_P \\\\frac{\\\\sigma}{\\\\sqrt{n}}\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\n\",\n",
    "    \"So what does this tell us? It tells us that we are P% confident that the\\n\",\n",
    "    \"population mean lies within the interval\\n\",\n",
    "    \"$\\\\bar{X} \\\\pm z_P \\\\frac{\\\\sigma}{\\\\sqrt{n}}$.\\n\",\n",
    "    \"\\n\",\n",
    "    \"-   The more confident we want to be, the larger the confidence level\\n\",\n",
    "    \"    $P$. But, the larger the confidence level, the larger the interval,\\n\",\n",
    "    \"    the lower the precision.\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Interpretation\\n\",\n",
    "    \"\\n\",\n",
    "    \"Since we only know $\\\\bar{x}, \\\\sigma \\\\text{ and } n$, we ****cannot\\n\",\n",
    "    \"conclude that**** the population mean lies within the interval\\n\",\n",
    "    \"$\\\\bar{X} \\\\pm z_P \\\\frac{\\\\sigma}{\\\\sqrt{n}}$.\\n\",\n",
    "    \"\\n\",\n",
    "    \"Why? Because we are not using a random sample for the mean. We can only\\n\",\n",
    "    \"conclude that if we repeated the experiment many times, the result we\\n\",\n",
    "    \"obtain would occur P% of the time. In other words, ****if we get 100\\n\",\n",
    "    \"different confidence intervals, $P%$ of them would contain the\\n\",\n",
    "    \"population mean.****\\n\",\n",
    "    \"\\n\",\n",
    "    \"<span class=\\\\\"image placeholder\\\\\"\\n\",\n",
    "    \"original-image-src=\\\\\"./Single<sub>Sample</sub><sub>Intervals</sub>/2023-03-04<sub>13</sub>-30-04<sub>screenshot.png</sub>\\\\\"\\n\",\n",
    "    \"original-image-title=\\\\\"\\\\\"></span>\\n\",\n",
    "    \"\\n\",\n",
    "    \"Diagram of the process of creating confidence intervals and interpreting\\n\",\n",
    "    \"them:\\n\",\n",
    "    \"\\n\",\n",
    "    \"![img](./Single_Sample_Intervals/inchart.png)\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Confidence Levels\\n\",\n",
    "    \"\\n\",\n",
    "    \"Thus far, we used a variable confidence level $P$. But, we can also use\\n\",\n",
    "    \"a fixed confidence level, such as 95%. This is the same as using a\\n\",\n",
    "    \"confidence level of 0.95. (You must use the decimal form, not the\\n\",\n",
    "    \"percentage form.) Normaly, the variable which is used to represent the\\n\",\n",
    "    \"confidence level is $\\\\alpha$. So, we can write the confidence interval\\n\",\n",
    "    \"as:\\n\",\n",
    "    \"\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\\\bar{X} \\\\pm z_{\\\\alpha/2} \\\\frac{\\\\sigma}{\\\\sqrt{n}}\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\n\",\n",
    "    \"Where $z_{\\\\alpha/2}$ is the z-score for the confidence level $\\\\alpha/2$.\\n\",\n",
    "    \"Why divide by 2? Because we are looking at the area under the curve on\\n\",\n",
    "    \"both sides of the mean. So, we are looking at the area under the curve\\n\",\n",
    "    \"for $\\\\alpha/2$ on each side of the mean.\\n\",\n",
    "    \"\\n\",\n",
    "    \"<span class=\\\\\"image placeholder\\\\\"\\n\",\n",
    "    \"original-image-src=\\\\\"./Single<sub>Sample</sub><sub>Intervals</sub>/2023-03-04<sub>13</sub>-49-22<sub>screenshot.png</sub>\\\\\"\\n\",\n",
    "    \"original-image-title=\\\\\"\\\\\"></span>\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Precision and Sample Size\\n\",\n",
    "    \"\\n\",\n",
    "    \"First, we need to define the width of the interval as:\\n\",\n",
    "    \"$2*z_P \\\\frac{\\\\sigma}{\\\\sqrt{n}}$. This is the width of the interval.\\n\",\n",
    "    \"\\n\",\n",
    "    \"-   Higher the confidence level, the wider the interval.\\n\",\n",
    "    \"-   Higher the sample size, the narrower the interval.\\n\",\n",
    "    \"-   Lower the population standard deviation, the narrower the interval.\\n\",\n",
    "    \"-   Higher the confidence level, the higher the sample size required to\\n\",\n",
    "    \"    achieve a given precision.\\n\",\n",
    "    \"\\n\",\n",
    "    \"We might want to ensure, that a confidence interval has a certain width.\\n\",\n",
    "    \"In this case, we can use the following formula:\\n\",\n",
    "    \"\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"n = (2*z_{\\\\alpha/2} \\\\frac{\\\\sigma}{\\\\text{width}})^2\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\n\",\n",
    "    \"The application of this\\n\",\n",
    "    \"\\n\",\n",
    "    \"## <span class=\\\\\"todo TODO\\\\\">TODO</span> Derivation of the Confidence Interval\\n\",\n",
    "    \"\\n\",\n",
    "    \"If we have a random sample of size $n$ from a population, we can\\n\",\n",
    "    \"construct a confidence interval for some parameter $\\\\theta$ using the\\n\",\n",
    "    \"following steps:\\n\",\n",
    "    \"\\n\",\n",
    "    \"1.  Check if the conditions are met:\\n\",\n",
    "    \"    -   The variable depends on the sample and parameter $\\\\theta$.\\n\",\n",
    "    \"    -   The probability distribution of the variable is known.\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Large Sample Confidence Intervals (Mean & Proportion)\\n\",\n",
    "    \"\\n\",\n",
    "    \"Previously, we assumed that the population standard deviation $\\\\&sigma;$\\n\",\n",
    "    \"was known and that the population distribution was normal. If we cannot\\n\",\n",
    "    \"assume these things, we can use the large sample confidence interval.\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Large Sample Confidence Interval for the Mean\\n\",\n",
    "    \"\\n\",\n",
    "    \"It goes back to the central limit theorem. If we take a random sample of\\n\",\n",
    "    \"size $n$ from a population, we can assume that the sample mean $\\\\\\bar{X}$\\n\",\n",
    "    \"is normally distributed. Therefore, we can use the following formula:\\n\",\n",
    "    \"\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"Z = \\\\frac{\\\\bar{X} - \\\\mu}{\\\\frac{\\\\sigma}{\\\\sqrt{n}}}\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\n\",\n",
    "    \"Where $\\\\mu$ is the population mean, and $\\\\sigma$ is the population\\n\",\n",
    "    \"standard deviation. Thus, we can write the confidence interval as:\\n\",\n",
    "    \"\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\\\frac{\\\\bar{X} - \\\\mu}{\\\\frac{\\\\sigma}{\\\\sqrt{n}}} \\\\pm z_{\\\\alpha/2}\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"P(\\\\frac{\\\\bar{X} - \\\\mu}{\\\\frac{\\\\sigma}{\\\\sqrt{n}}} \\\\pm z_{\\\\alpha/2}) \\\\approx 1 - \\\\alpha\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\n\",\n",
    "    \"the last equation tells us that we are 100% - \\\\\\\\$α\\\\\\\\$% confident that the\\n\",\n",
    "    \"population mean lies within the interval\\n\",\n",
    "    \"$\\\\frac{\\\\bar{X} - \\\\mu}{\\\\frac{\\\\sigma}{\\\\sqrt{n}}} \\\\pm z_{\\\\alpha/2}$.\\n\",\n",
    "    \"\\n\",\n",
    "    \"What happens if we replace $\\\\sigma$ with $s$ in the above equation?\\n\",\n",
    "    \"Since we adding a new random variable to the denominator, we get that:\\n\",\n",
    "    \"\\n\",\n",
    "    \"-   The confidence interval is wider.\\n\",\n",
    "    \"\\n\",\n",
    "    \"But, if our sample size is large enough, the difference between $\\\\&sigma;$\\n\",\n",
    "    \"and $s$ is small, and the confidence interval is not much wider. ****What\\n\",\n",
    "    \"is large enough?**** If $n \\\\geq 40$, then the difference between $\\\\&sigma;$\\n\",\n",
    "    \"and $s$ is small enough.\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Large Sample Confidence Interval for Population Proportion\\n\",\n",
    "    \"\\n\",\n",
    "    \"Up till now we talked about being confident that the mean of a\\n\",\n",
    "    \"population lies within a certain interval. But, what if we want to be\\n\",\n",
    "    \"confident that the proportion of a population lies within a certain\\n\",\n",
    "    \"interval? For example, we want to be 95% confident that the proportion\\n\",\n",
    "    \"of people who like chocolate is between 0.4 and 0.6. We can use the\\n\",\n",
    "    \"following formula:\\n\",\n",
    "    \"\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"P(-z_{\\\\alpha/2} \\\\leq \\\\frac{\\\\hat{p} - p}{\\\\sqrt{\\\\frac{p(1-p)}{n}}} \\\\leq z_{\\\\alpha/2}) \\\\approx 1 - \\\\alpha\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\n\",\n",
    "    \"Where $\\\\hat{p}$ is the sample proportion, $p$ is the population\\n\",\n",
    "    \"proportion, and $n$ is the sample size. Since we are talking about\\n\",\n",
    "    \"proportion, we are also talking about probability, and can use the\\n\",\n",
    "    \"binomial distribution, where $n$ is the number of trials, and $p$ is the\\n\",\n",
    "    \"probability of success. Remember that:\\n\",\n",
    "    \"\\n\",\n",
    "    \"An important rule to remember is that the sample proportion is\\n\",\n",
    "    \"approximately normally distributed if $np \\\\geq 10$ and $n(1-p) \\\\geq 10$.\\n\",\n",
    "    \"\\n\",\n",
    "    \"The general formula for a confidence interval for a population\\n\",\n",
    "    \"proportion is:\\n\",\n",
    "    \"\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\\\hat{p} \\\\pm z_{\\\\alpha/2} \\\\sqrt{\\\\frac{\\\\hat{p}(1-\\\\hat{p})}{n}}\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\n\",\n",
    "    \"This formula can only be used if the sample size is large enough, that\\n\",\n",
    "    \"is if it is above 40.\\n\",\n",
    "    \"\\n\",\n",
    "    \"## One Sided\\n\",\n",
    "    \"\\n\",\n",
    "    \"All previous confidence intervals talked about two bounds, one on the\\n\",\n",
    "    \"left and one on the right. But, what if we want to be confident that the\\n\",\n",
    "    \"population mean is greater than a certain value? For example, we want to\\n\",\n",
    "    \"be 95% confident that the population mean is within a certain range\\n\",\n",
    "    \"above the sample mean. We can use the following formula:\\n\",\n",
    "    \"\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\\\mu < \\\\bar{X} + z_{\\\\alpha} \\\\frac{\\\\sigma}{\\\\sqrt{n}}\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\n\",\n",
    "    \"Where $\\\\mu$ is the population mean, $\\\\bar{X}$ is the sample mean,\\n\",\n",
    "    \"$\\\\sigma$ is the population standard deviation, and $n$ is the sample\\n\",\n",
    "    \"size.\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Confidence Intervals for Normal Distributions\\n\",\n",
    "    \"\\n\",\n",
    "    \"We can assume that the population follows a normal distribution, that in\\n\",\n",
    "    \"only if $n$ is large enough, (viz the central limit theorem). If we have\\n\",\n",
    "    \"a sample of size $n$, then the sample mean $\\\\bar{X}$ is approximately\\n\",\n",
    "    \"normally distributed with mean $\\\\mu$ and standard deviation\\n\",\n",
    "    \"$\\\\frac{\\\\sigma}{\\\\sqrt{n}}$. We can use the following formula to calculate\\n\",\n",
    "    \"the confidence interval:\\n\",\n",
    "    \"\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\\\bar{X} \\\\pm z_{\\\\alpha/2} \\\\frac{\\\\sigma}{\\\\sqrt{n}}\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Confidence Interval for the t-Distribution\\n\",\n",
    "    \"\\n\",\n",
    "    \"If we have a sample for which the mean is $\\\\bar{X}$ and the standard\\n\",\n",
    "    \"deviation is $s$, then we can define a random variable $T$ as:\\n\",\n",
    "    \"\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"T = \\\\frac{\\\\bar{X} - \\\\mu}{\\\\frac{s}{\\\\sqrt{n}}}\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\n\",\n",
    "    \"The distribution of $T$ is called the ****Student's t-distribution****. The\\n\",\n",
    "    \"t-distribution is similar to the normal distribution, but it has fatter\\n\",\n",
    "    \"tails. The t-distribution is used when the population standard deviation\\n\",\n",
    "    \"is unknown, and the sample size is small. The t-distribution is also\\n\",\n",
    "    \"used when the population distribution is not normal.\\n\",\n",
    "    \"\\n\",\n",
    "    \"What are degrees of freedom? The degrees of freedom is the number of\\n\",\n",
    "    \"independent pieces of information in a sample. For example, if we have a\\n\",\n",
    "    \"sample of size $n$, then the degrees of freedom is $n-1$.\\n\",\n",
    "    \"\\n\",\n",
    "    \"Some key properties of the t-distribution:\\n\",\n",
    "    \"\\n\",\n",
    "    \"-   It is more spread out than the normal distribution.\\n\",\n",
    "    \"-   The higher $df$ is, the more similar the t-distribution is to the\\n\",\n",
    "    \"    normal distribution.\\n\",\n",
    "    \"\\n\",\n",
    "    \"Confidence interval for the mean using the t-distribution will then be\\n\",\n",
    "    \"given by this expression:\\n\",\n",
    "    \"\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\\\bar{X} \\\\pm t_{\\\\alpha, df} \\\\frac{s}{\\\\sqrt{n}}\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\n\",\n",
    "    \"Where $df$ is the degrees of freedom, and $s$ is the sample standard\\n\",\n",
    "    \"deviation and $\\\\alpha = 1 - \\\\text{confidence level}$.\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Prediction Interval for Future Values\\n\",\n",
    "    \"\\n\",\n",
    "    \"Now we finally get to discuss future values of some variable rather than\\n\",\n",
    "    \"estimating what might be the mean of a population.\\n\",\n",
    "    \"\\n\",\n",
    "    \"1.  We have a random sample of size $n$. ($X_1, X_2, \\\\dots, X_n$)\\n\",\n",
    "    \"2.  Now we want to know $X_{n+1}$.\\n\",\n",
    "    \"\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"E(\\\\bar{X} - X_{n+1}) = 0\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"Var(\\\\bar{X} - X_{n+1}) = \\\\frac{\\\\sigma^2}{n} + \\\\sigma^2 = \\\\sigma^2(1 + \\\\frac{1}{n})\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\n\",\n",
    "    \"Given the above, we can calculate a z-score for the confidence interval:\\n\",\n",
    "    \"\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"Z = \\\\frac{(\\\\bar{X} - X_{n+1}) - 0}{\\\\sigma^2 \\\\frac{1}{\\\\sqrt{n}}}\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\n\",\n",
    "    \"…\\n\",\n",
    "    \"\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"T = \\\\frac{(\\\\bar{X} - X_{n+1})}{S\\\\sqrt{1 \\\\frac{1}{n}}}\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\n\",\n",
    "    \"From this, we can build a confidence interval for the future value of\\n\",\n",
    "    \"$X_{n+1}$:\\n\",\n",
    "    \"\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\\\bar{x} \\\\pm t_{\\\\alpha, df} s \\\\sqrt{1 + \\\\frac{1}{n}}\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\n\",\n",
    "    \"We interpret this the same way as we did for the confidence interval for\\n\",\n",
    "    \"the mean. We are 95% confident that for multiple iterations, the future\\n\",\n",
    "    \"value of $X_{n+1}$ will be between the two bounds.\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Bootstrap\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Variance and Standard Deviation Confidence Intervals\\n\",\n",
    "    \"\\n\",\n",
    "    \"If we have a normal distribution, we might also be interested in finding\\n\",\n",
    "    \"the variance of the population if we do not have it. Given a sample of\\n\",\n",
    "    \"size $n$, we can define a random variable as follows:\\n\",\n",
    "    \"\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\\\frac{(n-1)s^2}{\\\\sigma^2} = \\\\frac{\\\\sum(X_i - \\\\bar{X})^2}{\\\\sigma^2}\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\n\",\n",
    "    \"Where $s$ is the sample standard deviation, and $\\\\sigma$ is the\\n\",\n",
    "    \"population standard deviation. The distribution of this random variable\\n\",\n",
    "    \"is called the ****chi-squared distribution****. The chi-squared distribution\\n\",\n",
    "    \"is used to find confidence intervals for the variance of a population.\\n\",\n",
    "    \"\\n\",\n",
    "    \"For this distribution, we must also define the degrees of freedom. The\\n\",\n",
    "    \"degrees of freedom is $n-1$.\\n\",\n",
    "    \"\\n\",\n",
    "    \"<span class=\\\\\"image placeholder\\\\\"\\n\",\n",
    "    \"original-image-src=\\\\\"Large<sub>Sample</sub><sub>Confidence</sub><sub>Intervals</sub><sub>(Mean\\_&amp;<sub>Proportion</sub>)</sub>/2023-03-11<sub>12</sub>-13-02<sub>.png</sub>\\\\\"\\n\",\n",
    "    \"original-image-title=\\\\\"\\\\\"></span>\\n\",\n",
    "    \"\\n\",\n",
    "    \"Key properties of the chi-squared distribution:\\n\",\n",
    "    \"\\n\",\n",
    "    \"-   It is more spread out than the normal distribution.\\n\",\n",
    "    \"-   Only positive values are possible.\\n\",\n",
    "    \"-   The higher $df$ is, the more similar the chi-squared distribution is\\n\",\n",
    "    \"    to the normal distribution.\\n\",\n",
    "    \"\\n\",\n",
    "    \"Finally, the confidence interval will look like this:\\n\",\n",
    "    \"\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\\\frac{(n-1)s^2}{\\\\chi^2_{\\\\alpha/2, df}} \\\\leq \\\\sigma^2 \\\\leq \\\\frac{(n-1)s^2}{\\\\chi^2_{1-\\\\alpha/2, df}}\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\n\",\n",
    "    \"We now have to calculate the confidence interval for the variance of the\\n\",\n",
    "    \"population. We can do this by using the following formula:\\n\",\n",
    "    \"\\n\",\n",
    "    \"-   Lower bound: $\\\\\\frac{(n-1)s^2}{\\\\&chi;<sup>2</sup><sub>\\\\&alpha;/2, df</sub>}$\\n\",\n",
    "    \"-   Upper bound: $\\\\\\frac{(n-1)s^2}{\\\\&chi;<sup>2</sup><sub>1-\\\\&alpha;/2, df</sub>}$\\n\",\n",
    "    \"\\n\",\n",
    "    \"Where $\\\\chi^2_{\\\\alpha/2, df}$ is the $\\\\alpha/2$ percentile of the\\n\",\n",
    "    \"chi-squared distribution with $df$ degrees of freedom.\\n\",\n",
    "    \"\\n\",\n",
    "    \"We can also calculate the confidence interval for the standard deviation\\n\",\n",
    "    \"of the population. We can do this by using the following formula:\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Confidence Intervals (Python)\\n\",\n",
    "    \"\\n\",\n",
    "    \"We can make our life easier by using Python to calculate confidence\\n\",\n",
    "    \"intervals. We will use the following packages:\\n\",\n",
    "    \"\\n\",\n",
    "    \"-   scipy.stats\\n\",\n",
    "    \"-   numpy\\n\",\n",
    "    \"-   pandas\\n\",\n",
    "    \"-   matplotlib\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Simple Confidence Intervals for the Mean\\n\",\n",
    "    \"\\n\",\n",
    "    \"For ease, we will use built-in datasets from pandas, such as the iris\\n\",\n",
    "    \"dataset. We will use the sepal length of the iris dataset.\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`\\`\\` python\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"from scipy import stats\\n\",\n",
    "    \"\\n\",\n",
    "    \"iris = pd.read<sub>csv</sub>(\\\\\"[https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv](https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv)\\\\\")\\n\",\n",
    "    \"sepal<sub>length</sub> = iris[\\\\\"sepal<sub>length</sub>\\\\\"]\\n\",\n",
    "    \"print(sepal<sub>length.head</sub>())\\n\",\n",
    "    \"\\`\\`\\`\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`\\`\\` example\\n\",\n",
    "    \"0    5.1\\n\",\n",
    "    \"1    4.9\\n\",\n",
    "    \"2    4.7\\n\",\n",
    "    \"3    4.6\\n\",\n",
    "    \"4    5.0\\n\",\n",
    "    \"Name: sepal<sub>length</sub>, dtype: float64\\n\",\n",
    "    \"\\`\\`\\`\\n\",\n",
    "    \"\\n\",\n",
    "    \"Now we have, the data. Lets create a confidence interval for the mean of\\n\",\n",
    "    \"the sepal length. We will use a confidence level of 95%.\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`\\`\\` python\\n\",\n",
    "    \"interval = stats.norm.interval(0.95, loc=np.mean(sepal<sub>length</sub>), scale=np.std(sepal<sub>length</sub>))\\n\",\n",
    "    \"print(interval)\\n\",\n",
    "    \"\\`\\`\\`\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`\\`\\` example\\n\",\n",
    "    \"(4.2257725250400755, 7.460894141626592)\\n\",\n",
    "    \"\\`\\`\\`\\n\",\n",
    "    \"\\n\",\n",
    "    \"We can see that the confidence interval is between 4.23 and 7.46.\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Confidence Interval for the Population Proportion\\n\",\n",
    "    \"\\n\",\n",
    "    \"We will use the same dataset as before, but this time we will use the\\n\",\n",
    "    \"sepal width. We will use a confidence level of 95%. In the first example\\n\",\n",
    "    \"we do not approximate, we use the exact formula.\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`\\`\\` python\\n\",\n",
    "    \"from statsmodels.stats.proportion import proportion<sub>confint\\n</sub>\",\n",
    "    \"# proportion where the sepal width is greater than 3.5\\n\",\n",
    "    \"X = np.sum(iris[\\\\\"sepal<sub>width</sub>\\\\\"] > 3.5)\\n\",\n",
    "    \"n = len(iris[\\\\\"sepal<sub>width</sub>\\\\\"])\\n\",\n",
    "    \"p = X/n\\n\",\n",
    "    \"\\n\",\n",
    "    \"interval = proportion<sub>confint</sub>(X, n, alpha=0.05, method=\\\\\"normal\\\\\")\\n\",\n",
    "    \"print(interval)\\n\",\n",
    "    \"\\`\\`\\`\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`\\`\\` example\\n\",\n",
    "    \"(0.0734406885907721, 0.17989264474256125)\\n\",\n",
    "    \"\\`\\`\\`\\n\",\n",
    "    \"\\n\",\n",
    "    \"The \\`method\\` parameter can be either \\`normal\\` or \\`wilson\\`.\\n\",\n",
    "    \"\\n\",\n",
    "    \"-   \\`normal\\` uses the normal approximation. We use this one if\\n\",\n",
    "    \"    $np \\\\geq 10$ and $n(1-p) \\\\geq 10$.\\n\",\n",
    "    \"-   \\`wilson\\` uses the Wilson score interval. If the conditions are not\\n\",\n",
    "    \"    met, we use this one.\\n\",\n",
    "    \"\\n\",\n",
    "    \"We can now try to approximate with the normal distribution. We will use\\n\",\n",
    "    \"the same confidence level of 95%.\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`\\`\\` python\\n\",\n",
    "    \"scale = np.sqrt(p\\*(1-p)/n)\\n\",\n",
    "    \"interval = stats.norm.interval(0.95, loc=p, scale=scale)\\n\",\n",
    "    \"print(interval)\\n\",\n",
    "    \"\\`\\`\\`\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`\\`\\` example\\n\",\n",
    "    \"(0.07344068859077212, 0.17989264474256123)\\n\",\n",
    "    \"\\`\\`\\`\\n\",\n",
    "    \"\\n\",\n",
    "    \"Now lets take a look at binomial approximation for the confidence\\n\",\n",
    "    \"interval. We will use the same confidence level of 95%. It is important\\n\",\n",
    "    \"to check if the conditions are met, that is if $np \\\\geq 10$ and\\n\",\n",
    "    \"$n(1-p) \\\\geq 10$.\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`\\`\\` python\\n\",\n",
    "    \"# conditions test\\n\",\n",
    "    \"print(\\\\\"np >= 10: \\\\\", n\\*p >= 10)\\n\",\n",
    "    \"print(\\\\\"n(1-p) >= 10: \\\\\", n\\*(1-p) >= 10)\\n\",\n",
    "    \"\\`\\`\\`\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`\\`\\` example\\n\",\n",
    "    \"np >= 10:  True\\n\",\n",
    "    \"n(1-p) >= 10:  True\\n\",\n",
    "    \"\\`\\`\\`\\n\",\n",
    "    \"\\n\",\n",
    "    \"Now we can use the binomial approximation.\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`\\`\\` python\\n\",\n",
    "    \"interval = stats.binom.interval(0.95, n, p)\\n\",\n",
    "    \"print(interval)\\n\",\n",
    "    \"interval = [x/n for x in interval]\\n\",\n",
    "    \"print(interval)\\n\",\n",
    "    \"\\`\\`\\`\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`\\`\\` example\\n\",\n",
    "    \"(11.0, 27.0)\\n\",\n",
    "    \"[0.07333333333333333, 0.18]\\n\",\n",
    "    \"\\`\\`\\`\\n\",\n",
    "    \"\\n\",\n",
    "    \"The last step is very important. We need to divide the interval by the\\n\",\n",
    "    \"sample size to get the proportion interval. We can see that the interval\\n\",\n",
    "    \"is between 0.073 and 0.18, which is a very close approximation to the\\n\",\n",
    "    \"normal approximation.\\n\",\n",
    "    \"\\n\",\n",
    "    \"## t Distribution Confidence Intervals\\n\",\n",
    "    \"\\n\",\n",
    "    \"We will use the same dataset as before, but this time we will use the\\n\",\n",
    "    \"petal length. We will use a confidence level of 95%. In the first\\n\",\n",
    "    \"example we do not approximate, we use the exact formula.\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`\\`\\` python\\n\",\n",
    "    \"from statsmodels.stats.weightstats import \\_tconfint<sub>generic\\n</sub>\",\n",
    "    \"# proportion where the sepal width is greater than 3.5\\n\",\n",
    "    \"X = np.sum(iris[\\\\\"petal<sub>length</sub>\\\\\"] > 3.5)\\n\",\n",
    "    \"n = len(iris[\\\\\"petal<sub>length</sub>\\\\\"])\\n\",\n",
    "    \"p = X/n\\n\",\n",
    "    \"\\n\",\n",
    "    \"interval = \\_tconfint<sub>generic</sub>(p, np.sqrt(p\\*(1-p)/n), n-1, 0.05, 'two-sided')\\n\",\n",
    "    \"print(interval)\\n\",\n",
    "    \"\\`\\`\\`\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Prediction Interval for Future Values\\n\",\n",
    "    \"\\n\",\n",
    "    \"We will use the same dataset as before, but this time we will use the\\n\",\n",
    "    \"petal width. We will use a confidence level of 95%. In this example we\\n\",\n",
    "    \"are trying to predict the future value of the petal width.\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`\\`\\` python\\n\",\n",
    "    \"from statsmodels.stats.weightstats import \\_tconfint<sub>generic\\n</sub>\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"sample<sub>mean</sub> = np.mean(iris[\\\\\"petal<sub>width</sub>\\\\\"])\\n\",\n",
    "    \"sample<sub>std</sub> = np.std(iris[\\\\\"petal<sub>width</sub>\\\\\"])\\n\",\n",
    "    \"n = len(iris[\\\\\"petal<sub>width</sub>\\\\\"])\\n\",\n",
    "    \"alpha = 0.05\\n\",\n",
    "    \"\\n\",\n",
    "    \"t<sub>score</sub> = stats.t.ppf(1-alpha/2, n-1) # t score for 95% confidence\\n\",\n",
    "    \"interval = (sample<sub>mean</sub> - t<sub>score</sub>\\*sample<sub>std</sub>/np.sqrt(n), sample<sub>mean</sub> + t<sub>score</sub>\\*sample<sub>std</sub>/np.sqrt(n))\\n\",\n",
    "    \"print(interval)\\n\",\n",
    "    \"\\`\\`\\`\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`\\`\\` example\\n\",\n",
    "    \"(1.0767639167319225, 1.3219027499347447)\\n\",\n",
    "    \"\\`\\`\\`\\n\",\n",
    "    \"\\n\",\n",
    "    \"So we can now be 95% confident that the future value of the petal width\\n\",\n",
    "    \"will be between 1.08 and 1.32. How can we validate this? We can use the\\n\",\n",
    "    \"bootstrap method.\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`\\`\\` python\\n\",\n",
    "    \"from sklearn.utils import resample\\n\",\n",
    "    \"\\n\",\n",
    "    \"# bootstrap\\n\",\n",
    "    \"n<sub>iterations</sub> = 1000\\n\",\n",
    "    \"n<sub>size</sub> = int(len(iris[\\\\\"petal<sub>width</sub>\\\\\"]) \\* 0.50)\\n\",\n",
    "    \"medians = list()\\n\",\n",
    "    \"for i in range(n<sub>iterations</sub>):\\n\",\n",
    "    \"  s = resample(iris[\\\\\"petal<sub>width</sub>\\\\\"], n<sub>samples</sub>=n<sub>size</sub>)\\n\",\n",
    "    \"  m = np.mean(s)\\n\",\n",
    "    \"  medians.append(m)\\n\",\n",
    "    \"\\n\",\n",
    "    \"interval = np.percentile(medians, [2.5, 97.5])\\n\",\n",
    "    \"print(interval)\\n\",\n",
    "    \"\\`\\`\\`\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`\\`\\` example\\n\",\n",
    "    \"[1.02796667 1.36133333]\\n\",\n",
    "    \"\\`\\`\\`\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Confidence Interval for the Population Variance\\n\",\n",
    "    \"\\n\",\n",
    "    \"We will use the same dataset as before, but this time we will use the\\n\",\n",
    "    \"petal width. We will use a confidence level of 95%.\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`\\`\\` python\\n\",\n",
    "    \"s = np.std(iris[\\\\\"petal<sub>width</sub>\\\\\"])\\n\",\n",
    "    \"var = s\\*\\*2\\n\",\n",
    "    \"n = len(iris[\\\\\"petal<sub>width</sub>\\\\\"])\\n\",\n",
    "    \"alpha = 0.05\\n\",\n",
    "    \"ucl = (n-1)\\*var/stats.chi2.ppf(alpha/2, n-1)\\n\",\n",
    "    \"lcl = (n-1)\\*var/stats.chi2.ppf(1-alpha/2, n-1)\\n\",\n",
    "    \"inteval = (lcl, ucl)\\n\",\n",
    "    \"print(interval)\\n\",\n",
    "    \"\\`\\`\\`\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\`\\`\\` example\\n\",\n",
    "    \"(117.09798286232113, 184.68695493443445)\\n\",\n",
    "    \"\\`\\`\\`\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Midterm Review\\n\",\n",
    "    \"\\n\",\n",
    "    \"This is a review of the key concepts mentioned in the notes thus far. It\\n\",\n",
    "    \"is not a complete list of all the concepts that will be on the midterm.\\n\",\n",
    "    \"It is meant to be a guide to help you study for the midterm.\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Random Variables\\n\",\n",
    "    \"\\n\",\n",
    "    \"Its a bit like musical chairs, as the music plays, we do not know the\\n\",\n",
    "    \"final outcome of the people that will end up sitting. Using this\\n\",\n",
    "    \"analogy, a random sample is a subset of the population that we are\\n\",\n",
    "    \"interested in. The random variable comes once we use that random sample\\n\",\n",
    "    \"to compute some statistic.\\n\",\n",
    "    \"\\n\",\n",
    "    \"-   A random variable is a variable whose value is determined by chance.\\n\",\n",
    "    \"-   A random variable can be discrete or continuous.\\n\",\n",
    "    \"-   When is it representative?\\n\",\n",
    "    \"    -   No replacement sampling\\n\",\n",
    "    \"    -   Large enough size (at most 5%)\\n\",\n",
    "    \"\\n\",\n",
    "    \"The variance of a random variable is called the ****standard error****.\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Central Limit Theorem\\n\",\n",
    "    \"\\n\",\n",
    "    \"If we take a sampling distribution of the sample mean. As long as the\\n\",\n",
    "    \"following are true, the sampling distribution of the sample mean will be\\n\",\n",
    "    \"approximately normal:\\n\",\n",
    "    \"\\n\",\n",
    "    \"-   The population is uni-modal\\n\",\n",
    "    \"-   Our sample size is large enough ($n \\\\geq 30$)\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Estimation\\n\",\n",
    "    \"\\n\",\n",
    "    \"We try to estimate some population parameter using a sample statistic.\\n\",\n",
    "    \"\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\\\hat{\\\\theta} = \\\\theta + \\\\epsilon\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\n\",\n",
    "    \"Where $\\\\epsilon$ is the error term. The error term is the difference\\n\",\n",
    "    \"between the population parameter and the sample statistic.\\n\",\n",
    "    \"\\n\",\n",
    "    \"Method of Moments  \\n\",\n",
    "    \"We can use this method to computer the estimate given the following\\n\",\n",
    "    \"formula: $\\\\hat{\\\\theta} = \\\\frac{1}{n}\\\\sum_{i=1}^n x_i^k$. All we need is\\n\",\n",
    "    \"the ****order of the moment**** and $x$, the sample.\\n\",\n",
    "    \"\\n\",\n",
    "    \"Maximum Likelihood Estimator  \\n\",\n",
    "    \"\\n\",\n",
    "    \"## Confidence Interval Distributions\\n\",\n",
    "    \"\\n\",\n",
    "    \"It is important to know which distribution to use when constructing a\\n\",\n",
    "    \"confidence interval for some population parameter.\\n\",\n",
    "    \"\\n\",\n",
    "    \"<span class=\\\\\"image placeholder\\\\\"\\n\",\n",
    "    \"original-image-src=\\\\\"./confint-dist-decision.png\\\\\"\\n\",\n",
    "    \"original-image-title=\\\\\"\\\\\"></span>\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Single Sample Hypothesis Testing\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Hypothesis Testing\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Hypothesis Testing for Normal Distributions\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Hypothesis Testing for Proportions\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Hypothesis Testing for Variances\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Two Sample Hypotheses Testing\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Hypothesis Testing for Two Means\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Hypothesis Testing for Two Proportions\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Hypothesis Testing for Two Variances\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Analysis of Variance: Single Factor\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Analysis of Variance\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Analysis of Variance for Normal Distributions\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Analysis of Variance for Proportions\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Analysis of Variance for Variances\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Analysis of Variance: Multi Factor\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Analysis of Variance for Two Factors\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Analysis of Variance for Three Factors\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Goodness-of-fit Tests\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Goodness-of-fit Tests for Normal Distributions\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Goodness-of-fit Tests for Proportions\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Goodness-of-fit Tests for Variances\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Categorical Data Analysis\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Chi-Square Tests for Independence\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Chi-Square Tests for Homogeneity\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Chi-Square Tests for Goodness-of-fit\\n\",\n",
    "    \"\\n\",\n",
    "    \"<footer style=\\\\\"height: 20vh;\\\\\"></footer>\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display<sub>name</sub>\": \"Python 3 (ipykernel)\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language<sub>info</sub>\": {\n",
    "   \"codemirror<sub>mode</sub>\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file<sub>extension</sub>\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert<sub>exporter</sub>\": \"python\",\n",
    "   \"pygments<sub>lexer</sub>\": \"ipython3\",\n",
    "   \"version\": \"3.10.9\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat<sub>minor</sub>\": 5\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "org": null
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
