{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Fundamentals of Data Analysis - Class Notes\n===========================================\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Table of Contents\n\n\n\n<script data-name=\"BMC-Widget\" data-cfasync=\"false\" src=\"https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js\" data-id=\"velocitatem24\" data-description=\"Support me on Buy me a coffee!\" data-message=\"\" data-color=\"#5F7FFF\" data-position=\"Right\" data-x_margin=\"18\" data-y_margin=\"18\"></script>\n\n<script>setTimeout(() => {alert(\"Finding this useful? Consider buying me a coffee! Bottom right cornner :) Takes just a few seconds\")}, 60*1000);_paq.push(['trackEvent', 'Exposure', 'Exposed to beg']);</script>\n\n**New** search feature! Make use of the amazing [fuzzy search](https://en.wikipedia.org/wiki/Approximate_string_matching) algorithm. Just type in the search box and it will find the closest match in the page. Hit `Enter` to jump to the next match. Lmk if it doesn't work for you.\n\n<input id=\"search\" type=\"text\" placeholder=\"Search\" /> <span id=\"resultCount\"></span>\n\n<script src=\"https://alves.world/fuzzy.js\"></script>\n\nUseful Resources:\n\n-   [Seeing Theory - Frequentist Inference](https://seeing-theory.brown.edu/frequentist-inference/index.html#section1)\n-   [Penn State Notes](https://online.stat.psu.edu/stat415/)\n\n<a href=\"https://colab.research.google.com/github/velocitatem/university-study-notes/blob/master/content/20221231174507-fundamentals_of_data_analysis_class_notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Manipulation & ipython\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pandas DataFrames\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is best to imagine a DataFrame as spreadsheet.\n\n> Hello There!#+BEGIN<sub>src</sub> python :session fda :results output :eval no\n  import pandas as pd\n\ndf = pd.DataFrame({'name': ['John', 'Jane', 'Joe'],\n                  'age': [34, 25, 67],\n                  'height': [1.78, 1.65, 1.89]})\n\ndf = pd.DataFrame([['John', 34, 1.78],\n                  ['Jane', 25, 1.65],\n                  ['Joe', 67, 1.89]],\n                  columns=['name', 'age', 'height'])\n\n\\#+END<sub>SRC</sub>\n\nHere are some of the most useful methods for working with DataFrames:\n\n-   **`.head()`:** returns the first 5 rows of the DataFrame\n-   **`.tail()`:** returns the last 5 rows of the DataFrame\n-   **`.transpose()`:** returns the transpose of the DataFrame\n-   **`.plot.scatter(/cols/)`:** \n\n-   **`.shape`:** returns the number of rows and columns in the DataFrame\n-   **`.dtypes`:** returns the data types of each column in the DataFrame\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Filtering DataFrames\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a DataFrame from a dictionary\ndf = pd.DataFrame({'name': ['John', 'Jane', 'Joe'],\n                  'age': [34, 25, 67],\n                  'height': [1.78, 1.65, 1.89]})\n\n# Filter the DataFrame to only include people over 30\ndf[df['age'] > 30]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The general syntax for filtering a DataFrame is: `df[condition]`. The `condition` is a boolean expression that evaluates to either `True` or `False` for each row in the DataFrame. The result of the filter is a new DataFrame containing only the rows where the condition is `True`.\n\nWe can also use the `.loc` method to filter a DataFrame. The `.loc` method takes a list of row labels and a list of column labels as arguments. The result is a new DataFrame containing only the rows and columns specified. Here is an example:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a DataFrame from a dictionary\ndf = pd.DataFrame({'name': ['John', 'Jane', 'Joe'],\n                  'age': [34, 25, 67],\n                  'height': [1.78, 1.65, 1.89]})\n\n# Filter the DataFrame to only include people over 30\ndf.loc[df['age'] > 30, ['name', 'age']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The main difference between the `.loc` method and the `[]` operator is that the `.loc` method can be used to filter rows and columns at the same time. We might want to do that if we want to filter a DataFrame by rows and then select a subset of the columns.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### `np.where()`\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `np.where()` function is a vectorized version of the `if` statement. It takes a condition, a value to return if the condition is `True`, and a value to return if the condition is `False`. Here is an example:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0 0 3 4 5]"
          ]
        }
      ],
      "source": [
        "import numpy as np\n\n# Create a NumPy array\narr = np.array([1, 2, 3, 4, 5])\n\n# Use np.where() to replace all values less than 3 with 0\nres = np.where(arr < 3, 0, arr)\nprint(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sorting\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use the `.sort_values()` method to sort a DataFrame by one or more columns. Here is an example:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "name  age  height\n1  Jane   25    1.65\n0  John   34    1.78\n2   Joe   67    1.89"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n# Create a DataFrame from a dictionary\ndf = pd.DataFrame({'name': ['John', 'Jane', 'Joe'],\n                  'age': [34, 25, 67],\n                  'height': [1.78, 1.65, 1.89]})\n\n# Sort the DataFrame by age\nprint(df.sort_values('age'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Grouping\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To avoid redundant filtering and aggregation, we can use the `.groupby()` method to group a DataFrame by one or more columns. Here is an example:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#+begin_example\n         age                                ... height\n       count  mean        std   min    25%  ...    min     25%    50%     75%   max\ngender                                      ...\nF        1.0  25.0        NaN  25.0  25.00  ...   1.65  1.6500  1.650  1.6500  1.65\nM        2.0  50.5  23.334524  34.0  42.25  ...   1.78  1.8075  1.835  1.8625  1.89\n\n[2 rows x 16 columns]\n         age  height\ngender\nF       25.0   1.650\nM       50.5   1.835\ngender\nF    25.0\nM    50.5\nName: age, dtype: float64\n#+end_example"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n# Create a DataFrame from a dictionary\ndf = pd.DataFrame({'name': ['John', 'Jane', 'Joe'],\n                  'age': [34, 25, 67],\n                  'gender': [\"M\", \"F\", \"M\"],\n                  'height': [1.78, 1.65, 1.89]})\n\n# Group the DataFrame by gender\nprint(df.groupby('gender').describe())\n# Group the DataFrame by gender and calculate the mean of each group\nprint(df.groupby('gender').mean())\n# calculate the mean age for each gender\nprint(df.groupby('gender')['age'].mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ipython: Descriptive Statistics\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_74156/2522582876.py:2: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n  plt.style.use(\"seaborn\")"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\nplt.style.use(\"seaborn\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Histograms\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['some_values'].hist(bins=15, edgecolor='white')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also set some other parameters such as the title and labels:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.title('Some Title')\nplt.xlabel('Some X Label')\nplt.ylabel('Some Y Label')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Histograms: Side by Side\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we have two different groups of data, we can plot them side by side:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "group1 = DataFrame\ngroup2 = DataFrame\nplt.hist([group1, group2], bins=15, edgecolor='white', label=['Group 1', 'Group 2'])\nplt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bar Plots\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also plot bar plots (they are very similar to histograms, but plot the frequency of categorical data):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "categories = ['A', 'B', 'C', 'D']\nfrequencies = [10, 20, 30, 40]\nplt.bar(categories, frequencies, edgecolor='white')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Box Plots\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Box plots are a great way to visualize the distribution of data. They are very useful for comparing different groups of data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.boxplot([group1, group2])\nplt.xticks([1, 2], ['Group 1', 'Group 2'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Annotations\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also add annotations to our plots:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.annotate('Some Text', xy=(x, y), xytext=(x, y), arrowprops={'arrowstyle': '->'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `xy` and `xytext` parameters are the coordinates of the text and the arrow, respectively.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Centrality and Spread\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use the `mean` and `median` functions to calculate the mean and median of a dataset:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "mean = df['some_values'].mean()\nmedian = df['some_values'].median()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also use the `std` function to calculate the standard deviation:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "std = df['some_values'].std()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To get a summary of the descriptive statistics of a dataset, we can use the `describe` function:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['some_values'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All of these functions are methods on the DataFrame object.\n\n-   **Minimum:** `df['some_values'].min()`\n-   **Quartile:** `df['some_values'].quantile(0.25)`\n-   **IQR:** `df['some_values'].quantile(0.75) - df['some_values'].quantile(0.25)`\n-   **Mode:** `df['some_values'].mode()`\n-   **Skew:** `df['some_values'].skew()`\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using `numpy`\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For each of the following methods, we need to pass the dataframe column as a numpy array:\n\n-   **`np.mean`:** The mean of the array\n-   **`np.median`:** The median of the array\n-   **`np.std`:** The standard deviation of the array\n-   **`np.var`:** The variance of the array\n-   **`np.percentile`:** The percentile of the array\n-   **`np.quantile`:** The quantile of the array\n-   **`np.corrcoef`:** The correlation coefficient of the array\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using `scipy.stats`\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we assume it is imported as `ss`. We can use the following methods:\n\n-   **`ss.mode`:** The mode of the array\n-   **`ss.skew`:** The skew of the array\n-   **`ss.iqr`:** The interquartile range of the array\n-   **`ss.pearsonr`:** The Pearson correlation coefficient of two arrays\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Statistical Distributions\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A statistic is a metric, which can be calculated for any sample. Before that sample is collected, we do not know what the values are going to be. That is why we can represent a statistic as a **random variable**.\n\nFor example, the sample mean of a distribution, before we actually take the samples, is going to be $\\bar{X}$. Once we take the samples, and calculate the statistics, we get $\\bar{x}$.\n\nSince any statistic can also be a random variable, we can make distributions for these random variables. This distribution, is called the **sampling distribution**.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Random Samples\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<center>\n\n<iframe class=\"centeredIframe\" scrolling=\"NO\" src=\"https://www.statcrunch.com/app/index.html?launch=scapps&amp;app=samplingdist&amp;dist=uniform&amp;firststat=0&amp;secondstat=1\" title=\"Sampling distributions applet\" width=\"650\" height=\"700\" frameborder=\"0\"></iframe>\n\n</center>\n\nSo what determines the distribution of a statistic? It is determined by the **random samples** that we take from the population. If we take a random sample from a population, and calculate the statistic, we get a value. If we take another random sample, and calculate the statistic, we get another value. And so on.\n\nThe key factors which determine the distribution of a statistic are:\n\n-   The size of the sample\n-   The distribution of the population\n-   Sampling method\n\nFor our sample to be representative or valid, they must be **independent** and **identically distributed**. This means that the samples must be independent of each other, and the distribution of the population must be the same for each sample.\n\nThese conditions will be satisfied if:\n\n-   We have no replacement\n-   We have a large enough sample size\n\nGenerally, if at most, we sample 5% of the populations, we can assume that the X<sub>i</sub> distribution is a random sample.\n\nHere is an implementation of the example 5.12 from the book:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#+begin_example\n/tmp/ipykernel_74156/523053060.py:21: UserWarning:\n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(sample_means, ax=axes[samples.tolist().index(sampleSize)])\n/tmp/ipykernel_74156/523053060.py:21: UserWarning:\n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(sample_means, ax=axes[samples.tolist().index(sampleSize)])\n/tmp/ipykernel_74156/523053060.py:21: UserWarning:\n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(sample_means, ax=axes[samples.tolist().index(sampleSize)])\n/tmp/ipykernel_74156/523053060.py:21: UserWarning:\n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(sample_means, ax=axes[samples.tolist().index(sampleSize)])\n#+end_example"
          ]
        }
      ],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\nmu = 106\nvariance = 244\nsigma = np.sqrt(variance)\nog_population = {\n    80: 0.2,\n    100: 0.3,\n    120: 0.5\n}\nsamples = np.arange(10, 110, 30)\nfig, axes = plt.subplots(1, len(samples), figsize=(15, 5))\nfor sampleSize in samples:\n    sample_means = []\n    for i in range(1000):\n        sample = np.random.choice(list(og_population.keys()), size=sampleSize, p=list(og_population.values()))\n        sample_mean = np.mean(sample)\n        sample_means.append(sample_mean)\n    sns.distplot(sample_means, ax=axes[samples.tolist().index(sampleSize)])\n    axes[samples.tolist().index(sampleSize)].set_title('Sample Size: {}'.format(sampleSize))\n    axes[samples.tolist().index(sampleSize)].set_xlabel('Sample Mean')\n    axes[samples.tolist().index(sampleSize)].set_ylabel('Probability')\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And here is the output:\n\n![img](./sampling-distributions-5.21-extra.png)\n\nYou can see that as the sample size increases, the distribution of the sample means becomes more normal (I think).\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Derivation\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's say we have a population with a mean of $\\mu$, a standard deviation of $\\sigma$ and any probability distribution. We take a random sample of size $n$ from this population. We calculate the sample mean, and we get $\\bar{x}$. We can represent this as a random variable, $\\bar{X}$.\nWe have to consider all the possible values of $\\bar{x}$, and their probabilities. From this, we can then calculate the distribution of $\\bar{X}$.\n\nTo now calculate the statistics for the distribution of $\\bar{X}$, we can use the following formulas\n\n-   **Mean:** $\\mu_{\\bar{X}} = \\mu$\n-   **Variance:** $\\sigma_{\\bar{X}}^2 = \\frac{\\sigma^2}{n}$ (this is also called the **standard error [se]**)\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sample Mean\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The sample mean is the most common statistic. It is the average of the sample. It is also the most common statistic to use in hypothesis testing.\n\nWe previously defined the mean and variance for sampling distributions. Now we change that up a bit. We first sum up all the random statistics $T_O = X_0 + X_1 + \\dots + X_n$. From there on, we can get the expected value and variance of this **sample total**:\n\n-   **Expected Value:** $E(T_O) =n \\mu$\n-   **Variance:** $V(T_O) = n \\sigma^2$\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Central Limit Theorem\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The central limit theorem states that the sampling distribution of the sample mean will be approximately normal, as long as the sample size is large enough.\n\n![img](./Statistical_Distributions/2023-02-07_13-00-52_.png)\n\n| Population|Sample Size|Sample|\n|---|---|---|\n| Normal|Any|Normal|\n| Unknown|Huge|Normal|\n\nWe can infer from the CLT, that with a higher $n$, we will have a lower standard error.\n\n$$\n\\sigma_{\\bar{X}} = \\frac{\\sigma}{\\sqrt{n}}\n$$\n\nThe conditions to satisfy the CLT if the population is not normal are:\n\n-   The population must be **unimodal**\n-   The sample size must be large enough (usually $n \\geq 30$)\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Linear Combinations\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we have a random variable $X$, and come constants $c$, we can define a new random variable $Y$ as a linear combination of $X$ and $c$:\n\n$$\nY = c_1 X_1 + c_2 X_2 + \\dots + c_n X_n\n$$\n\nWhere the expected value and variance of $Y$ are:\n\n$$\nE(Y) = c_1 E(X_1) + c_2 E(X_2) + \\dots + c_n E(X_n)\n$$\n\n$$\nV(Y) = c_1^2 V(X_1) + c_2^2 V(X_2) + \\dots + c_n^2 V(X_n)\n$$\n\nFor the above, we assume that the $X_i$ are independent of each other. If they are not, we have to add the covariance terms.\n\n$$\nV(Y) = c_1^2 V(X_1) + c_2^2 V(X_2) + \\dots + c_n^2 V(X_n) + 2c_1c_2Cov(X_1, X_2) + \\dots + 2c_1c_nCov(X_1, X_n) + \\dots + 2c_2c_nCov(X_2, X_n)\n$$\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Point Estimation\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With point estimation, we are trying to estimate a single value, which is the best estimate of the population parameter. We can use the sample statistics to do this.\n\nThe core idea is that if we take a random sample from a population, and calculate the sample statistics, **also a random variable**, we can use that to estimate the population parameter.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Properties\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\\begin{align}\n\\text{Estimator} = \\bar{X} \\quad \\text{Estimate} = \\bar{x} \\quad \\text{Population Parameter} = \\mu \\\\\n\\end{align}\n\nGenerally, any estimator $\\hat{\\theta}$ is just a function of the population parameter $\\theta$.\n\n$$\n\\hat{\\theta} = \\theta + \\epsilon\n$$\n\nWhere $\\epsilon$ is the error term. This error term is the difference between the estimator and the actual population parameter.\n\nA way to measure the **accuracy** of an estimator is to use the **mean squared error**:\n\n$$\nMSE = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{\\theta} - \\theta)^2\n$$\n\nThe smaller the MSE, the better the estimator.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Estimator Bias\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "An estimator is unbiased only if the expected value of the estimator is equal to the population parameter. This is represented by the following formula:\n\n$$\nE(\\hat{\\theta}) = \\theta\n$$\n\nIf there is any difference, that difference is the bias of the estimator.\n\nIf $X$ is a random variable given by a **binomial** distribution, then $\\hat{p} = \\frac{X}{n}$ is an unbiased estimator of $p$.\n\nWe previously defined the estimate for the mean, now lets take a look at the estimate for the variance:\n\n$$\n\\hat{\\sigma}^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2\n$$\n\nThis is an unbiased estimator of $\\sigma^2$. This is because the expected value of the estimator is equal to the population parameter: $E(\\hat{\\sigma}^2) = \\sigma^2$. Really? How? Well, we can use the **linearity of expectation** to show that:\n\n$$\nE(S^2)=E\\left[\\dfrac{\\sigma^2}{n-1}\\cdot \\dfrac{(n-1)S^2}{\\sigma^2}\\right]=\\dfrac{\\sigma^2}{n-1} E\\left[\\dfrac{(n-1)S^2}{\\sigma^2}\\right]=\\dfrac{\\sigma^2}{n-1}\\cdot (n-1)=\\sigma^2\n$$\n\nHere it is demonstrated in ipython:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample Mean: 0.010814943108814246\nSample Variance: 1.0183366045883282\nE(hat(sigma)^2): 0.001019355960548877"
          ]
        }
      ],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate 1000 samples from a normal distribution\nsamples = np.random.normal(0, 1, 1000)\n\n# Calculate the sample mean\nsample_mean = np.mean(samples)\n\n# Calculate the sample variance\nsample_variance = np.var(samples, ddof=1)\n\n# linearity of expectation\nE_hat_sigma_squared = (1/(len(samples)-1)) * np.sum(np.var(samples, ddof=1))\n\n# Print the results\nprint(\"Sample Mean: {}\".format(sample_mean))\nprint(\"Sample Variance: {}\".format(sample_variance))\nprint(\"E(hat(sigma)^2): {}\".format(E_hat_sigma_squared))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Minimum Variance Estimators\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We look at all the unbiased estimators of $\\theta$, and we choose the one with the smallest variance. This is called the **minimum variance estimator**.\n\n-   The less variance, the more accurate the estimator\n\nThe primary influence over the estimator, is still the original distribution.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Estimator Reporting\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When we report an estimator, we have to report the **standard error** of the estimator. This is the standard deviation of the estimator.\n\n-   **$\\hat{\\theta}$ has a normal distribution:** The value of $\\theta$ lies within $\\pm 2 se$ of $\\hat{\\theta}$\n-   **$\\hat{\\theta}$ has a non-normal distribution:** The value of $\\theta$ lies within $\\pm 4 se$ of $\\hat{\\theta}$\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Point Estimation (Methods)\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Method of Moments\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The method of moments is a method to estimate the parameters of a distribution. We use the sample moments to estimate the population moments. In simpler terms, we use the sample statistics to estimate the population parameters.\n\n-   What is a moment? A moment is a function of the random variable $X$: $E(X^k)$ (where $k$ is the order of the moment)\n\nThe way we go about this is by using the following formula:\n\n$$\n\\hat{\\theta} = \\frac{1}{n} \\sum_{i=1}^{n} x_i^k\n$$\n\nWhere $k$ is the order of the moment, and $x_i$ is the $i^{th}$ observation.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Example\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let $X$ be a random variable with a normal distribution with mean $\\mu$ and variance $\\sigma^2$. We take a random sample of size $n$ from the population, and calculate the sample mean $\\bar{x}$. We want to estimate $\\mu$ using the method of moments.\n\nSolution:\n\nThe first step to solving this problem is to find the sample mean $\\bar{x}$:\n\n$$\n\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n$$\n\nIn the above equation we can clearly demonstrate how the method of moments applies. In fact, the definition of the method of moments, if we set $k=1$ is the mean of the sample: $\\frac{1}{n} \\sum_{i=1}^{n} x_i$.\n\nThe next step is to find the sample variance $s^2$:\n\n$$\ns^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2\n$$\n\nFor the above, we can see that the method of moments applies again. If we set $k=2$ and consider $x_i$ to be $x_i - \\bar{x}$, we get the sample variance. There is of course the matter of the $-1$ in the denominator, this can be explained by the fact that the sample variance is an **unbiased** estimator of the population variance.\n\nNow we can use the method of moments to estimate $\\mu$:\n\n$$\n\\hat{\\mu} = \\bar{x}\n$$\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Maximum Likelihood Estimation\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![img](./mle.png)\n\nMaximum likelihood estimation is a method of estimating the parameters of a statistical model, given observations. It uses calculus to find the maximum likelihood of the parameters.\n\nFirst, we need a likelihood function. This is a function of the parameters, which gives the probability of the observations. The likelihood function is defined as:\n\n$$\nL(\\theta) = P(X_1, X_2, \\dots, X_n | \\theta)\n$$\n\nWhere $\\theta$ is the parameter of the distribution. The likelihood function is the probability of the observations, given the parameter.\n\nHow can we get this probability? We can use the following formula:\n\n$$\nL(\\theta) = \\prod_{i=1}^{n} f(x_i | \\theta)\n$$\n\nWhere $f(x_i | \\theta)$ is the probability density function of the distribution, given the parameter $\\theta$. $x_i$ is the $i^{th}$ statistic of the sample.\n\nThe maximum likelihood estimator is the value of the parameter that maximizes the likelihood function. This is represented by the following formula:\n\n$$\n\\hat{\\theta} = \\underset{\\theta}{\\text{argmax}} L(\\theta)\n$$\n\nWe will not be using this formula, but it is a good step to understanding. We will take our likelihood function and wrap a natural log around it. This is called the **log-likelihood function**. The log-likelihood function is defined as:\n\n$$\nl(\\theta) = \\ln L(\\theta)\n$$\n\nWe will then take the derivative of the log-likelihood function, and set it equal to zero. This will give us the maximum likelihood estimator.\n\n> This might seem a bit pointless, but as AI students, this somewhat resembles the process of backpropagation. We take the derivative of the loss function, and set it equal to zero. This gives us the gradient of the loss function, which we can use to update the weights of the neural network. (This is a very basic explanation, but it is a good way to understand the concept)\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Point Estimation (Python)\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For ease, we will use built-in datasets from pandas, such as the iris dataset. We will use the sepal length of the iris dataset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\niris = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv')\n# craete a random sample of size 60\nsample = iris.sample(60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will try to estimate the mean of the sepal length of the iris dataset. We will use the method of moments to estimate the mean.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5.843333333333334"
          ]
        }
      ],
      "source": [
        "sepal_length = iris['sepal_length']\nmean = sepal_length.mean()\nprint(mean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now know the actual mean of the sepal length of the iris dataset. We will now try to estimate the mean using the method of moments.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5.691666666666666"
          ]
        }
      ],
      "source": [
        "# \\hat{\\theta} = \\frac{1}{n} \\sum_{i=1}^{n} x_i^k\n# we use the sample\nmean = sample['sepal_length'].sum() / sample['sepal_length'].size\nprint(mean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we will try to estimate the variance using the maximum likelihood estimator.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Estimated variance: 0.680421052631579, Actual variance: 0.6856935123042507\nError:  -0.005272459672671648"
          ]
        }
      ],
      "source": [
        "iris = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv')\n# craete a random sample of size 60\nn = 10\nsamples = [iris.sample(n*2) for i in range(n)]\nvariances = [sample['sepal_length'].var() for sample in samples]\n\n# we get the probability density function for each variance\nf = [stats.norm.pdf(variances[i], iris['sepal_length'].var(), iris['sepal_length'].std()) for i in range(n)]\n\n# we get the log of the probability density function\nf_log = np.log(f)\n\n# we get the maximum likelihood estimator\nmle = variances[f_log.argmax()]\n\n# instead of argmax, we could use the following method using derivatives:\n# 1. we get the derivative of the log-likelihood function\n# 2. we set the derivative equal to zero\n# 3. we solve for the variance\n\n# we can express the estimated variance as a sum of the actual variance and the error\nprint(f\"Estimated variance: {mle}, Actual variance: {iris['sepal_length'].var()}\")\nprint(\"Error: \", mle - iris['sepal_length'].var())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Single Sample Intervals\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   [Animation](./Animations/cls.mp4)\n-   [DeepAI](https://deepai.org/machine-learning-glossary-and-terms/confidence-interval)\n\nIn this section, we will look at confidence intervals for a single sample. This will combine the idea id random variables, and the idea of sampling distributions.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Confidence Intervals\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is a range between two values, which we are P% confident that the population parameter lies in. To better understand this, here is a very 'boilerplate' example:\n\n1.  We choose a confidence level, $P$.\n2.  We get its z-score\n\n$$\n(\\bar{X} - z_P \\frac{\\sigma}{\\sqrt{n}}, \\bar{X} + z_P \\frac{\\sigma}{\\sqrt{n}})\n$$\n\nWhere $\\bar{X}$ is the sample mean, and $\\sigma$ is the population standard deviation, therefore $\\frac{\\sigma}{\\sqrt{n}}$ is the standard error.\n\nWe can also write this as:\n\n$$\n\\bar{X} \\pm z_P \\frac{\\sigma}{\\sqrt{n}}\n$$\n\nSo what does this tell us? It tells us that we are P% confident that the population mean lies within the interval $\\bar{X} \\pm z_P \\frac{\\sigma}{\\sqrt{n}}$.\n\n-   The more confident we want to be, the larger the confidence level $P$. But, the larger the confidence level, the larger the interval, the lower the precision.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Interpretation\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since we only know $\\bar{x}, \\sigma \\text{ and } n$, we **cannot conclude that** the population mean lies within the interval $\\bar{X} \\pm z_P \\frac{\\sigma}{\\sqrt{n}}$.\n\nWhy? Because we are not using a random sample for the mean. We can only conclude that if we repeated the experiment many times, the result we obtain would occur P% of the time. In other words, **if we get 100 different confidence intervals, $P%$ of them would contain the population mean.**\n\n![img](./Single_Sample_Intervals/2023-03-04_13-30-04_screenshot.png)\n\nDiagram of the process of creating confidence intervals and interpreting them:\n![img](./Single_Sample_Intervals/inchart.png)\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Confidence Levels\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Thus far, we used a variable confidence level $P$. But, we can also use a fixed confidence level, such as 95%. This is the same as using a confidence level of 0.95. (You must use the decimal form, not the percentage form.)\nNormaly, the variable which is used to represent the confidence level is $\\alpha$. So, we can write the confidence interval as:\n\n$$\n\\bar{X} \\pm z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\n$$\n\nWhere $z_{\\alpha/2}$ is the z-score for the confidence level $\\alpha/2$. Why divide by 2? Because we are looking at the area under the curve on both sides of the mean. So, we are looking at the area under the curve for $\\alpha/2$ on each side of the mean.\n\n![img](./Single_Sample_Intervals/2023-03-04_13-49-22_screenshot.png)\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Precision and Sample Size\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we need to define the width of the interval as: $2*z_P \\frac{\\sigma}{\\sqrt{n}}$. This is the width of the interval.\n\n-   Higher the confidence level, the wider the interval.\n-   Higher the sample size, the narrower the interval.\n-   Lower the population standard deviation, the narrower the interval.\n-   Higher the confidence level, the higher the sample size required to achieve a given precision.\n\nWe might want to ensure, that a confidence interval has a certain width. In this case, we can use the following formula:\n\n$$\nn = (2*z_{\\alpha/2} \\frac{\\sigma}{\\text{width}})^2\n$$\n\nThe application of this\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Derivation of the Confidence Interval\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we have a random sample of size $n$ from a population, we can construct a confidence interval for some parameter $\\theta$ using the following steps:\n\n1.  Check if the conditions are met:\n    -   The variable depends on the sample and parameter $\\theta$.\n    -   The probability distribution of the variable is known.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Large Sample Confidence Intervals (Mean & Proportion)\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Previously, we assumed that the population standard deviation $\\sigma$ was known and that the population distribution was normal. If we cannot assume these things, we can use the large sample confidence interval.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Large Sample Confidence Interval for the Mean\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It goes back to the central limit theorem. If we take a random sample of size $n$ from a population, we can assume that the sample mean $\\bar{X}$ is normally distributed. Therefore, we can use the following formula:\n\n$$\nZ = \\frac{\\bar{X} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}}\n$$\n\nWhere $\\mu$ is the population mean, and $\\sigma$ is the population standard deviation. Thus, we can write the confidence interval as:\n\n$$\n\\frac{\\bar{X} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\pm z_{\\alpha/2}\n$$\n\n$$\nP(\\frac{\\bar{X} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\pm z_{\\alpha/2}) \\approx 1 - \\alpha\n$$\n\nthe last equation tells us that we are 100% - $&alpha;$% confident that the population mean lies within the interval $\\frac{\\bar{X} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\pm z_{\\alpha/2}$.\n\nWhat happens if we replace $\\sigma$ with $s$ in the above equation? Since we adding a new random variable to the denominator, we get that:\n\n-   The confidence interval is wider.\n\nBut, if our sample size is large enough, the difference between $\\sigma$ and $s$ is small, and the confidence interval is not much wider. **What is large enough?** If $n \\geq 40$, then the difference between $\\sigma$ and $s$ is small enough.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Large Sample Confidence Interval for Population Proportion\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Up till now we talked about being confident that the mean of a population lies within a certain interval. But, what if we want to be confident that the proportion of a population lies within a certain interval? For example, we want to be 95% confident that the proportion of people who like chocolate is between 0.4 and 0.6. We can use the following formula:\n\n$$\nP(-z_{\\alpha/2} \\leq \\frac{\\hat{p} - p}{\\sqrt{\\frac{p(1-p)}{n}}} \\leq z_{\\alpha/2}) \\approx 1 - \\alpha\n$$\n\nWhere $\\hat{p}$ is the sample proportion, $p$ is the population proportion, and $n$ is the sample size. Since we are talking about proportion, we are also talking about probability, and can use the binomial distribution, where $n$ is the number of trials, and $p$ is the probability of success. Remember that:\n\n\\begin{align}\n\\hat{p} = \\frac{X}{n} \\\\\nE(X) = np \\\\\nVar(X) = np(1-p)\n\\end{align}\n\nAn important rule to remember is that the sample proportion is approximately normally distributed if $np \\geq 10$ and $n(1-p) \\geq 10$.\n\nThe general formula for a confidence interval for a population proportion is:\n\n$$\n\\hat{p} \\pm z_{\\alpha/2} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\n$$\n\nThis formula can only be used if the sample size is large enough, that is if it is above 40.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### One Sided\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All previous confidence intervals talked about two bounds, one on the left and one on the right. But, what if we want to be confident that the population mean is greater than a certain value? For example, we want to be 95% confident that the population mean is within a certain range above the sample mean. We can use the following formula:\n\n$$\n\\mu < \\bar{X} + z_{\\alpha} \\frac{\\sigma}{\\sqrt{n}}\n$$\n\nWhere $\\mu$ is the population mean, $\\bar{X}$ is the sample mean, $\\sigma$ is the population standard deviation, and $n$ is the sample size.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Confidence Intervals for Normal Distributions\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can assume that the population follows a normal distribution, that in only if $n$ is large enough, (viz the central limit theorem). If we have a sample of size $n$, then the sample mean $\\bar{X}$ is approximately normally distributed with mean $\\mu$ and standard deviation $\\frac{\\sigma}{\\sqrt{n}}$. We can use the following formula to calculate the confidence interval:\n\n$$\n\\bar{X} \\pm z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\n$$\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Confidence Interval for the t-Distribution\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we have a sample for which the mean is $\\bar{X}$ and the standard deviation is $s$, then we can define a random variable $T$ as:\n\n$$\nT = \\frac{\\bar{X} - \\mu}{\\frac{s}{\\sqrt{n}}}\n$$\n\nThe distribution of $T$ is called the **Student's t-distribution**. The t-distribution is similar to the normal distribution, but it has fatter tails. The t-distribution is used when the population standard deviation is unknown, and the sample size is small. The t-distribution is also used when the population distribution is not normal.\n\nWhat are degrees of freedom? The degrees of freedom is the number of independent pieces of information in a sample. For example, if we have a sample of size $n$, then the degrees of freedom is $n-1$.\n\nSome key properties of the t-distribution:\n\n-   It is more spread out than the normal distribution.\n-   The higher $df$ is, the more similar the t-distribution is to the normal distribution.\n\nConfidence interval for the mean using the t-distribution will then be given by this expression:\n\n$$\n\\bar{X} \\pm t_{\\alpha, df} \\frac{s}{\\sqrt{n}}\n$$\n\nWhere $df$ is the degrees of freedom, and $s$ is the sample standard deviation and $\\alpha = 1 - \\text{confidence level}$.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prediction Interval for Future Values\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we finally get to discuss future values of some variable rather than estimating what might be the mean of a population.\n\n1.  We have a random sample of size $n$. ($X_1, X_2, \\dots, X_n$)\n2.  Now we want to know $X_{n+1}$.\n\n$$\nE(\\bar{X} - X_{n+1}) = 0\n$$\n\n$$\nVar(\\bar{X} - X_{n+1}) = \\frac{\\sigma^2}{n} + \\sigma^2 = \\sigma^2(1 + \\frac{1}{n})\n$$\n\nGiven the above, we can calculate a z-score for the confidence interval:\n\n$$\nZ = \\frac{(\\bar{X} - X_{n+1}) - 0}{\\sigma^2 \\frac{1}{\\sqrt{n}}}\n$$\n\n&#x2026;\n\n$$\nT = \\frac{(\\bar{X} - X_{n+1})}{S\\sqrt{1 \\frac{1}{n}}}\n$$\n\nFrom this, we can build a confidence interval for the future value of $X_{n+1}$:\n\n$$\n\\bar{x} \\pm t_{\\alpha, df} s \\sqrt{1 + \\frac{1}{n}}\n$$\n\nWe interpret this the same way as we did for the confidence interval for the mean. We are 95% confident that for multiple iterations, the future value of $X_{n+1}$ will be between the two bounds.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bootstrap\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Variance and Standard Deviation Confidence Intervals\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we have a normal distribution, we might also be interested in finding the variance of the population if we do not have it. Given a sample of size $n$, we can define a random variable as follows:\n\n$$\n\\frac{(n-1)s^2}{\\sigma^2} = \\frac{\\sum(X_i - \\bar{X})^2}{\\sigma^2}\n$$\n\nWhere $s$ is the sample standard deviation, and $\\sigma$ is the population standard deviation. The distribution of this random variable is called the **chi-squared distribution**. The chi-squared distribution is used to find confidence intervals for the variance of a population.\n\nFor this distribution, we must also define the degrees of freedom. The degrees of freedom is $n-1$.\n\n![img](Large_Sample_Confidence_Intervals_(Mean_&_Proportion)/2023-03-11_12-13-02_.png)\n\nKey properties of the chi-squared distribution:\n\n-   It is more spread out than the normal distribution.\n-   Only positive values are possible.\n-   The higher $df$ is, the more similar the chi-squared distribution is to the normal distribution.\n\nFinally, the confidence interval will look like this:\n\n$$\n\\frac{(n-1)s^2}{\\chi^2_{\\alpha/2, df}} \\leq \\sigma^2 \\leq \\frac{(n-1)s^2}{\\chi^2_{1-\\alpha/2, df}}\n$$\n\nWe now have to calculate the confidence interval for the variance of the population. We can do this by using the following formula:\n\n-   Lower bound: $\\frac{(n-1)s^2}{\\chi^2_{\\alpha/2, df}}$\n-   Upper bound: $\\frac{(n-1)s^2}{\\chi^2_{1-\\alpha/2, df}}$\n\nWhere $\\chi^2_{\\alpha/2, df}$ is the $\\alpha/2$ percentile of the chi-squared distribution with $df$ degrees of freedom.\n\nWe can also calculate the confidence interval for the standard deviation of the population. We can do this by using the following formula:\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Confidence Intervals (Python)\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can make our life easier by using ipython to calculate confidence intervals. We will use the following packages:\n\n-   scipy.stats\n-   numpy\n-   pandas\n-   matplotlib\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Simple Confidence Intervals for the Mean\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For ease, we will use built-in datasets from pandas, such as the iris dataset. We will use the sepal length of the iris dataset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0    5.1\n1    4.9\n2    4.7\n3    4.6\n4    5.0\nName: sepal_length, dtype: float64"
          ]
        }
      ],
      "source": [
        "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n\niris = pd.read_csv(\"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv\")\nsepal_length = iris[\"sepal_length\"]\nprint(sepal_length.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we have, the data. Lets create a confidence interval for the mean of the sepal length. We will use a confidence level of 95%.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(4.2257725250400755, 7.460894141626592)"
          ]
        }
      ],
      "source": [
        "interval = stats.norm.interval(0.95, loc=np.mean(sepal_length), scale=np.std(sepal_length))\nprint(interval)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that the confidence interval is between 4.23 and 7.46.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Confidence Interval for the Population Proportion\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will use the same dataset as before, but this time we will use the sepal width. We will use a confidence level of 95%. In the first example we do not approximate, we use the exact formula.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(0.0734406885907721, 0.17989264474256125)"
          ]
        }
      ],
      "source": [
        "from statsmodels.stats.proportion import proportion_confint\n# proportion where the sepal width is greater than 3.5\nX = np.sum(iris[\"sepal_width\"] > 3.5)\nn = len(iris[\"sepal_width\"])\np = X/n\n\ninterval = proportion_confint(X, n, alpha=0.05, method=\"normal\")\nprint(interval)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `method` parameter can be either `normal` or `wilson`.\n\n-   `normal` uses the normal approximation. We use this one if $np \\geq 10$ and $n(1-p) \\geq 10$.\n-   `wilson` uses the Wilson score interval. If the conditions are not met, we use this one.\n\nWe can now try to approximate with the normal distribution. We will use the same confidence level of 95%.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(0.07344068859077212, 0.17989264474256123)"
          ]
        }
      ],
      "source": [
        "scale = np.sqrt(p*(1-p)/n)\ninterval = stats.norm.interval(0.95, loc=p, scale=scale)\nprint(interval)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now lets take a look at binomial approximation for the confidence interval. We will use the same confidence level of 95%. It is important to check if the conditions are met, that is if $np \\geq 10$ and $n(1-p) \\geq 10$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "np >= 10:  True\nn(1-p) >= 10:  True"
          ]
        }
      ],
      "source": [
        "# conditions test\nprint(\"np >= 10: \", n*p >= 10)\nprint(\"n(1-p) >= 10: \", n*(1-p) >= 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can use the binomial approximation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(11.0, 27.0)\n[0.07333333333333333, 0.18]"
          ]
        }
      ],
      "source": [
        "interval = stats.binom.interval(0.95, n, p)\nprint(interval)\ninterval = [x/n for x in interval]\nprint(interval)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The last step is very important. We need to divide the interval by the sample size to get the proportion interval. We can see that the interval is between 0.073 and 0.18, which is a very close approximation to the normal approximation.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### t Distribution Confidence Intervals\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will use the same dataset as before, but this time we will use the petal length. We will use a confidence level of 95%. In the first example we do not approximate, we use the exact formula.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from statsmodels.stats.weightstats import _tconfint_generic\n# proportion where the sepal width is greater than 3.5\nX = np.sum(iris[\"petal_length\"] > 3.5)\nn = len(iris[\"petal_length\"])\np = X/n\n\ninterval = _tconfint_generic(p, np.sqrt(p*(1-p)/n), n-1, 0.05, 'two-sided')\nprint(interval)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prediction Interval for Future Values\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will use the same dataset as before, but this time we will use the petal width. We will use a confidence level of 95%. In this example we are trying to predict the future value of the petal width.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1.0767639167319225, 1.3219027499347447)"
          ]
        }
      ],
      "source": [
        "from statsmodels.stats.weightstats import _tconfint_generic\n\n\nsample_mean = np.mean(iris[\"petal_width\"])\nsample_std = np.std(iris[\"petal_width\"])\nn = len(iris[\"petal_width\"])\nalpha = 0.05\n\nt_score = stats.t.ppf(1-alpha/2, n-1) # t score for 95% confidence\ninterval = (sample_mean - t_score*sample_std/np.sqrt(n), sample_mean + t_score*sample_std/np.sqrt(n))\nprint(interval)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So we can now be 95% confident that the future value of the petal width will be between 1.08 and 1.32.\nHow can we validate this? We can use the bootstrap method.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1.02796667 1.36133333]"
          ]
        }
      ],
      "source": [
        "from sklearn.utils import resample\n\n# bootstrap\nn_iterations = 1000\nn_size = int(len(iris[\"petal_width\"]) * 0.50)\nmedians = list()\nfor i in range(n_iterations):\n  s = resample(iris[\"petal_width\"], n_samples=n_size)\n  m = np.mean(s)\n  medians.append(m)\n\ninterval = np.percentile(medians, [2.5, 97.5])\nprint(interval)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Confidence Interval for the Population Variance\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will use the same dataset as before, but this time we will use the petal width. We will use a confidence level of 95%.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "s = np.std(iris[\"petal_width\"])\nvar = s**2\nn = len(iris[\"petal_width\"])\nalpha = 0.05\nucl = (n-1)*var/stats.chi2.ppf(alpha/2, n-1)\nlcl = (n-1)*var/stats.chi2.ppf(1-alpha/2, n-1)\ninteval = (lcl, ucl)\nprint(interval)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Midterm Review\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is a review of the key concepts mentioned in the notes thus far. It is not a complete list of all the concepts that will be on the midterm. It is meant to be a guide to help you study for the midterm.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Random Variables\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Its a bit like musical chairs, as the music plays, we do not know the final outcome of the people that will end up sitting. Using this analogy, a random sample is a subset of the population that we are interested in. The random variable comes once we use that random sample to compute some statistic.\n\n-   A random variable is a variable whose value is determined by chance.\n-   A random variable can be discrete or continuous.\n-   When is it representative?\n    -   No replacement sampling\n    -   Large enough size (at most 5%)\n\n\\begin{align}\n\\mu_\\bar{x} &= \\mu \\\\\n\\sigma_\\bar{x} &= \\frac{\\sigma}{\\sqrt{n}}\n\\end{align}\n\nThe variance of a random variable is called the **standard error**.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Central Limit Theorem\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we take a sampling distribution of the sample mean. As long as the following are true, the sampling distribution of the sample mean will be approximately normal:\n\n-   The population is uni-modal\n-   Our sample size is large enough ($n \\geq 30$)\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Estimation\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We try to estimate some population parameter using a sample statistic.\n\n$$\n\\hat{\\theta} = \\theta + \\epsilon\n$$\n\nWhere $\\epsilon$ is the error term. The error term is the difference between the population parameter and the sample statistic.\n\n-   **Method of Moments:** We can use this method to computer the estimate given the following formula: $\\hat{\\theta} = \\frac{1}{n}\\sum_{i=1}^n x_i^k$. All we need is the **order of the moment** and $x$, the sample.\n-   **Maximum Likelihood Estimator:** \n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Confidence Interval Distributions\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is important to know which distribution to use when constructing a confidence interval for some population parameter.\n\n![img](./confint-dist-decision.png)\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Single Sample Hypothesis Testing\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hypothesis Testing\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is a process of taking some sample data and using it to make a decision about the population parameter. We start by forming a null hypothesis and an alternative hypothesis.\n\n-   **Null Hypothesis:** The hypothesis that we are trying to disprove. It is usually the status quo. Can also be called the **boring hypothesis**.\n-   **Alternative Hypothesis:** The hypothesis that we are trying to prove. It is usually the new idea. Can also be called the **researchers hypothesis**.\n\nWe talk about some population parameter $\\theta$. We can write the hypotheses as follows:\n\n| Null Hypothesis|Alternative Hypothesis|\n|---|---|\n| $\\theta = \\theta_0$|$\\theta \\neq \\theta_0$|\n| $\\theta \\leq \\theta_0$|$\\theta > \\theta_0$|\n| $\\theta \\geq \\theta_0$|$\\theta < \\theta_0$|\n\nWe then denote the hypotheses pair as follows:\n\n\\begin{align}\nH_0: \\theta = \\theta_0 \\\\\nH_a: \\theta \\neq \\theta_0\n\\end{align}\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Procedures\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use a test procedure to determine if we should or should not reject the null hypothesis. The test procedure is a set of rules.\n\nThe main question is: if H<sub>0</sub> is actually true, how likely is it that we would get a sample that contradicts the hypothesis as much as our current sample does?\n\nWe use a **test statistic** to answer this question. We can use the following test statistics:\n\n-   **Z-Test:** Used when we are estimating the mean of a normal distribution or the proportion of a binomial distribution.\n-   **t-Test:** Used when we are estimating the mean of a non-normal distribution.\n-   **Chi-Square Test:** Used when we are estimating the variance of a normal distribution.\n\nWith these statistics, we compute a **p-value**. The p-value is the probability of getting a sample that contradicts the null hypothesis as much as our current sample does. **If the p-value is less than our significance level, we reject the null hypothesis.**\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Type I and Type II Errors\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can make two types of errors when we are doing hypothesis testing:\n\n-   **Type I Error:** Rejecting the null hypothesis when it is actually true.\n-   **Type II Error:** Failing to reject the null hypothesis when it is actually false.\n\nHere is a nice table from AP Stats to help you better remember the difference between the two errors:\n\n![img](Single_Sample_Hypothesis_Testing/2023-03-25_10-45-23_.png)\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Significance Level\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The significance level is the probability of making a type I error. It is denoted by $\\alpha$. The significance level is usually set to 0.05 or 0.01.\n\nWe also have $\\beta$, which is the probability of making a type II error. We can compute $\\beta$ using the following formula:\n\n$$\n\\beta = P(\\text{Reject } H_0 \\text{ when } H_0 \\text{ is false})\n$$\n\nNow we can put it all together:\n\n1.  We form a null hypothesis and an alternative hypothesis.\n2.  We compute a test statistic.\n3.  We compute a p-value.\n4.  We compare the p-value to the significance level.\n5.  If the p-value is less than the significance level, we reject the null hypothesis.\n\nNow lets look at some of those test statistics.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hypothesis Testing for Means\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use the Z-Test to test hypotheses about the mean of a normal distribution. This statistic will vary, depending of it we know the population standard deviation or not.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Hypothesis Testing for Normal Distributions with Known Standard Deviation\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We took some sample data and we want to test the hypothesis that the population mean is equal to some value. We can use the following formula to compute the test statistic:\n\n$$\nZ = \\frac{\\bar{x} - \\mu_0}{\\frac{\\sigma}{\\sqrt{n}}}\n$$\n\nNow to compute the p-value, we need to pay attention to the alternative hypothesis, here is a table to help you remember the different cases:\n\n| Alternative Hypothesis|P-Value|\n|---|---|\n| $\\theta \\neq \\theta_0$|$2[1 - \\Phi(Z)]$|\n| $\\theta \\leq \\theta_0$|$\\Phi(Z)$|\n| $\\theta \\geq \\theta_0$|$1 - \\Phi(Z)$|\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### $\\beta$ and Sample Size Determination\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Large Sample Tests\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we have a large sample size (n > 30), we can use the following formula to compute the test statistic:\n\n$$\nZ = \\frac{\\bar{x} - \\mu_0}{S / \\sqrt{n}}\n$$\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hypothesis Testing with t Distribution\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hypothesis Testing for Proportions\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hypothesis Testing for Variances\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Two Sample Hypotheses Testing\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hypothesis Testing for Two Means\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hypothesis Testing for Two Proportions\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hypothesis Testing for Two Variances\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis of Variance: Single Factor\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analysis of Variance\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analysis of Variance for Normal Distributions\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analysis of Variance for Proportions\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analysis of Variance for Variances\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis of Variance: Multi Factor\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analysis of Variance for Two Factors\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analysis of Variance for Three Factors\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Goodness-of-fit Tests\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Goodness-of-fit Tests for Normal Distributions\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Goodness-of-fit Tests for Proportions\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Goodness-of-fit Tests for Variances\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Categorical Data Analysis\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Chi-Square Tests for Independence\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Chi-Square Tests for Homogeneity\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Chi-Square Tests for Goodness-of-fit\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<footer style=\"height: 20vh;\"></footer>\n\n"
      ]
    }
  ],
  "metadata": [
    [
      "org"
    ],
    null,
    null
  ],
  "nbformat": 4,
  "nbformat_minor": 0
}
