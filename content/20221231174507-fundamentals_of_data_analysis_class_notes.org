:PROPERTIES:
:ID:       4ea2e0d3-f2f4-4083-b3e3-7ea638872d56
:END:
#+title: Fundamentals of Data Analysis - Class Notes
#+HTML_HEAD: <link rel="stylesheet" href="https://alves.world/org.css" type="text/css">
#+HTML_HEAD: <style type="text/css" media="print"> body { visibility: hidden; display: none }  </style>
#+HTML_HEAD: <style type="text/css"> .sideline {    /* only if on a wide screen */    @media (min-width: 768px) {        border-left: 1px solid #ddd;        position: absolute;        left: 2em;        width: 20vw;    }} </style>
#+OPTIONS: toc:2
#+HTML_HEAD: <script src="https://alves.world/tracking.js" ></script>
#+HTML_HEAD: <script src="anti-cheat.js"></script>
#+HTML: <script data-name="BMC-Widget" data-cfasync="false" src="https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js" data-id="velocitatem24" data-description="Support me on Buy me a coffee!" data-message="" data-color="#5F7FFF" data-position="Right" data-x_margin="18" data-y_margin="18"></script>
#+HTML: <script>setTimeout(() => {alert("Finding this useful? Consider buying me a coffee! Bottom right cornner :) Takes just a few seconds")}, 60*1000);_paq.push(['trackEvent', 'Exposure', 'Exposed to beg']);</script>

#+begin_src ipython :session test
x = [1,2,3,4,5]
y = [1,4,9,16,25]
print([a-b for a,b in zip(x,y)])
#+end_src

*New* search feature! Make use of the amazing [[https://en.wikipedia.org/wiki/Approximate_string_matching][fuzzy search]] algorithm. Just type in the search box and it will find the closest match in the page. Hit =Enter= to jump to the next match. Lmk if it doesn't work for you.
#+HTML: <input id="search" type="text" placeholder="Search" /> <span id="resultCount"></span>
#+HTML: <script src="https://alves.world/fuzzy.js"></script>


Useful Resources:
+ [[https://seeing-theory.brown.edu/frequentist-inference/index.html#section1][Seeing Theory - Frequentist Inference]]
+ [[https://online.stat.psu.edu/stat415/][Penn State Notes]]

# [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb)
#+HTML: <a href="https://colab.research.google.com/github/velocitatem/university-study-notes/blob/master/content/20221231174507-fundamentals_of_data_analysis_class_notes.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

* Data Manipulation & ipython
** Pandas DataFrames
It is best to imagine a DataFrame as spreadsheet.

#+BEGIN_SRC ipython
  import pandas as pd

  # Create a DataFrame from a dictionary
  df = pd.DataFrame({'name': ['John', 'Jane', 'Joe'],
                    'age': [34, 25, 67],
                    'height': [1.78, 1.65, 1.89]})

  # Create a DataFrame from a list of lists
  df = pd.DataFrame([['John', 34, 1.78],
                    ['Jane', 25, 1.65],
                    ['Joe', 67, 1.89]],
                    columns=['name', 'age', 'height'])

#+END_SRC

Here are some of the most useful methods for working with DataFrames:
+ =.head()= :: returns the first 5 rows of the DataFrame
+ =.tail()= :: returns the last 5 rows of the DataFrame
+ =.transpose()= :: returns the transpose of the DataFrame
+ =.plot.scatter(/cols/)= ::
+ =.shape= :: returns the number of rows and columns in the DataFrame
+ =.dtypes= :: returns the data types of each column in the DataFrame

** Filtering DataFrames
#+begin_src ipython
  # Create a DataFrame from a dictionary
  df = pd.DataFrame({'name': ['John', 'Jane', 'Joe'],
                    'age': [34, 25, 67],
                    'height': [1.78, 1.65, 1.89]})

  # Filter the DataFrame to only include people over 30
  df[df['age'] > 30]
#+end_src

The general syntax for filtering a DataFrame is: =df[condition]=. The =condition= is a boolean expression that evaluates to either =True= or =False= for each row in the DataFrame. The result of the filter is a new DataFrame containing only the rows where the condition is =True=.

We can also use the =.loc= method to filter a DataFrame. The =.loc= method takes a list of row labels and a list of column labels as arguments. The result is a new DataFrame containing only the rows and columns specified. Here is an example:

#+begin_src ipython
  # Create a DataFrame from a dictionary
  df = pd.DataFrame({'name': ['John', 'Jane', 'Joe'],
                    'age': [34, 25, 67],
                    'height': [1.78, 1.65, 1.89]})

  # Filter the DataFrame to only include people over 30
  df.loc[df['age'] > 30, ['name', 'age']]
#+end_src

The main difference between the =.loc= method and the =[]= operator is that the =.loc= method can be used to filter rows and columns at the same time. We might want to do that if we want to filter a DataFrame by rows and then select a subset of the columns.

** =np.where()=
The =np.where()= function is a vectorized version of the =if= statement. It takes a condition, a value to return if the condition is =True=, and a value to return if the condition is =False=. Here is an example:

#+begin_src ipython :results output :exports both
  import numpy as np

  # Create a NumPy array
  arr = np.array([1, 2, 3, 4, 5])

  # Use np.where() to replace all values less than 3 with 0
  res = np.where(arr < 3, 0, arr)
  print(res)
#+end_src

#+RESULTS:
: [0 0 3 4 5]

** Sorting
We can use the =.sort_values()= method to sort a DataFrame by one or more columns. Here is an example:

#+begin_src ipython :results output :exports both
  import pandas as pd
  # Create a DataFrame from a dictionary
  df = pd.DataFrame({'name': ['John', 'Jane', 'Joe'],
                    'age': [34, 25, 67],
                    'height': [1.78, 1.65, 1.89]})

  # Sort the DataFrame by age
  print(df.sort_values('age'))
#+end_src

#+RESULTS:
:    name  age  height
: 1  Jane   25    1.65
: 0  John   34    1.78
: 2   Joe   67    1.89

** Grouping
To avoid redundant filtering and aggregation, we can use the =.groupby()= method to group a DataFrame by one or more columns. Here is an example:

#+begin_src ipython :results output :exports both
  import pandas as pd
  # Create a DataFrame from a dictionary
  df = pd.DataFrame({'name': ['John', 'Jane', 'Joe'],
                    'age': [34, 25, 67],
                    'gender': ["M", "F", "M"],
                    'height': [1.78, 1.65, 1.89]})

  # Group the DataFrame by gender
  print(df.groupby('gender').describe())
  # Group the DataFrame by gender and calculate the mean of each group
  print(df.groupby('gender').mean())
  # calculate the mean age for each gender
  print(df.groupby('gender')['age'].mean())
#+end_src

#+RESULTS:
#+begin_example
         age                                ... height
       count  mean        std   min    25%  ...    min     25%    50%     75%   max
gender                                      ...
F        1.0  25.0        NaN  25.0  25.00  ...   1.65  1.6500  1.650  1.6500  1.65
M        2.0  50.5  23.334524  34.0  42.25  ...   1.78  1.8075  1.835  1.8625  1.89

[2 rows x 16 columns]
         age  height
gender
F       25.0   1.650
M       50.5   1.835
gender
F    25.0
M    50.5
Name: age, dtype: float64
#+end_example

* ipython: Descriptive Statistics
#+begin_src ipython
import matplotlib.pyplot as plt
plt.style.use("seaborn")
#+end_src

We will be

** Histograms
#+begin_src ipython
df['some_values'].hist(bins=15, edgecolor='white')
#+end_src

We can also set some other parameters such as the title and labels:

#+begin_src ipython
  plt.title('Some Title')
  plt.xlabel('Some X Label')
  plt.ylabel('Some Y Label')
#+end_src

** Histograms: Side by Side
If we have two different groups of data, we can plot them side by side:

#+begin_src ipython
  group1 = DataFrame
  group2 = DataFrame
  plt.hist([group1, group2], bins=15, edgecolor='white', label=['Group 1', 'Group 2'])
  plt.legend()
#+end_src

** Bar Plots
We can also plot bar plots (they are very similar to histograms, but plot the frequency of categorical data):

#+begin_src ipython
  categories = ['A', 'B', 'C', 'D']
  frequencies = [10, 20, 30, 40]
  plt.bar(categories, frequencies, edgecolor='white')
#+end_src

** Box Plots
Box plots are a great way to visualize the distribution of data. They are very useful for comparing different groups of data.

#+begin_src ipython
  plt.boxplot([group1, group2])
  plt.xticks([1, 2], ['Group 1', 'Group 2'])
#+end_src

** Annotations
We can also add annotations to our plots:

#+begin_src ipython
  plt.annotate('Some Text', xy=(x, y), xytext=(x, y), arrowprops={'arrowstyle': '->'})
#+end_src

The =xy= and =xytext= parameters are the coordinates of the text and the arrow, respectively.

** Centrality and Spread
We can use the =mean= and =median= functions to calculate the mean and median of a dataset:

#+begin_src ipython
  mean = df['some_values'].mean()
  median = df['some_values'].median()
#+end_src

We can also use the =std= function to calculate the standard deviation:

#+begin_src ipython
  std = df['some_values'].std()
#+end_src

To get a summary of the descriptive statistics of a dataset, we can use the =describe= function:

#+begin_src ipython
  df['some_values'].describe()
#+end_src

All of these functions are methods on the DataFrame object.


+ Minimum :: =df['some_values'].min()=
+ Quartile :: =df['some_values'].quantile(0.25)=
+ IQR :: =df['some_values'].quantile(0.75) - df['some_values'].quantile(0.25)=
+ Mode :: =df['some_values'].mode()=
+ Skew ::  =df['some_values'].skew()=
** Using =numpy=

For each of the following methods, we need to pass the dataframe column as a numpy array:
+ =np.mean= :: The mean of the array
+ =np.median= :: The median of the array
+ =np.std= :: The standard deviation of the array
+ =np.var= :: The variance of the array
+ =np.percentile= :: The percentile of the array
+ =np.quantile= :: The quantile of the array
+ =np.corrcoef= :: The correlation coefficient of the array

** Using =scipy.stats=
Here we assume it is imported as =ss=. We can use the following methods:

+ =ss.mode= :: The mode of the array
+ =ss.skew= :: The skew of the array
+ =ss.iqr= :: The interquartile range of the array
+ =ss.pearsonr= :: The Pearson correlation coefficient of two arrays

* Statistical Distributions
A statistic is a metric, which can be calculated for any sample. Before that sample is collected, we do not know what the values are going to be. That is why we can represent a statistic as a *random variable*.

For example, the sample mean of a distribution, before we actually take the samples, is going to be $\bar{X}$. Once we take the samples, and calculate the statistics, we get $\bar{x}$.

Since any statistic can also be a random variable, we can make distributions for these random variables. This distribution, is called the *sampling distribution*.

** Random Samples
#+HTML: <center>
#+HTML: <iframe class="centeredIframe" scrolling="NO" src="https://www.statcrunch.com/app/index.html?launch=scapps&amp;app=samplingdist&amp;dist=uniform&amp;firststat=0&amp;secondstat=1" title="Sampling distributions applet" width="650" height="700" frameborder="0"></iframe>
#+HTML: </center>
So what determines the distribution of a statistic? It is determined by the *random samples* that we take from the population. If we take a random sample from a population, and calculate the statistic, we get a value. If we take another random sample, and calculate the statistic, we get another value. And so on.

The key factors which determine the distribution of a statistic are:
+ The size of the sample
+ The distribution of the population
+ Sampling method

For our sample to be representative or valid, they must be *independent* and *identically distributed*. This means that the samples must be independent of each other, and the distribution of the population must be the same for each sample.

These conditions will be satisfied if:
+ We have no replacement
+ We have a large enough sample size
Generally, if at most, we sample 5% of the populations, we can assume that the X_i distribution is a random sample.

Here is an implementation of the example 5.12 from the book:
#+begin_src ipython
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style('whitegrid')
mu = 106
variance = 244
sigma = np.sqrt(variance)
og_population = {
    80: 0.2,
    100: 0.3,
    120: 0.5
}
samples = np.arange(10, 110, 30)
fig, axes = plt.subplots(1, len(samples), figsize=(15, 5))
for sampleSize in samples:
    sample_means = []
    for i in range(1000):
        sample = np.random.choice(list(og_population.keys()), size=sampleSize, p=list(og_population.values()))
        sample_mean = np.mean(sample)
        sample_means.append(sample_mean)
    sns.distplot(sample_means, ax=axes[samples.tolist().index(sampleSize)])
    axes[samples.tolist().index(sampleSize)].set_title('Sample Size: {}'.format(sampleSize))
    axes[samples.tolist().index(sampleSize)].set_xlabel('Sample Mean')
    axes[samples.tolist().index(sampleSize)].set_ylabel('Probability')
plt.show()
#+end_src

And here is the output:

[[./sampling-distributions-5.21-extra.png]]

You can see that as the sample size increases, the distribution of the sample means becomes more normal (I think).

** Derivation
Let's say we have a population with a mean of $\mu$, a standard deviation of $\sigma$ and any probability distribution. We take a random sample of size $n$ from this population. We calculate the sample mean, and we get $\bar{x}$. We can represent this as a random variable, $\bar{X}$.
We have to consider all the possible values of $\bar{x}$, and their probabilities. From this, we can then calculate the distribution of $\bar{X}$.

To now calculate the statistics for the distribution of $\bar{X}$, we can use the following formulas
+ Mean :: $\mu_{\bar{X}} = \mu$
+ Variance :: $\sigma_{\bar{X}}^2 = \frac{\sigma^2}{n}$ (this is also called the *standard error [se]*)
** Sample Mean
The sample mean is the most common statistic. It is the average of the sample. It is also the most common statistic to use in hypothesis testing.

We previously defined the mean and variance for sampling distributions. Now we change that up a bit. We first sum up all the random statistics $T_O = X_0 + X_1 + \dots + X_n$. From there on, we can get the expected value and variance of this *sample total*:
+ Expected Value :: $E(T_O) =n \mu$
+ Variance :: $V(T_O) = n \sigma^2$

** Central Limit Theorem
The central limit theorem states that the sampling distribution of the sample mean will be approximately normal, as long as the sample size is large enough.


#+DOWNLOADED: https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fmiro.medium.com%2Fmax%2F3796%2F1*AhMCbLVd5s82XV6M4KTK6A.png&f=1&nofb=1&ipt=97b377a92d82bc9e139bde10c247423e784e7efe723096cc5cbb9fa2013d7e78&ipo=images @ 2023-02-07 13:00:52
[[file:./Statistical_Distributions/2023-02-07_13-00-52_.png]]


| Population | Sample Size | Sample |
|------------+-------------+--------|
| Normal     | Any         | Normal |
| Unknown    | Huge        | Normal |

We can infer from the CLT, that with a higher $n$, we will have a lower standard error.

\[
\sigma_{\bar{X}} = \frac{\sigma}{\sqrt{n}}
\]

The conditions to satisfy the CLT if the population is not normal are:
+ The population must be *unimodal*
+ The sample size must be large enough (usually $n \geq 30$)

** Linear Combinations
If we have a random variable $X$, and come constants $c$, we can define a new random variable $Y$ as a linear combination of $X$ and $c$:

\[
Y = c_1 X_1 + c_2 X_2 + \dots + c_n X_n
\]

Where the expected value and variance of $Y$ are:

\[
E(Y) = c_1 E(X_1) + c_2 E(X_2) + \dots + c_n E(X_n)
\]

\[
V(Y) = c_1^2 V(X_1) + c_2^2 V(X_2) + \dots + c_n^2 V(X_n)
\]

For the above, we assume that the $X_i$ are independent of each other. If they are not, we have to add the covariance terms.

\[
V(Y) = c_1^2 V(X_1) + c_2^2 V(X_2) + \dots + c_n^2 V(X_n) + 2c_1c_2Cov(X_1, X_2) + \dots + 2c_1c_nCov(X_1, X_n) + \dots + 2c_2c_nCov(X_2, X_n)
\]

* Point Estimation
With point estimation, we are trying to estimate a single value, which is the best estimate of the population parameter. We can use the sample statistics to do this.

The core idea is that if we take a random sample from a population, and calculate the sample statistics, *also a random variable*, we can use that to estimate the population parameter.

** Properties
\begin{align}
\text{Estimator} = \bar{X} \quad \text{Estimate} = \bar{x} \quad \text{Population Parameter} = \mu \\
\end{align}


Generally, any estimator $\hat{\theta}$ is just a function of the population parameter $\theta$.

\[
\hat{\theta} = \theta + \epsilon
\]

Where $\epsilon$ is the error term. This error term is the difference between the estimator and the actual population parameter.

A way to measure the *accuracy* of an estimator is to use the *mean squared error*:

\[
MSE = \frac{1}{n} \sum_{i=1}^{n} (\hat{\theta} - \theta)^2
\]

The smaller the MSE, the better the estimator.

** Estimator Bias
An estimator is unbiased only if the expected value of the estimator is equal to the population parameter. This is represented by the following formula:

\[
E(\hat{\theta}) = \theta
\]

If there is any difference, that difference is the bias of the estimator.

If $X$ is a random variable given by a *binomial* distribution, then $\hat{p} = \frac{X}{n}$ is an unbiased estimator of $p$.

We previously defined the estimate for the mean, now lets take a look at the estimate for the variance:

\[
\hat{\sigma}^2 = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})^2
\]

This is an unbiased estimator of $\sigma^2$. This is because the expected value of the estimator is equal to the population parameter: $E(\hat{\sigma}^2) = \sigma^2$. Really? How? Well, we can use the *linearity of expectation* to show that:

\[
E(S^2)=E\left[\dfrac{\sigma^2}{n-1}\cdot \dfrac{(n-1)S^2}{\sigma^2}\right]=\dfrac{\sigma^2}{n-1} E\left[\dfrac{(n-1)S^2}{\sigma^2}\right]=\dfrac{\sigma^2}{n-1}\cdot (n-1)=\sigma^2
\]

Here it is demonstrated in ipython:

#+BEGIN_SRC ipython :results output
import numpy as np
import matplotlib.pyplot as plt

# Generate 1000 samples from a normal distribution
samples = np.random.normal(0, 1, 1000)

# Calculate the sample mean
sample_mean = np.mean(samples)

# Calculate the sample variance
sample_variance = np.var(samples, ddof=1)

# linearity of expectation
E_hat_sigma_squared = (1/(len(samples)-1)) * np.sum(np.var(samples, ddof=1))

# Print the results
print("Sample Mean: {}".format(sample_mean))
print("Sample Variance: {}".format(sample_variance))
print("E(hat(sigma)^2): {}".format(E_hat_sigma_squared))
#+END_SRC

#+RESULTS:
: Sample Mean: 0.05227371951431922
: Sample Variance: 1.0050258720852923
: E(hat(sigma)^2): 0.0010060319039892815



** Minimum Variance Estimators
We look at all the unbiased estimators of $\theta$, and we choose the one with the smallest variance. This is called the *minimum variance estimator*.
+ The less variance, the more accurate the estimator

The primary influence over the estimator, is still the original distribution.

** Estimator Reporting
When we report an estimator, we have to report the *standard error* of the estimator. This is the standard deviation of the estimator.

+ $\hat{\theta}$ has a normal distribution :: The value of $\theta$ lies within $\pm 2 se$ of $\hat{\theta}$
+ $\hat{\theta}$ has a non-normal distribution :: The value of $\theta$ lies within $\pm 4 se$ of $\hat{\theta}$

* Point Estimation (Methods)

** Method of Moments
The method of moments is a method to estimate the parameters of a distribution. We use the sample moments to estimate the population moments. In simpler terms, we use the sample statistics to estimate the population parameters.

+ What is a moment? A moment is a function of the random variable $X$: $E(X^k)$ (where $k$ is the order of the moment)

The way we go about this is by using the following formula:

\[
\hat{\theta} = \frac{1}{n} \sum_{i=1}^{n} x_i^k
\]

Where $k$ is the order of the moment, and $x_i$ is the $i^{th}$ observation.

**** Example
Let $X$ be a random variable with a normal distribution with mean $\mu$ and variance $\sigma^2$. We take a random sample of size $n$ from the population, and calculate the sample mean $\bar{x}$. We want to estimate $\mu$ using the method of moments.

Solution:

The first step to solving this problem is to find the sample mean $\bar{x}$:

\[
\bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i
\]

In the above equation we can clearly demonstrate how the method of moments applies. In fact, the definition of the method of moments, if we set $k=1$ is the mean of the sample: $\frac{1}{n} \sum_{i=1}^{n} x_i$.

The next step is to find the sample variance $s^2$:

\[
s^2 = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})^2
\]

For the above, we can see that the method of moments applies again. If we set $k=2$ and consider $x_i$ to be $x_i - \bar{x}$, we get the sample variance. There is of course the matter of the $-1$ in the denominator, this can be explained by the fact that the sample variance is an *unbiased* estimator of the population variance.

Now we can use the method of moments to estimate $\mu$:

\[
\hat{\mu} = \bar{x}
\]

** Maximum Likelihood Estimation

[[./mle.png]]

Maximum likelihood estimation is a method of estimating the parameters of a statistical model, given observations. It uses calculus to find the maximum likelihood of the parameters.

First, we need a likelihood function. This is a function of the parameters, which gives the probability of the observations. The likelihood function is defined as:

\[
L(\theta) = P(X_1, X_2, \dots, X_n | \theta)
\]

Where $\theta$ is the parameter of the distribution. The likelihood function is the probability of the observations, given the parameter.

How can we get this probability? We can use the following formula:

\[
L(\theta) = \prod_{i=1}^{n} f(x_i | \theta)
\]

Where $f(x_i | \theta)$ is the probability density function of the distribution, given the parameter $\theta$. $x_i$ is the $i^{th}$ statistic of the sample.

The maximum likelihood estimator is the value of the parameter that maximizes the likelihood function. This is represented by the following formula:

\[
\hat{\theta} = \underset{\theta}{\text{argmax}} L(\theta)
\]

We will not be using this formula, but it is a good step to understanding. We will take our likelihood function and wrap a natural log around it. This is called the *log-likelihood function*. The log-likelihood function is defined as:

\[
l(\theta) = \ln L(\theta)
\]

We will then take the derivative of the log-likelihood function, and set it equal to zero. This will give us the maximum likelihood estimator.

#+BEGIN_QUOTE
This might seem a bit pointless, but as AI students, this somewhat resembles the process of backpropagation. We take the derivative of the loss function, and set it equal to zero. This gives us the gradient of the loss function, which we can use to update the weights of the neural network. (This is a very basic explanation, but it is a good way to understand the concept)
#+END_QUOTE

* Point Estimation (Python)
For ease, we will use built-in datasets from pandas, such as the iris dataset. We will use the sepal length of the iris dataset.


#+begin_src ipython :tangle yes :results output :exports both :noweb yes :session fda
  import pandas as pd
  import numpy as np
  import matplotlib.pyplot as plt
  from scipy import stats
  iris = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv')
  # craete a random sample of size 60
  sample = iris.sample(60)
#+end_src

#+RESULTS:

We will try to estimate the mean of the sepal length of the iris dataset. We will use the method of moments to estimate the mean.

#+begin_src ipython :tangle yes :results output :exports both :noweb yes :session fda
  sepal_length = iris['sepal_length']
  mean = sepal_length.mean()
  print(mean)
#+end_src

#+RESULTS:
: 5.843333333333334

We now know the actual mean of the sepal length of the iris dataset. We will now try to estimate the mean using the method of moments.

#+begin_src ipython :tangle yes :results output :exports both :noweb yes :session fda
  # \hat{\theta} = \frac{1}{n} \sum_{i=1}^{n} x_i^k
  # we use the sample
  mean = sample['sepal_length'].sum() / sample['sepal_length'].size
  print(mean)
#+end_src

#+RESULTS:
: 5.691666666666666

Now, we will try to estimate the variance using the maximum likelihood estimator.

#+begin_src ipython :tangle yes :results output :exports both :noweb yes :session fda
  iris = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv')
  # craete a random sample of size 60
  n = 10
  samples = [iris.sample(n*2) for i in range(n)]
  variances = [sample['sepal_length'].var() for sample in samples]

  # we get the probability density function for each variance
  f = [stats.norm.pdf(variances[i], iris['sepal_length'].var(), iris['sepal_length'].std()) for i in range(n)]

  # we get the log of the probability density function
  f_log = np.log(f)

  # we get the maximum likelihood estimator
  mle = variances[f_log.argmax()]

  # instead of argmax, we could use the following method using derivatives:
  # 1. we get the derivative of the log-likelihood function
  # 2. we set the derivative equal to zero
  # 3. we solve for the variance

  # we can express the estimated variance as a sum of the actual variance and the error
  print(f"Estimated variance: {mle}, Actual variance: {iris['sepal_length'].var()}")
  print("Error: ", mle - iris['sepal_length'].var())
#+end_src

#+RESULTS:
: Estimated variance: 0.680421052631579, Actual variance: 0.6856935123042507
: Error:  -0.005272459672671648

* Single Sample Intervals
[[file:./Animations/cls.mp4][Animation]]

In this section, we will look at confidence intervals for a single sample. This will combine the idea id random variables, and the idea of sampling distributions.

** Confidence Intervals
This is a range between two values, which we are P% confident that the population parameter lies in. To better understand this, here is a very 'boilerplate' example:

1. We choose a confidence level, $P$.
2. We get its z-score

\[
(\bar{X} - z_P \frac{\sigma}{\sqrt{n}}, \bar{X} + z_P \frac{\sigma}{\sqrt{n}})
\]

Where $\bar{X}$ is the sample mean, and $\sigma$ is the population standard deviation, therefore $\frac{\sigma}{\sqrt{n}}$ is the standard error.

We can also write this as:

\[
\bar{X} \pm z_P \frac{\sigma}{\sqrt{n}}
\]

So what does this tell us? It tells us that we are P% confident that the population mean lies within the interval $\bar{X} \pm z_P \frac{\sigma}{\sqrt{n}}$.
+ The more confident we want to be, the larger the confidence level $P$. But, the larger the confidence level, the larger the interval, the lower the precision.


*** Interpretation
Since we only know $\bar{x}, \sigma \text{ and } n$, we *cannot conclude that* the population mean lies within the interval $\bar{X} \pm z_P \frac{\sigma}{\sqrt{n}}$.

Why? Because we are not using a random sample for the mean. We can only conclude that if we repeated the experiment many times, the result we obtain would occur P% of the time. In other words, *if we get 100 different confidence intervals, $P%$ of them would contain the population mean.*



#+DOWNLOADED: screenshot @ 2023-03-04 13:30:04
[[file:./Single_Sample_Intervals/2023-03-04_13-30-04_screenshot.png]]

Diagram of the process of creating confidence intervals and interpreting them:
#+BEGIN_SRC plantuml :exports none
@startuml
(*) --> "1. Choose a confidence level, P"
--> "2. Get the z-score"
note left
<latex>
z_P = \Phi^{-1}(P)
</latex>
end note
--> "3. Calculate the interval"
note right
<latex>
(\bar{X} - z_P \frac{\sigma}{\sqrt{n}}, \bar{X} + z_P \frac{\sigma}{\sqrt{n}})
</latex>
end note
--> "4. Interpret the interval"
note left
Important: We cannot conclude
that the population mean lies
within the interval.
end note
--> "5. Repeat the experiment"
note right
We can only conclude that if we
repeated the experiment many times,
the result we obtain would occur P% of the time.
end note
@enduml
#+END_SRC



file:./Single_Sample_Intervals/inchart.png


** Confidence Levels
Thus far, we used a variable confidence level $P$. But, we can also use a fixed confidence level, such as 95%. This is the same as using a confidence level of 0.95. (You must use the decimal form, not the percentage form.)
Normaly, the variable which is used to represent the confidence level is $\alpha$. So, we can write the confidence interval as:

\[
\bar{X} \pm z_{\alpha/2} \frac{\sigma}{\sqrt{n}}
\]

Where $z_{\alpha/2}$ is the z-score for the confidence level $\alpha/2$. Why divide by 2? Because we are looking at the area under the curve on both sides of the mean. So, we are looking at the area under the curve for $\alpha/2$ on each side of the mean.


#+DOWNLOADED: screenshot @ 2023-03-04 13:49:22
[[file:./Single_Sample_Intervals/2023-03-04_13-49-22_screenshot.png]]

** Precision and Sample Size
First, we need to define the width of the interval as: $2*z_P \frac{\sigma}{\sqrt{n}}$. This is the width of the interval.
+ Higher the confidence level, the wider the interval.
+ Higher the sample size, the narrower the interval.
+ Lower the population standard deviation, the narrower the interval.
+ Higher the confidence level, the higher the sample size required to achieve a given precision.

We might want to ensure, that a confidence interval has a certain width. In this case, we can use the following formula:

\[
n = (2*z_{\alpha/2} \frac{\sigma}{\text{width}})^2
\]

The application of this

** TODO Derivation of the Confidence Interval
If we have a random sample of size $n$ from a population, we can construct a confidence interval for some parameter $\theta$ using the following steps:
1. Check if the conditions are met:
   + The variable depends on the sample and parameter $\theta$.
   + The probability distribution of the variable is known.
* Large Sample Confidence Intervals (Mean & Proportion)
Previously, we assumed that the population standard deviation $\sigma$ was known and that the population distribution was normal. If we cannot assume these things, we can use the large sample confidence interval.
** Large Sample Confidence Interval for the Mean
It goes back to the central limit theorem. If we take a random sample of size $n$ from a population, we can assume that the sample mean $\bar{X}$ is normally distributed. Therefore, we can use the following formula:

\[
Z = \frac{\bar{X} - \mu}{\frac{\sigma}{\sqrt{n}}}
\]

Where $\mu$ is the population mean, and $\sigma$ is the population standard deviation. Thus, we can write the confidence interval as:

\[
\frac{\bar{X} - \mu}{\frac{\sigma}{\sqrt{n}}} \pm z_{\alpha/2}
\]

\[
P(\frac{\bar{X} - \mu}{\frac{\sigma}{\sqrt{n}}} \pm z_{\alpha/2}) \approx 1 - \alpha
\]

the last equation tells us that we are 100% - $\alpha$% confident that the population mean lies within the interval $\frac{\bar{X} - \mu}{\frac{\sigma}{\sqrt{n}}} \pm z_{\alpha/2}$.


What happens if we replace $\sigma$ with $s$ in the above equation? Since we adding a new random variable to the denominator, we get that:
+ The confidence interval is wider.

But, if our sample size is large enough, the difference between $\sigma$ and $s$ is small, and the confidence interval is not much wider. *What is large enough?* If $n \geq 40$, then the difference between $\sigma$ and $s$ is small enough.


** Large Sample Confidence Interval for Population Proportion
Up till now we talked about being confident that the mean of a population lies within a certain interval. But, what if we want to be confident that the proportion of a population lies within a certain interval? For example, we want to be 95% confident that the proportion of people who like chocolate is between 0.4 and 0.6. We can use the following formula:

\[
P(-z_{\alpha/2} \leq \frac{\hat{p} - p}{\sqrt{\frac{p(1-p)}{n}}} \leq z_{\alpha/2}) \approx 1 - \alpha
\]

Where $\hat{p}$ is the sample proportion, $p$ is the population proportion, and $n$ is the sample size. Since we are talking about proportion, we are also talking about probability, and can use the binomial distribution, where $n$ is the number of trials, and $p$ is the probability of success. Remember that:

\begin{align}
\hat{p} = \frac{X}{n} \\
E(X) = np \\
Var(X) = np(1-p)
\end{align}

An important rule to remember is that the sample proportion is approximately normally distributed if $np \geq 10$ and $n(1-p) \geq 10$.

The general formula for a confidence interval for a population proportion is:

\[
\hat{p} \pm z_{\alpha/2} \sqrt{\frac{\hat{p}(1-\hat{p})}{n}}
\]

This formula can only be used if the sample size is large enough, that is if it is above 40.

** One Sided
All previous confidence intervals talked about two bounds, one on the left and one on the right. But, what if we want to be confident that the population mean is greater than a certain value? For example, we want to be 95% confident that the population mean is within a certain range above the sample mean. We can use the following formula:

\[
\mu < \bar{X} + z_{\alpha} \frac{\sigma}{\sqrt{n}}
\]

Where $\mu$ is the population mean, $\bar{X}$ is the sample mean, $\sigma$ is the population standard deviation, and $n$ is the sample size.


** Confidence Intervals for Normal Distributions
We can assume that the population follows a normal distribution, that in only if $n$ is large enough, (viz the central limit theorem). If we have a sample of size $n$, then the sample mean $\bar{X}$ is approximately normally distributed with mean $\mu$ and standard deviation $\frac{\sigma}{\sqrt{n}}$. We can use the following formula to calculate the confidence interval:

\[
\bar{X} \pm z_{\alpha/2} \frac{\sigma}{\sqrt{n}}
\]

** Confidence Interval for the t-Distribution
If we have a sample for which the mean is $\bar{X}$ and the standard deviation is $s$, then we can define a random variable $T$ as:

\[
T = \frac{\bar{X} - \mu}{\frac{s}{\sqrt{n}}}
\]

The distribution of $T$ is called the *Student's t-distribution*. The t-distribution is similar to the normal distribution, but it has fatter tails. The t-distribution is used when the population standard deviation is unknown, and the sample size is small. The t-distribution is also used when the population distribution is not normal.

What are degrees of freedom? The degrees of freedom is the number of independent pieces of information in a sample. For example, if we have a sample of size $n$, then the degrees of freedom is $n-1$.

Some key properties of the t-distribution:
+ It is more spread out than the normal distribution.
+ The higher $df$ is, the more similar the t-distribution is to the normal distribution.


Confidence interval for the mean using the t-distribution will then be given by this expression:

\[
\bar{X} \pm t_{\alpha, df} \frac{s}{\sqrt{n}}
\]

Where $df$ is the degrees of freedom, and $s$ is the sample standard deviation and $\alpha = 1 - \text{confidence level}$.



** Prediction Interval for Future Values
Now we finally get to discuss future values of some variable rather than estimating what might be the mean of a population.
1. We have a random sample of size $n$. ($X_1, X_2, \dots, X_n$)
2. Now we want to know $X_{n+1}$.

\[
E(\bar{X} - X_{n+1}) = 0
\]

\[
Var(\bar{X} - X_{n+1}) = \frac{\sigma^2}{n} + \sigma^2 = \sigma^2(1 + \frac{1}{n})
\]

Given the above, we can calculate a z-score for the confidence interval:

\[
Z = \frac{(\bar{X} - X_{n+1}) - 0}{\sigma^2 \frac{1}{\sqrt{n}}}
\]

...

\[
T = \frac{(\bar{X} - X_{n+1})}{S\sqrt{1 \frac{1}{n}}}
\]


From this, we can build a confidence interval for the future value of $X_{n+1}$:

\[
\bar{x} \pm t_{\alpha, df} s \sqrt{1 + \frac{1}{n}}
\]

We interpret this the same way as we did for the confidence interval for the mean. We are 95% confident that for multiple iterations, the future value of $X_{n+1}$ will be between the two bounds.


** Bootstrap
** Variance and Standard Deviation Confidence Intervals
If we have a normal distribution, we might also be interested in finding the variance of the population if we do not have it. Given a sample of size $n$, we can define a random variable as follows:

\[
\frac{(n-1)s^2}{\sigma^2} = \frac{\sum(X_i - \bar{X})^2}{\sigma^2}
\]

Where $s$ is the sample standard deviation, and $\sigma$ is the population standard deviation. The distribution of this random variable is called the *chi-squared distribution*. The chi-squared distribution is used to find confidence intervals for the variance of a population.

For this distribution, we must also define the degrees of freedom. The degrees of freedom is $n-1$.


#+DOWNLOADED: https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fi0.wp.com%2Fwww.allaboutlean.com%2Fwp-content%2Fuploads%2F2015%2F08%2FChi-squared-Distribution.png%3Ffit%3D724%252C482%26ssl%3D1&f=1&nofb=1&ipt=d1214ca8c146faba3ec2cea71d64f71a962a6bf8f9f62464ee7bd3bd5a04a0c2&ipo=images @ 2023-03-11 12:13:02
[[file:Large_Sample_Confidence_Intervals_(Mean_&_Proportion)/2023-03-11_12-13-02_.png]]

Key properties of the chi-squared distribution:
+ It is more spread out than the normal distribution.
+ Only positive values are possible.
+ The higher $df$ is, the more similar the chi-squared distribution is to the normal distribution.


Finally, the confidence interval will look like this:

\[
\frac{(n-1)s^2}{\chi^2_{\alpha/2, df}} \leq \sigma^2 \leq \frac{(n-1)s^2}{\chi^2_{1-\alpha/2, df}}
\]

We now have to calculate the confidence interval for the variance of the population. We can do this by using the following formula:

+ Lower bound: $\frac{(n-1)s^2}{\chi^2_{\alpha/2, df}}$
+ Upper bound: $\frac{(n-1)s^2}{\chi^2_{1-\alpha/2, df}}$


Where $\chi^2_{\alpha/2, df}$ is the $\alpha/2$ percentile of the chi-squared distribution with $df$ degrees of freedom.

We can also calculate the confidence interval for the standard deviation of the population. We can do this by using the following formula:

* Confidence Intervals (Python)
We can make our life easier by using ipython to calculate confidence intervals. We will use the following packages:
+ scipy.stats
+ numpy
+ pandas
+ matplotlib


** Simple Confidence Intervals for the Mean
For ease, we will use built-in datasets from pandas, such as the iris dataset. We will use the sepal length of the iris dataset.

#+BEGIN_SRC ipython :exports both :results output :session *Python*
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats


iris = pd.read_csv("https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv")
sepal_length = iris["sepal_length"]
print(sepal_length.head())
#+END_SRC

#+RESULTS:
: 0    5.1
: 1    4.9
: 2    4.7
: 3    4.6
: 4    5.0
: Name: sepal_length, dtype: float64

Now we have, the data. Lets create a confidence interval for the mean of the sepal length. We will use a confidence level of 95%.

#+BEGIN_SRC ipython :exports both :results output :session *Python*
interval = stats.norm.interval(0.95, loc=np.mean(sepal_length), scale=np.std(sepal_length))
print(interval)
#+END_SRC

#+RESULTS:
: (4.2257725250400755, 7.460894141626592)

We can see that the confidence interval is between 4.23 and 7.46.

** Confidence Interval for the Population Proportion
We will use the same dataset as before, but this time we will use the sepal width. We will use a confidence level of 95%. In the first example we do not approximate, we use the exact formula.

#+BEGIN_SRC ipython :exports both :results output :session *Python*
from statsmodels.stats.proportion import proportion_confint
# proportion where the sepal width is greater than 3.5
X = np.sum(iris["sepal_width"] > 3.5)
n = len(iris["sepal_width"])
p = X/n

interval = proportion_confint(X, n, alpha=0.05, method="normal")
print(interval)
#+END_SRC

#+RESULTS:
: (0.0734406885907721, 0.17989264474256125)

The =method= parameter can be either =normal= or =wilson=.
+ =normal= uses the normal approximation. We use this one if $np \geq 10$ and $n(1-p) \geq 10$.
+ =wilson= uses the Wilson score interval. If the conditions are not met, we use this one.


We can now try to approximate with the normal distribution. We will use the same confidence level of 95%.

#+BEGIN_SRC ipython :exports both :results output :session *Python*
  scale = np.sqrt(p*(1-p)/n)
  interval = stats.norm.interval(0.95, loc=p, scale=scale)
  print(interval)
#+END_SRC

#+RESULTS:
: (0.07344068859077212, 0.17989264474256123)


Now lets take a look at binomial approximation for the confidence interval. We will use the same confidence level of 95%. It is important to check if the conditions are met, that is if $np \geq 10$ and $n(1-p) \geq 10$.

#+BEGIN_SRC ipython :exports both :results output :session *Python*
# conditions test
print("np >= 10: ", n*p >= 10)
print("n(1-p) >= 10: ", n*(1-p) >= 10)
#+END_SRC

#+RESULTS:
: np >= 10:  True
: n(1-p) >= 10:  True

Now we can use the binomial approximation.

#+BEGIN_SRC ipython :exports both :results output :session *Python*
  interval = stats.binom.interval(0.95, n, p)
  print(interval)
  interval = [x/n for x in interval]
  print(interval)
#+END_SRC

#+RESULTS:
: (11.0, 27.0)
: [0.07333333333333333, 0.18]

The last step is very important. We need to divide the interval by the sample size to get the proportion interval. We can see that the interval is between 0.073 and 0.18, which is a very close approximation to the normal approximation.

** t Distribution Confidence Intervals
We will use the same dataset as before, but this time we will use the petal length. We will use a confidence level of 95%. In the first example we do not approximate, we use the exact formula.

#+BEGIN_SRC ipython :exports both :results output :session *Python*
from statsmodels.stats.weightstats import _tconfint_generic
# proportion where the sepal width is greater than 3.5
X = np.sum(iris["petal_length"] > 3.5)
n = len(iris["petal_length"])
p = X/n

interval = _tconfint_generic(p, np.sqrt(p*(1-p)/n), n-1, 0.05, 'two-sided')
print(interval)
#+END_SRC

** Prediction Interval for Future Values
We will use the same dataset as before, but this time we will use the petal width. We will use a confidence level of 95%. In this example we are trying to predict the future value of the petal width.

#+BEGIN_SRC ipython :exports both :results output :session *Python*
  from statsmodels.stats.weightstats import _tconfint_generic


  sample_mean = np.mean(iris["petal_width"])
  sample_std = np.std(iris["petal_width"])
  n = len(iris["petal_width"])
  alpha = 0.05

  t_score = stats.t.ppf(1-alpha/2, n-1) # t score for 95% confidence
  interval = (sample_mean - t_score*sample_std/np.sqrt(n), sample_mean + t_score*sample_std/np.sqrt(n))
  print(interval)
#+END_SRC

#+RESULTS:
: (1.0767639167319225, 1.3219027499347447)

So we can now be 95% confident that the future value of the petal width will be between 1.08 and 1.32.
How can we validate this? We can use the bootstrap method.

#+BEGIN_SRC ipython :exports both :results output :session *Python*
  from sklearn.utils import resample

  # bootstrap
  n_iterations = 1000
  n_size = int(len(iris["petal_width"]) * 0.50)
  medians = list()
  for i in range(n_iterations):
    s = resample(iris["petal_width"], n_samples=n_size)
    m = np.mean(s)
    medians.append(m)

  interval = np.percentile(medians, [2.5, 97.5])
  print(interval)
#+END_SRC

#+RESULTS:
: [1.02796667 1.36133333]






** Confidence Interval for the Population Variance
We will use the same dataset as before, but this time we will use the petal width. We will use a confidence level of 95%.

#+BEGIN_SRC ipython :exports both :results output :session *Python*
  s = np.std(iris["petal_width"])
  var = s**2
  n = len(iris["petal_width"])
  alpha = 0.05
  ucl = (n-1)*var/stats.chi2.ppf(alpha/2, n-1)
  lcl = (n-1)*var/stats.chi2.ppf(1-alpha/2, n-1)
  inteval = (lcl, ucl)
  print(interval)
#+END_SRC

#+RESULTS:
: (117.09798286232113, 184.68695493443445)


* Midterm Review
This is a review of the key concepts mentioned in the notes thus far. It is not a complete list of all the concepts that will be on the midterm. It is meant to be a guide to help you study for the midterm.

** Random Variables
Its a bit like musical chairs, as the music plays, we do not know the final outcome of the people that will end up sitting. Using this analogy, a random sample is a subset of the population that we are interested in. The random variable comes once we use that random sample to compute some statistic.
+ A random variable is a variable whose value is determined by chance.
+ A random variable can be discrete or continuous.
+ When is it representative?
  + No replacement sampling
  + Large enough size (at most 5%)

\begin{align}
\mu_\bar{x} &= \mu \\
\sigma_\bar{x} &= \frac{\sigma}{\sqrt{n}}
\end{align}

The variance of a random variable is called the *standard error*.

** Central Limit Theorem
If we take a sampling distribution of the sample mean. As long as the following are true, the sampling distribution of the sample mean will be approximately normal:
+ The population is uni-modal
+ Our sample size is large enough ($n \geq 30$)


** Estimation
We try to estimate some population parameter using a sample statistic.

\[
\hat{\theta} = \theta + \epsilon
\]

Where $\epsilon$ is the error term. The error term is the difference between the population parameter and the sample statistic.


+ Method of Moments :: We can use this method to computer the estimate given the following formula: $\hat{\theta} = \frac{1}{n}\sum_{i=1}^n x_i^k$. All we need is the *order of the moment* and $x$, the sample.
+ Maximum Likelihood Estimator ::

** Confidence Interval Distributions
It is important to know which distribution to use when constructing a confidence interval for some population parameter.

#+BEGIN_SRC plantuml :exports none
@startuml
start
if (Sample Size >= 30) then (yes)
  if (Estimating Mean) then (yes)
    :Normal;
    stop
  else (no)
    if (Estimating Proportion) then (yes)
      :Normal;
      stop
    else (no)
      if (Estimating Variance) then (yes)
        :Chi-Squared;
        stop
      else (no)
        stop
      endif
    endif
  endif
else (no)
  if (Estimating Mean) then (yes)
    :t;
    stop
  else (no)
    if (Estimating Proportion) then (yes)
      :Binomial;
      stop
    else (no)
      if (Estimating Variance) then (yes)
        :Chi-Squared;
        stop
      else (no)
        stop
      endif
    endif
  endif
@enduml
#+END_SRC

[[file:./confint-dist-decision.png]]



* Midterm


* Single Sample Hypothesis Testing
** Hypothesis Testing
** Hypothesis Testing for Normal Distributions
** Hypothesis Testing for Proportions
** Hypothesis Testing for Variances
* Two Sample Hypotheses Testing
** Hypothesis Testing for Two Means
** Hypothesis Testing for Two Proportions
** Hypothesis Testing for Two Variances
* Analysis of Variance: Single Factor
** Analysis of Variance
** Analysis of Variance for Normal Distributions
** Analysis of Variance for Proportions
** Analysis of Variance for Variances
* Analysis of Variance: Multi Factor
** Analysis of Variance for Two Factors
** Analysis of Variance for Three Factors
* Goodness-of-fit Tests
** Goodness-of-fit Tests for Normal Distributions
** Goodness-of-fit Tests for Proportions
** Goodness-of-fit Tests for Variances
* Categorical Data Analysis
** Chi-Square Tests for Independence
** Chi-Square Tests for Homogeneity
** Chi-Square Tests for Goodness-of-fit

#+HTML: <footer style="height: 20vh;"></footer>
